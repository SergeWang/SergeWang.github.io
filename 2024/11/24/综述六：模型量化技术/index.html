<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>综述六：模型量化技术 | Model The World</title><meta name="author" content="Serge Wang"><meta name="copyright" content="Serge Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="量化技术在机器学习（ML）和大型语言模型（LLM）中的应用，指的是将模型的权重和激活值的数值精度降低，通常目的是提高计算效率、减少内存占用，并加速推理过程，而不显著降低准确度。随着时间的推移，这些方法在传统机器学习和深度学习的研究中不断发展。 以下是按引入时间排序的 量化方法 列表：  1. 定点量化（Fixed-Point Quantization） (1980年代 - 1990年代初)  核心">
<meta property="og:type" content="article">
<meta property="og:title" content="综述六：模型量化技术">
<meta property="og:url" content="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E5%85%AD%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/">
<meta property="og:site_name" content="Model The World">
<meta property="og:description" content="量化技术在机器学习（ML）和大型语言模型（LLM）中的应用，指的是将模型的权重和激活值的数值精度降低，通常目的是提高计算效率、减少内存占用，并加速推理过程，而不显著降低准确度。随着时间的推移，这些方法在传统机器学习和深度学习的研究中不断发展。 以下是按引入时间排序的 量化方法 列表：  1. 定点量化（Fixed-Point Quantization） (1980年代 - 1990年代初)  核心">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sergewang.github.io/images/mamba.png">
<meta property="article:published_time" content="2024-11-24T11:04:52.000Z">
<meta property="article:modified_time" content="2024-11-27T07:55:25.838Z">
<meta property="article:author" content="Serge Wang">
<meta property="article:tag" content="量化">
<meta property="article:tag" content="综述">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sergewang.github.io/images/mamba.png"><link rel="shortcut icon" href="/img/newlogo.png"><link rel="canonical" href="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E5%85%AD%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IDUolVgWkW_cmu1mtW5hsxrrIjQfCHvq5hOTOkXeVNE"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9dcb7eb7a8a6225c2b1f242f3b0894bf";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XERFYF0N5K"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-XERFYF0N5K')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-XERFYF0N5K', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '综述六：模型量化技术',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-27 15:55:25'
}</script></head><body><div id="web_bg" style="background-image: url(/img/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/newlogo.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/mamba.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/newlogo.png" alt="Logo"><span class="site-name">Model The World</span></a><a class="nav-page-title" href="/"><span class="site-name">综述六：模型量化技术</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">综述六：模型量化技术</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-24T11:04:52.000Z" title="发表于 2024-11-24 19:04:52">2024-11-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-27T07:55:25.838Z" title="更新于 2024-11-27 15:55:25">2024-11-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>量化技术在机器学习（ML）和大型语言模型（LLM）中的应用，指的是将模型的权重和激活值的数值精度降低，通常目的是提高计算效率、减少内存占用，并加速推理过程，而不显著降低准确度。随着时间的推移，这些方法在传统机器学习和深度学习的研究中不断发展。</p>
<p>以下是按引入时间排序的 <strong>量化方法</strong> 列表：</p>
<hr>
<h3 id="1-定点量化（Fixed-Point-Quantization）-1980年代-1990年代初">1. <strong>定点量化（Fixed-Point Quantization） (1980年代 - 1990年代初)</strong></h3>
<ul>
<li><strong>核心思想</strong>：将浮点数（32位）转换为定点数（例如16位或8位整数）。这可以减少内存需求，并加速计算，因为定点操作通常比浮点操作在硬件上更高效。</li>
<li><strong>应用</strong>：早期的信号处理和硬件实现。</li>
</ul>
<hr>
<h3 id="2-向量量化（Vector-Quantization）-1980年代-1990年代">2. <strong>向量量化（Vector Quantization）(1980年代 - 1990年代)</strong></h3>
<ul>
<li><strong>核心思想</strong>：通过使用较少的质心或码本来表示高维向量（如特征表示）。这是一种有损压缩技术，用于减少表示的维度。</li>
<li><strong>应用</strong>：用于图像和语音压缩。</li>
<li><strong>重要论文</strong>：Gray, “Vector Quantization and Signal Compression”, 1984。</li>
</ul>
<hr>
<h3 id="3-产品量化（Product-Quantization）-2006">3. <strong>产品量化（Product Quantization） (2006)</strong></h3>
<ul>
<li><strong>核心思想</strong>：通过将向量分割成较小的子向量，并分别对每个子向量进行量化，来扩展向量量化。这减少了搜索复杂度，并提高了最近邻搜索的效率。</li>
<li><strong>应用</strong>：大规模图像检索和特征压缩。</li>
<li><strong>重要论文</strong>：Hervé Jégou 等人，“Product Quantization for Nearest Neighbor Search” ，2006。</li>
</ul>
<hr>
<h3 id="4-权重量化（Weight-Quantization）-2015">4. <strong>权重量化（Weight Quantization） (2015)</strong></h3>
<ul>
<li><strong>核心思想</strong>：将神经网络中的权重的精度从32位浮点数降低到较低位数的表示（例如8位或16位整数）。其主要目标是减少内存占用，并加速推理，特别是在专用硬件（如GPU和TPU）上。</li>
<li><strong>应用</strong>：用于深度神经网络，使其能够更高效地部署在硬件上。</li>
<li><strong>重要论文</strong>：Rastegari 等人，“Quantizing deep convolutional networks for efficient inference: A whitepaper” ，2016。</li>
</ul>
<hr>
<h3 id="5-二值量化（Binary-Quantization）-2016">5. <strong>二值量化（Binary Quantization）(2016)</strong></h3>
<ul>
<li><strong>核心思想</strong>：一种更激进的量化方法，将权重限制为两个可能的值，通常是±1。这大大减少了模型的大小和计算复杂度，因为二进制权重可以用一个比特表示。</li>
<li><strong>应用</strong>：通常用于硬件实现中，需要极高效率的场景。</li>
<li><strong>重要论文</strong>：Hubara 等人，“Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1” ，2016。</li>
</ul>
<hr>
<h3 id="6-三值量化（Ternary-Quantization）-2017">6. <strong>三值量化（Ternary Quantization）(2017)</strong></h3>
<ul>
<li><strong>核心思想</strong>：扩展二值量化，将权重限制为三种可能的值（通常是-1、0、+1），在保持比二值量化更多信息的同时，进一步减少了内存占用。</li>
<li><strong>应用</strong>：通常用于硬件受限的环境，如嵌入式系统或边缘设备。</li>
<li><strong>重要论文</strong>：Zhang 等人，“Ternary Weight Networks” ，2017。</li>
</ul>
<hr>
<h3 id="7-量化感知训练（Quantization-Aware-Training-QAT）-2017">7. <strong>量化感知训练（Quantization-Aware Training, QAT）(2017)</strong></h3>
<ul>
<li><strong>核心思想</strong>：与传统的后训练量化方法不同，QAT在训练过程中模拟低精度算术操作。这样，模型可以适应量化的影响，并减小精度损失。</li>
<li><strong>应用</strong>：确保量化后能更好地保留准确性，尤其是对于复杂的深度神经网络。</li>
<li><strong>重要论文</strong>：Jacob 等人，“Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference” ，2018。</li>
</ul>
<hr>
<h3 id="8-动态量化（Dynamic-Quantization）-2018">8. <strong>动态量化（Dynamic Quantization）(2018)</strong></h3>
<ul>
<li><strong>核心思想</strong>：在训练后进行量化，动态地确定每一层或激活的最佳量化方案。这可以在不修改模型的情况下对预训练模型进行量化，且无需重新训练。</li>
<li><strong>应用</strong>：用于推理场景中，尤其是快速推理且不方便重新训练的情况。</li>
<li><strong>重要论文</strong>：Jacob 等人，“Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference” ，2018。</li>
</ul>
<hr>
<h3 id="9-仅整数量化（Integer-Only-Quantization）-2019">9. <strong>仅整数量化（Integer-Only Quantization）(2019)</strong></h3>
<ul>
<li><strong>核心思想</strong>：将所有操作（包括权重和激活值）转换为仅整数的算术（例如8位整数）。这使得模型可以完全在整数硬件上运行，这比浮点数运算更高效。</li>
<li><strong>应用</strong>：通常用于在资源受限的边缘设备（如手机和物联网设备）上的部署。</li>
<li><strong>重要论文</strong>：Rastegari 等人，“Integer-only quantization of deep convolutional networks” ，2019。</li>
</ul>
<hr>
<h3 id="10-低秩量化（Low-Rank-Quantization）-2020">10. <strong>低秩量化（Low-Rank Quantization）(2020)</strong></h3>
<ul>
<li><strong>核心思想</strong>：通过将权重矩阵分解为低秩组件，而不是对所有权重使用固定精度，这样可以对这些组件进行更低位的量化。这对于减少内存和计算成本特别有效。</li>
<li><strong>应用</strong>：用于大规模神经网络，特别是变换器模型（transformer）。</li>
<li><strong>重要论文</strong>：Shen 等人，“Low-Rank Quantization for Neural Network Compression” ，2020。</li>
</ul>
<hr>
<h3 id="11-后训练量化（Post-Training-Quantization-PTQ）-2020">11. <strong>后训练量化（Post-Training Quantization, PTQ）(2020)</strong></h3>
<ul>
<li><strong>核心思想</strong>：一系列技术，允许在不重新训练的情况下对预训练模型进行量化，通常使用校准集来微调量化参数（例如尺度、零点）。PTQ因其易于部署和快速回馈而变得流行。</li>
<li><strong>应用</strong>：主要用于资源受限的环境（如边缘设备）上的部署。</li>
<li><strong>重要论文</strong>：Choukroun 等人，“Post-Training Quantization for Neural Networks: A Survey” ，2020。</li>
</ul>
<hr>
<h3 id="12-变换器模型的量化（Quantization-for-Transformer-Models）-2020-2021">12. <strong>变换器模型的量化（Quantization for Transformer Models）(2020-2021)</strong></h3>
<ul>
<li><strong>核心思想</strong>：专注于变换器（如BERT、GPT等）模型的高效量化。由于这些模型通常较大且占用大量内存，量化在使其能够部署在资源受限的环境中时扮演了关键角色。</li>
<li><strong>应用</strong>：将大型语言模型（LLM）部署到手机设备和云服务等资源有限的环境中。</li>
<li><strong>重要论文</strong>：Zafrir 等人，“Q8BERT: Quantized 8-bit BERT” ，2021。</li>
</ul>
<hr>
<h3 id="13-基于注意力的量化（Attention-based-Quantization）-2021">13. <strong>基于注意力的量化（Attention-based Quantization）(2021)</strong></h3>
<ul>
<li><strong>核心思想</strong>：一种专门针对变换器模型的量化方法，在量化过程中考虑注意力权重的重要性。这允许在不显著损失性能的情况下进行更高效的压缩。</li>
<li><strong>应用</strong>：用于NLP任务中，尤其是变换器模型较为常见的任务。</li>
<li><strong>重要论文</strong>：Tay 等人，“Efficient Transformers: A Survey” ，2020。</li>
</ul>
<hr>
<h3 id="14-混合精度量化（Mixed-Precision-Quantization）-2021">14. <strong>混合精度量化（Mixed-Precision Quantization）(2021)</strong></h3>
<ul>
<li><strong>核心思想</strong>：为模型的不同部分使用不同的位宽。例如，某些层可能使用8位量化，而其他层使用16位，这取决于层对模型整体性能的重要性。</li>
<li><strong>应用</strong>：优化模型大小和推理速度之间的折衷，特别是在大规模深度模型的部署中。</li>
<li><strong>重要论文</strong>：Zhou 等人，“Mixed Precision Quantization for Deep Neural Networks” ，2021。</li>
</ul>
<hr>
<h3 id="15-学习量化（Learned-Quantization）-2021-至今">15. <strong>学习量化（Learned Quantization）(2021-至今)</strong></h3>
<ul>
<li><strong>核心思想</strong>：与固定量化水平不同，学习量化技术在训练过程中学习最优的量化方案，允许根据任务和模型动态调整位宽和量化方案。</li>
<li><strong>应用</strong>：应用于大规模神经网络（例如变换器），提高准确性保留和计算效率。</li>
<li><strong>重要论文</strong>：Xu 等人，“Learned Quantization for Efficient Neural Network Inference” ，2021。</li>
</ul>
<hr>
<h3 id="量化方法总结：">量化方法总结：</h3>
<ol>
<li><strong>定点量化</strong>（1980年代 - 1990年代初）</li>
<li><strong>向量量化</strong>（1980年代 - 1990年代）</li>
<li><strong>产品量化</strong>（2006）</li>
<li><strong>权重量化</strong>（2015）</li>
<li><strong>二值量化</strong>（2016）</li>
<li><strong>三值量化</strong>（2017）</li>
<li><strong>量化感知训练（QAT）</strong>（2017）</li>
<li><strong>动态量化</strong>（2018）</li>
<li><strong>仅整数量化</strong>（2019）</li>
<li><strong>低秩量化</strong>（2020）</li>
<li><strong>后训练量化（PTQ）</strong>（2020）</li>
<li><strong>变换器模型的量化</strong>（2020-2021）</li>
<li><strong>基于注意力的量化</strong>（2021）</li>
<li><strong>混合精度量化</strong>（2021）</li>
<li><strong>学习量化</strong>（2021-至今）</li>
</ol>
<hr>
<p>这些量化方法通过逐步引入不同的技术，帮助提升深度学习模型的计算效率和内存利用率。随着计算硬件（如边缘设备、移动设备）能力的增强，以及大规模模型（如大型语言模型）的兴起，量化技术越来越成为实际部署过程中不可或缺的一部分。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SergeWang.github.io">Serge Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E5%85%AD%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/">https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E5%85%AD%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SergeWang.github.io" target="_blank">Model The World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%87%8F%E5%8C%96/">量化</a><a class="post-meta__tags" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a></div><div class="post-share"><div class="social-share" data-image="/images/mamba.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E5%85%AB%EF%BC%9A%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85/" title="综述八：高斯泼溅"><img class="cover" src="/images/mamba.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">综述八：高斯泼溅</div></div><div class="info-2"><div class="info-item-1">模型 3d gaussian splatting for real-time radiance field rendering （23/08） 论文阅读三十一：3D高斯溅射用于实时辐射场渲染 贡献：  引入各向异性（anisotropic）3D高斯作为高质量、非结构化的辐射场表示。 3D高斯属性的优化方法，与自适应密度控制交织，为捕获的场景创建高质量的表示。 快速可微GPU渲染方法，具有可见性感知，允许各向异性泼溅和快速后向传播，从而获得高质量的新视图合成。     </div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E5%9B%9B%EF%BC%9AYolo%E7%B3%BB%E5%88%97%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述四：Yolo系列及其变体"><img class="cover" src="/images/mamba.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">综述四：Yolo系列及其变体</div></div><div class="info-2"><div class="info-item-1">YOLO（You Only Look Once）是最著名的目标检测系列之一，由Joseph Redmon及其团队开发。YOLO的核心特点是速度快，因为它采用单次前向传递进行目标检测，比许多其他检测模型要快。随着时间的推移，YOLO经历了多个版本的迭代和改进，每个版本都在精度、速度和处理不同应用场景的能力上有所提升。 以下是YOLO的主要版本及其变种的列表：  1. YOLOv1（2015年）  关键特性：  YOLO的第一个版本。 提出了一个单一的神经网络来进行端到端的目标检测。 将目标检测视为回归问题，直接从图像中预测边界框和类别概率。   变种：  Tiny YOLOv1：YOLOv1的轻量版，优化了速度，牺牲了精度，适用于实时应用和低资源设备。   局限性：  对小物体和密集场景的检测较差。 定位精度比当时的其他方法差。     2. YOLOv2（2016年） —...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/10/30/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E9%87%8F%E5%8C%96%E3%80%81%E5%8A%A0%E9%80%9F%E5%92%8C%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95/" title="扩散模型中的量化、加速和采样方法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-30</div><div class="info-item-2">扩散模型中的量化、加速和采样方法</div></div><div class="info-2"><div class="info-item-1">1. 量化方法 核心思想: 量化方法旨在通过将模型参数和激活值从高精度转换为低精度来减小模型大小和计算量，从而提高模型效率。例如，将 FP32 精度的参数转换为 FP16 或 INT8 精度。 工作流程:  训练:  模型量化可以在训练过程中或训练后进行。 推理:  推理阶段使用量化后的模型，通常需要特定的硬件或软件支持。  对象: 模型参数和激活值。 优缺点:  优点:  减少内存占用，允许在资源受限的设备上部署模型。 降低计算量，提高推理速度。 降低功耗，延长电池寿命（尤其适用于移动设备）。   缺点:  可能导致精度损失，需要权衡模型大小/速度和性能。 需要仔细选择量化方法和精度，以最小化精度损失。    应用:  量化方法广泛应用于各种深度学习模型中，包括扩散模型，以提高效率并使其更易于部署。 关于扩散模型量化的额外信息:  来源中没有明确提及特定于扩散模型的量化方法。有关量化方法的更多信息来自外部知识，您可能需要独立核实。  2. 加速方法 核心思想:  加速方法旨在通过减少采样过程中的迭代次数或计算量来提高扩散模型的生成速度。 工作流程:  训练:...</div></div></div></a><a class="pagination-related" href="/2024/11/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%89%E5%8D%81%E5%85%AB%EF%BC%9ATaQ-DiT%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3Transformer%E7%9A%84%E6%97%B6%E9%97%B4%E6%84%9F%E7%9F%A5%E9%87%8F%E5%8C%96/" title="论文阅读三十八：TaQ-DiT：用于扩散Transformer的时间感知量化"><img class="cover" src="/images/TaQ-DiT.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-23</div><div class="info-item-2">论文阅读三十八：TaQ-DiT：用于扩散Transformer的时间感知量化</div></div><div class="info-2"><div class="info-item-1">摘要 基于Transformer的扩散模型，称为扩散Transformers（DiTs），已经在图像和视频生成任务中取得先进性能。然而，它们的大型模型尺寸和缓慢推理速度限制它们的实际应用，呼唤模型压缩方法，如量化。不幸地是，现有DiT量化方法忽略了（1）重建的影响和（2）跨不同层的不同的量化敏感度，阻碍它们的性能。为了解决这些问题，我们提出创新的用于DiTs的时间感知量化（TaQ-DiT）。具体地，（1）当在量化阶段分别重建权重和激活，我们观察到不收敛问题，并引入联合重建方法来解决这个问题。（2）我们发现Post-GELU激活对量化尤其敏感，因为它们在不同的去噪步骤中具有显著的可变性，并且在每个步骤中都存在极端的不对称性和变化。为此，我们提出时变感知变换来促进更有效的量化。实现结果表明，当量化DiT的权重到4位和激活到8位（W4A8）时，我们的方法显著超越先前量化方法。 引言 由于分层架构的高效性，基于 UNet 的扩散模型（DM）[1]...</div></div></div></a><a class="pagination-related" href="/2024/11/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%89%E5%8D%81%E5%9B%9B%EF%BC%9ADuQuant%EF%BC%9A%E9%80%9A%E8%BF%87%E5%8F%8C%E9%87%8D%E8%BD%AC%E6%8D%A2%E5%88%86%E9%85%8D%E5%BC%82%E5%B8%B8%E5%80%BC%E5%8F%AF%E4%BB%A5%E5%A2%9E%E5%BC%BA%E9%87%8F%E5%8C%96LLM/" title="论文阅读三十四：DuQuant：通过双重变换分布异常值可以增强量化LLM"><img class="cover" src="/images/DuQuant.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-20</div><div class="info-item-2">论文阅读三十四：DuQuant：通过双重变换分布异常值可以增强量化LLM</div></div><div class="info-2"><div class="info-item-1">大型语言模型（LLM）的量化面临着重大挑战，特别是由于存在阻碍高效低位表示的异常值激活。传统方法主要是解决正常异常值（Normal Outliers），即所有标记中具有相对较大幅度的激活。然而，这些方法难以平滑显示明显更大值的巨大异常值，这导致低位量化的性能显著下降。本文中，我们介绍DuQuant，一种新的方法，利用旋转和置换变换来更有效的消除大量和正常异常值。首先，DuQuant由构建旋转矩阵开始，使用特定的异常值维度作为先验知识，使用逐块旋转来重分布异常值到相邻通道。第二，我们进一步使用锯齿置换（zigzag permutation）来平衡块间的异常值分布，从而减少逐块方差。后续的旋转进一步平滑激活环境，增强了模型表现。DuQuant简化量化过程，且善于管理异常值，在多个任务上超越各种大小和类型的LLMs的先进基准，即便是4位权重激活量化。我们的代码在： https://github.com/Hsu1023/DuQuant...</div></div></div></a><a class="pagination-related" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%BA%8C%EF%BC%9A1%E4%BD%8DAI%E6%9E%B6%E6%9E%84%EF%BC%9A%E9%83%A8%E5%88%861-1%EF%BC%8C%E5%9F%BA%E4%BA%8EGPU%E7%9A%84%E5%BF%AB%E9%80%9F%E6%97%A0%E6%8D%9FBitNet-b1-58%E6%8E%A8%E7%90%86/" title="论文阅读四十二：1位AI架构：部分1.1，基于GPU的快速无损BitNet b1.58推理"><img class="cover" src="/images/bitnet.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-26</div><div class="info-item-2">论文阅读四十二：1位AI架构：部分1.1，基于GPU的快速无损BitNet b1.58推理</div></div><div class="info-2"><div class="info-item-1">1位大语言模型（LLM）的最新进展，如BitNet[WMD+23]和BitNet b1.58[MWM+24]，为提高LLM在速度和能耗方面的效率提供了一种有前景的方法。这些发展还使本地LLM能够在各种设备上部署。在这项工作中，我们介绍了bitnet.cpp，这是一个量身定制的软件栈，旨在释放1位LLM的全部潜力。具体来说，我们开发了一组内核来支持CPU上三进制BitNet b1.58 LLM的快速无损推理。大量的实验表明，bitnet.cpp在各种型号的CPU上实现了显著的加速，在x86 CPU上从2.37倍到6.17倍不等，在ARM CPU上从1.37倍到5.07倍不等。该代码可在 https://github.com/microsoft/BitNet 上获得。  bitnet.cpp bitnet.cpp是1位LLM（例如bitnet b1.58模型）的推理框架。它提供无损推理，同时优化速度和能耗。bitnet.cpp的初始版本支持CPU上的推理。 如图1所示，bitnet.cpp在ARM...</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%80%EF%BC%9ATransformer%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述一：Transformer及其变体"><img class="cover" src="/images/transformer.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述一：Transformer及其变体</div></div><div class="info-2"><div class="info-item-1">Transformer架构，因其自注意力机制而闻名，能够让模型根据输入序列中不同的标记之间的关系进行加权。这种机制消除了循环神经网络的需求，使得训练变得更加高效。自从2017年原始Transformer的提出以来，已经出现了多个变种，旨在优化性能、扩展其适用范围或解决一些挑战。以下是一些著名的Transformer变种，特别是那些专注于自注意力机制的： 1. 原始Transformer (Vanilla Transformer)  关键特性：原始Transformer架构包括一个编码器-解码器结构，采用自注意力机制。 目的：消除了递归神经网络的需求，使得模型训练更加并行化和高效。 自注意力机制：多头自注意力，通过计算输入序列中所有标记之间的关系来决定重要性。  2. BERT（双向编码器表示）  关键特性：BERT只使用Transformer的编码器部分，采用双向自注意力机制来捕获标记的左右上下文。 目的：通过掩蔽语言模型（MLM）进行预训练，并可以通过微调来提升在下游NLP任务中的表现。  3....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%83%EF%BC%9ACNN%E6%A8%A1%E5%9E%8B/" title="综述七：CNN模型"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述七：CNN模型</div></div><div class="info-2"><div class="info-item-1">Here’s a list of major CNN (Convolutional Neural Network) variants ordered by the time of their publication. CNNs have evolved over the years to improve accuracy, reduce computational costs, and adapt to different domains.  1. LeNet (1998)  Key Idea: One of the first CNN architectures, designed for handwritten digit recognition (MNIST). It used convolutional layers followed by pooling and fully connected layers. Application: Digit classification. Notable Paper: Yann LeCun et al.,...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/newlogo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Serge Wang</div><div class="author-info-description">今日事，今日毕</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SergeWang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SergeWang" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sergew027@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AE%9A%E7%82%B9%E9%87%8F%E5%8C%96%EF%BC%88Fixed-Point-Quantization%EF%BC%89-1980%E5%B9%B4%E4%BB%A3-1990%E5%B9%B4%E4%BB%A3%E5%88%9D"><span class="toc-number">1.</span> <span class="toc-text">1. 定点量化（Fixed-Point Quantization） (1980年代 - 1990年代初)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%90%91%E9%87%8F%E9%87%8F%E5%8C%96%EF%BC%88Vector-Quantization%EF%BC%89-1980%E5%B9%B4%E4%BB%A3-1990%E5%B9%B4%E4%BB%A3"><span class="toc-number">2.</span> <span class="toc-text">2. 向量量化（Vector Quantization）(1980年代 - 1990年代)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BA%A7%E5%93%81%E9%87%8F%E5%8C%96%EF%BC%88Product-Quantization%EF%BC%89-2006"><span class="toc-number">3.</span> <span class="toc-text">3. 产品量化（Product Quantization） (2006)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%9D%83%E9%87%8D%E9%87%8F%E5%8C%96%EF%BC%88Weight-Quantization%EF%BC%89-2015"><span class="toc-number">4.</span> <span class="toc-text">4. 权重量化（Weight Quantization） (2015)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E4%BA%8C%E5%80%BC%E9%87%8F%E5%8C%96%EF%BC%88Binary-Quantization%EF%BC%89-2016"><span class="toc-number">5.</span> <span class="toc-text">5. 二值量化（Binary Quantization）(2016)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E4%B8%89%E5%80%BC%E9%87%8F%E5%8C%96%EF%BC%88Ternary-Quantization%EF%BC%89-2017"><span class="toc-number">6.</span> <span class="toc-text">6. 三值量化（Ternary Quantization）(2017)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5%E8%AE%AD%E7%BB%83%EF%BC%88Quantization-Aware-Training-QAT%EF%BC%89-2017"><span class="toc-number">7.</span> <span class="toc-text">7. 量化感知训练（Quantization-Aware Training, QAT）(2017)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%8A%A8%E6%80%81%E9%87%8F%E5%8C%96%EF%BC%88Dynamic-Quantization%EF%BC%89-2018"><span class="toc-number">8.</span> <span class="toc-text">8. 动态量化（Dynamic Quantization）(2018)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E4%BB%85%E6%95%B4%E6%95%B0%E9%87%8F%E5%8C%96%EF%BC%88Integer-Only-Quantization%EF%BC%89-2019"><span class="toc-number">9.</span> <span class="toc-text">9. 仅整数量化（Integer-Only Quantization）(2019)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-%E4%BD%8E%E7%A7%A9%E9%87%8F%E5%8C%96%EF%BC%88Low-Rank-Quantization%EF%BC%89-2020"><span class="toc-number">10.</span> <span class="toc-text">10. 低秩量化（Low-Rank Quantization）(2020)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E5%90%8E%E8%AE%AD%E7%BB%83%E9%87%8F%E5%8C%96%EF%BC%88Post-Training-Quantization-PTQ%EF%BC%89-2020"><span class="toc-number">11.</span> <span class="toc-text">11. 后训练量化（Post-Training Quantization, PTQ）(2020)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E5%8F%98%E6%8D%A2%E5%99%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%87%8F%E5%8C%96%EF%BC%88Quantization-for-Transformer-Models%EF%BC%89-2020-2021"><span class="toc-number">12.</span> <span class="toc-text">12. 变换器模型的量化（Quantization for Transformer Models）(2020-2021)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E9%87%8F%E5%8C%96%EF%BC%88Attention-based-Quantization%EF%BC%89-2021"><span class="toc-number">13.</span> <span class="toc-text">13. 基于注意力的量化（Attention-based Quantization）(2021)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E9%87%8F%E5%8C%96%EF%BC%88Mixed-Precision-Quantization%EF%BC%89-2021"><span class="toc-number">14.</span> <span class="toc-text">14. 混合精度量化（Mixed-Precision Quantization）(2021)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-%E5%AD%A6%E4%B9%A0%E9%87%8F%E5%8C%96%EF%BC%88Learned-Quantization%EF%BC%89-2021-%E8%87%B3%E4%BB%8A"><span class="toc-number">15.</span> <span class="toc-text">15. 学习量化（Learned Quantization）(2021-至今)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="toc-number">16.</span> <span class="toc-text">量化方法总结：</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%9B%9B%EF%BC%9A%E7%94%A8%E4%BA%8E%E9%AB%98%E6%95%88%E7%BB%86%E8%87%B4%E8%A1%A8%E9%9D%A2%E9%87%8D%E5%BB%BA%E7%9A%84%E4%BA%8C%E6%AC%A1%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85/" title="论文阅读四十四：用于高效细致表面重建的二次高斯泼溅"><img src="/images/QGS.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十四：用于高效细致表面重建的二次高斯泼溅"/></a><div class="content"><a class="title" href="/2024/11/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%9B%9B%EF%BC%9A%E7%94%A8%E4%BA%8E%E9%AB%98%E6%95%88%E7%BB%86%E8%87%B4%E8%A1%A8%E9%9D%A2%E9%87%8D%E5%BB%BA%E7%9A%84%E4%BA%8C%E6%AC%A1%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85/" title="论文阅读四十四：用于高效细致表面重建的二次高斯泼溅">论文阅读四十四：用于高效细致表面重建的二次高斯泼溅</a><time datetime="2024-11-27T02:15:57.000Z" title="发表于 2024-11-27 10:15:57">2024-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%89%EF%BC%9A%E6%B5%8B%E8%AF%95%E6%97%B6%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A0%EF%BC%9ALLMs%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%BE%AE%E8%B0%83/" title="论文阅读四十三：测试时高效学习：LLMs的主动微调"><img src="/images/SIFT.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十三：测试时高效学习：LLMs的主动微调"/></a><div class="content"><a class="title" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%89%EF%BC%9A%E6%B5%8B%E8%AF%95%E6%97%B6%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A0%EF%BC%9ALLMs%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%BE%AE%E8%B0%83/" title="论文阅读四十三：测试时高效学习：LLMs的主动微调">论文阅读四十三：测试时高效学习：LLMs的主动微调</a><time datetime="2024-11-26T13:05:57.000Z" title="发表于 2024-11-26 21:05:57">2024-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%BA%8C%EF%BC%9A1%E4%BD%8DAI%E6%9E%B6%E6%9E%84%EF%BC%9A%E9%83%A8%E5%88%861-1%EF%BC%8C%E5%9F%BA%E4%BA%8EGPU%E7%9A%84%E5%BF%AB%E9%80%9F%E6%97%A0%E6%8D%9FBitNet-b1-58%E6%8E%A8%E7%90%86/" title="论文阅读四十二：1位AI架构：部分1.1，基于GPU的快速无损BitNet b1.58推理"><img src="/images/bitnet.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十二：1位AI架构：部分1.1，基于GPU的快速无损BitNet b1.58推理"/></a><div class="content"><a class="title" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%BA%8C%EF%BC%9A1%E4%BD%8DAI%E6%9E%B6%E6%9E%84%EF%BC%9A%E9%83%A8%E5%88%861-1%EF%BC%8C%E5%9F%BA%E4%BA%8EGPU%E7%9A%84%E5%BF%AB%E9%80%9F%E6%97%A0%E6%8D%9FBitNet-b1-58%E6%8E%A8%E7%90%86/" title="论文阅读四十二：1位AI架构：部分1.1，基于GPU的快速无损BitNet b1.58推理">论文阅读四十二：1位AI架构：部分1.1，基于GPU的快速无损BitNet b1.58推理</a><time datetime="2024-11-26T10:57:57.000Z" title="发表于 2024-11-26 18:57:57">2024-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%80%EF%BC%9ATransformer%E4%B8%AD%E9%87%8D%E8%A6%81%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E5%B9%B6%E9%9D%9E%E6%89%80%E6%9C%89%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%83%BD%E9%9C%80%E8%A6%81/" title="论文阅读四十一：Transformer中重要的是什么？并非所有的注意力都需要"><img src="/images/llm-drop.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十一：Transformer中重要的是什么？并非所有的注意力都需要"/></a><div class="content"><a class="title" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%80%EF%BC%9ATransformer%E4%B8%AD%E9%87%8D%E8%A6%81%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E5%B9%B6%E9%9D%9E%E6%89%80%E6%9C%89%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%83%BD%E9%9C%80%E8%A6%81/" title="论文阅读四十一：Transformer中重要的是什么？并非所有的注意力都需要">论文阅读四十一：Transformer中重要的是什么？并非所有的注意力都需要</a><time datetime="2024-11-26T10:55:38.000Z" title="发表于 2024-11-26 18:55:38">2024-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%EF%BC%9A%E5%A4%A7%E5%9E%8B%E8%A7%86%E8%A7%89%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83/" title="论文阅读四十：大型视觉编码器的多模态自回归预训练"><img src="/images/AIMv2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十：大型视觉编码器的多模态自回归预训练"/></a><div class="content"><a class="title" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%EF%BC%9A%E5%A4%A7%E5%9E%8B%E8%A7%86%E8%A7%89%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E8%87%AA%E5%9B%9E%E5%BD%92%E9%A2%84%E8%AE%AD%E7%BB%83/" title="论文阅读四十：大型视觉编码器的多模态自回归预训练">论文阅读四十：大型视觉编码器的多模态自回归预训练</a><time datetime="2024-11-26T10:52:16.000Z" title="发表于 2024-11-26 18:52:16">2024-11-26</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Serge Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>
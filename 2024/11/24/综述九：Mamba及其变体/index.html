<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>综述九：Mamba及其变体 | Model The World</title><meta name="author" content="Serge Wang"><meta name="copyright" content="Serge Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SSM模型 Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (20&#x2F;06) 论文地址 核心思想：将自注意表示为核特征图的线性点积，并使用矩阵乘法的结合属性将复杂度从 O(N2)\mathcal{O}(N^2)O(N2) 减少到 O(N)\mathcal{O}(N)O(N) ，其中N是序列长度">
<meta property="og:type" content="article">
<meta property="og:title" content="综述九：Mamba及其变体">
<meta property="og:url" content="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9AMamba%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/">
<meta property="og:site_name" content="Model The World">
<meta property="og:description" content="SSM模型 Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (20&#x2F;06) 论文地址 核心思想：将自注意表示为核特征图的线性点积，并使用矩阵乘法的结合属性将复杂度从 O(N2)\mathcal{O}(N^2)O(N2) 减少到 O(N)\mathcal{O}(N)O(N) ，其中N是序列长度">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sergewang.github.io/images/mamba.png">
<meta property="article:published_time" content="2024-11-24T11:04:52.000Z">
<meta property="article:modified_time" content="2024-12-04T08:40:21.059Z">
<meta property="article:author" content="Serge Wang">
<meta property="article:tag" content="综述">
<meta property="article:tag" content="Mamba">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sergewang.github.io/images/mamba.png"><link rel="shortcut icon" href="/img/newlogo.png"><link rel="canonical" href="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9AMamba%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IDUolVgWkW_cmu1mtW5hsxrrIjQfCHvq5hOTOkXeVNE"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9dcb7eb7a8a6225c2b1f242f3b0894bf";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XERFYF0N5K"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-XERFYF0N5K')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-XERFYF0N5K', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '综述九：Mamba及其变体',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-04 16:40:21'
}</script></head><body><div id="web_bg" style="background-image: url(/img/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/newlogo.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/mamba.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/newlogo.png" alt="Logo"><span class="site-name">Model The World</span></a><a class="nav-page-title" href="/"><span class="site-name">综述九：Mamba及其变体</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">综述九：Mamba及其变体</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-24T11:04:52.000Z" title="发表于 2024-11-24 19:04:52">2024-11-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-04T08:40:21.059Z" title="更新于 2024-12-04 16:40:21">2024-12-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Mamba/">Mamba</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="SSM模型">SSM模型</h2>
<h3 id="Transformers-are-RNNs-Fast-Autoregressive-Transformers-with-Linear-Attention-20-06">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (20/06)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.16236">论文地址</a><br>
核心思想：将自注意表示为核特征图的线性点积，并使用矩阵乘法的结合属性将复杂度从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 减少到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span> ，其中N是序列长度。我们证明，这个公式允许迭代实现，大大加速了自回归Transformers，并揭示了它们与循环神经网络的关系。(一种涉及循环的自我注意近似，可以看作是退化的线性SSM)</p>
<p><img src="transformers-are-rnns.png" alt=""></p>
<h3 id="Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models-22-12">Hungry Hungry Hippos: Towards Language Modeling with State Space Models (22/12)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.14052">论文地址</a><br>
核心思想：状态空间模型（SSMs）已经在一些模态中展示先进的序列建模性能，但在语言建模中落后于注意力。而且，尽管近乎线性地而不是二次的序列长度缩放，由于硬件利用率低，SSMs仍然比Transformers慢。本文中，理解语言建模中SSM和注意力之间的表现力差距，以及减小SSMs和注意力间的硬件阻碍。发现现有的SSM在两个方面存在困难：一是回忆序列中的早期标记，二是比较整个序列中的比较。为了理解对语言建模的影响，提出了一个新的SSM层H3，它是专门为这些能力设计的。接下来，为了提高在现代硬件上训练SSM的效率，提出了FlashConv。FlashConv使用融合块FFT算法来提高高达8K序列的效率，并引入了一种新的状态传递算法，该算法利用SSM的循环特性来扩展到更长的序列。(将循环推广到S4，可以视为由两个门控连接夹着SSM的架构。H3还在主SSM层中插入标准局部卷积，将其定义为shift-SSM。)<br>
<img src="H3.png" alt=""></p>
<p><img src="H3-algo.png" alt=""></p>
<h3 id="Hyena-Hierarchy-Towards-Larger-Convolutional-Language-Models-23-02">Hyena Hierarchy: Towards Larger Convolutional Language Models (23/02)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.10866">论文地址</a><br>
核心思想：深度学习的近期发展严重依赖大型Transformers，由于它们的大规模学习能力。然而，Transformers的核心构建模块，注意力操作，在序列长度上表现出二次成本，限制了可访问的上下文数量。基于低秩和稀疏近似的现有次二次方法需要与密集的注意力层相结合，以匹配Transformers，表明能力存在差距。在这项工作中，提出了Hyena，这是一种通过交织隐式参数化长卷积和数据控制门控构建的次二次直接注意力替代方案。在数千到数十万个标记序列的召回和推理任务中，Hyena与依赖状态空间和其他隐式和显式方法的操作相比，将准确性提高了50多个点，与基于注意力的模型相匹配。（与H3使用同样的架构，但使用MLP参数化的全局卷积替换了S4层。）<br>
<img src="Hyena.png" alt=""><br>
<img src="Hyena-algo.png" alt=""></p>
<h3 id="Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models-23-07">Retentive Network: A Successor to Transformer for Large Language Models (23/07)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.08621">论文地址</a><br>
核心思想：在这项工作中，提出了Retentive Network（RETNET）作为大型语言模型的基础架构，同时实现了训练并行性、低成本推理和良好的性能。从理论上推导出<strong>循环和注意力之间的联系</strong>。然后，提出了序列建模的保留机制，该机制支持三种计算范式，即<strong>并行、递归和分块递归</strong>。具体来说，并行表示允许训练并行性。循环表示实现了低成本的O（1）推理，在不牺牲性能的情况下提高了解码吞吐量、延迟和GPU内存。分块递归表示有助于以线性复杂度进行高效的长序列建模，其中每个块都是并行编码的，同时对块进行递归总结。语言建模的实验结果表明，RETNET实现了良好的缩放效果、并行训练、低成本部署和高效推理。这些有趣的特性使RETNET成为大型语言模型Transformer的有力继承者。代码 <a target="_blank" rel="noopener" href="https://aka.ms/retnet">https://aka.ms/retnet</a> 。 （架构中添加额外的门控，并使用更加简单的SSM，允许候选的可并行计算路径，使用多头注意力（MHA）的变体，而不是卷积。）<br>
<img src="RetNet.png" alt=""><br>
<img src="RetNet-presudocode.png" alt=""></p>
<h3 id="RWKV-Reinventing-RNNs-for-the-Transformer-Era-23-05">RWKV: Reinventing RNNs for the Transformer Era (23/05)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.13048">论文地址</a><br>
核心思想：Transformer 彻底改变了几乎所有的自然语言处理 (NLP) 任务，但其内存和计算复杂度却随序列长度呈二次方增长。相比之下，循环神经网络 (RNN) 在内存和计算要求方面表现出线性扩展，但由于并行化和可扩展性的限制，难以达到与 Transformer 相同的性能。我们提出了一种新颖的模型架构，即接受加权键值 (RWKV)，它将 Transformer 的高效并行训练与 RNN 的高效推理相结合。方法利用了线性注意机制，并允许我们将模型制定为 Transformer 或 RNN，从而在训练期间并行计算，并在推理期间保持恒定的计算和内存复杂度。将模型扩展到 140 亿个参数，这是迄今为止训练过的最大的密集 RNN，并且发现 RWKV 的性能与类似大小的 Transformer 相当，这表明未来的工作可以利用这种架构来创建更高效​​的模型。这项工作为在序列处理任务中协调计算效率和模型性能之间的权衡迈出了重要一步。（一种最近的额RNN，旨在语言建模，基于另一种线性注意力近似，无注意力的Transformer。其主要的“WKV”机制设计LTI循环，可以视为两个SSMs的比率。）<br>
<img src="RWKV.png" alt=""><br>
<img src="RWKV-language-modeling.png" alt=""></p>
<h3 id="On-the-Parameterization-and-Initialization-of-Diagonal-State-Space-Models-22-06">On the Parameterization and Initialization of Diagonal State Space Models (22/06)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.11893">论文地址</a><br>
核心思想：状态空间模型作为深度学习层被证明可以有效作为RNNs、CNNs或者Transformers等序列模型的有前途的替代。S4模型首个展示这种潜力，通过使用称为HiPPO矩阵的规定状态矩阵，在涉及远程依赖的任务上特别有效。虽然这有一个可解释的数学机制来建模长依赖关系，但它引入了一种难以实现的自定义表示和算法。另一方面，S4的一个最新变体DSS表明，在使用基于近似S4矩阵的特定初始化时，将状态矩阵限制为完全对角仍然可以保持原始模型的性能。这项工作试图系统地理解如何参数化和初始化这种对角状态空间模型。虽然从经典结果可以得出几乎所有SSM都具有等效的对角线形式，但文章表明初始化对性能至关重要。最终模型S4D是S4的简单对角线版本，其内核计算只需要2行代码，在几乎所有设置下的性能都与S4相当。<br>
主要贡献：</p>
<ul>
<li>首先，描述了S4D，这是一种由S4提出的计算对角而不是DPLR矩阵的简单方法，它基于Vandermonde矩阵乘法，甚至比DSS更简单、更高效。在核心状态矩阵之外，对SSM其他组件的不同表示进行了分类，引入了灵活的设计选择，这些选择同时捕捉了S4和DSS，并允许系统地比较不同的SSM参数化。</li>
<li>对DSS的初始化进行了新的数学分析，表明当状态大小变为无穷大时，原始HiPPO矩阵的对角近似令人惊讶地产生了与S4相同的动态。我们提出了使用不同状态矩阵初始化的对角SSM的更简单变体。</li>
<li>对许多领域、任务和序列长度的这些不同设计选择进行了对照研究，并比较了对角线（S4D）和DPLR（S4）变体。最好的S4D方法在几乎所有设置上都与S4具有竞争力，在图像、音频和医疗时间序列基准测试上接近最先进的结果，在Long Range Arena基准测试中达到85%。</li>
</ul>
<p><img src="s4d.png" alt=""></p>
<h3 id="Effectively-Modeling-Time-Series-with-Simple-Discrete-State-Spaces-23-03">Effectively Modeling Time Series with Simple Discrete State Spaces (23/03)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.09489">论文地址</a><br>
核心思想：时间序列建模方法通常需要（1）丰富表示复杂依赖（2）预测长视野（3）在长序列上高效训练。状态空间模型（SSMs）是用于时间序列的经典模型，以及先前工作结合SSMs和深度学习层来有效序列建模。然而，这些方法具有根本性限制：它们的SSM表示不能表达自回归时间序列过程。因此，引入SPACETIME，一种新的状态空间时间序列架构，改进所有三种标准。对于表达能力，提出一种新的基于伴随矩阵（离散时间过程的规范表示）的SSM参数化，实现SPACETIME的SSM层可以学习期望的自回归过程。对于长视野预测，引入伴随SSM的“closed-loop”变体，实现SPACETIME可以预测许多未来的时间步，通过生成其自身的分层输入。对于高效训练和推理，引入一种通过伴随矩阵减少前向传递中内存和计算的算法。对于序列长度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">l</mi></mrow><annotation encoding="application/x-tex">\mathcal{l}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span> 和状态空间大小d，从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>O</mi><mo>~</mo></mover><mo stretchy="false">(</mo><mi>d</mi><mi>l</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{O}(dl)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1702em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span></span></span></span> 变为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>O</mi><mo>~</mo></mover><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><mi>l</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{O}(d + l)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1702em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span></span></span></span> 。</p>
<p><img src="discrete-state-space.png" alt=""><br>
<img src="spacetime-network.png" alt=""></p>
<h3 id="Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces-21-11">Efficiently Modeling Long Sequences with Structured State Spaces (21/11)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.00396">论文地址</a><br>
核心思想：序列建模的中心目标是设计单一原则模型，解决一系列模态和任务的序列数据，尤其是远程依赖。虽然传统模型，包括RNNs、CNNs和Transformers拥有专门的捕捉远程依赖的变体，它们难以扩展到非常长的10000或更多步的序列。最近一项有前途的方法提出通过模拟基本状态模型 （SSM） <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>A</mi><mi>x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>B</mi><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mi>x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>D</mi><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x&#x27;(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 建模序列，并表明，选择适当的状态矩阵A，该系统可以在数学和经验上处理远程依赖。然而，该方法有严重计算和内存需要，使其不适合作为通序列建模方案。提出结构化状态空间模型（S4），基于SSM的新的参数化，表明，可以比先前方法更高效计算，同时保留理论能力。技术涉及用低秩校正来调节A，使其能够稳定地对角化，并将SSM简化为柯西核的充分研究计算。</p>
<p><img src="S4.png" alt=""></p>
<h3 id="Simplified-State-Space-Layers-for-Sequence-Modeling-22-08">Simplified State Space Layers for Sequence Modeling (22/08)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.04933">论文地址</a><br>
核心思想：使用结构化状态空间序列（S4）层的模型在远程序列建模任务上取得先进性能。S4层结合线性状态空间模型（SSMs）、HiPPO框架，和深度学习取得高性能。构建在S4之上，引入新的状态空间层，S5。鉴于S4层使用许多独立的单一输入、单一输出的SSMs，S5层使用一个多输入、多输出的SSM。文中构建了S5和S4之间的连接，用于开发S5模型使用的初始化和参数化。其结果是，状态空间层可以利用高效且广泛实施的并行扫描，使S5的计算效率与S4相匹配，同时在几个远程序列建模任务上实现了最先进的性能。<br>
<img src="S5.png" alt=""></p>
<h3 id="Structured-State-Space-Models-for-In-Context-Reinforcement-Learning-23-03">Structured State Space Models for In-Context Reinforcement Learning (23/03)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.03982">论文地址</a><br>
核心思想：S4模型最近在远程序列建模任务上取得先进性能。这些模型还具有快速推理速度和并行化训练，使得它们在许多强化学习设置中具有潜在用途。提出S4变体的改进，使得可以并行地初始和重置隐藏状态，允许解决强化学习任务。经证明，改进的架构在序列长度上比Transformers渐近运行得更快，在简单的基于内存的任务上比RNN表现更好。在一组部分可观察的环境中评估了修改后的架构，发现在实践中，模型表现优于RNN，同时运行速度也快了五倍多。然后，通过利用模型处理长距离序列的能力，在具有挑战性的元学习任务中取得了很好的性能，在该任务中，智能体被给予一个随机采样的连续控制环境，并结合了环境观察和动作的随机采样线性投影。此外，证明了所得到的模型可以适应未分配的任务。总体而言，本文的研究结果表明，结构化状态空间模型对于上下文强化学习任务来说是快速和高效的。代码<a target="_blank" rel="noopener" href="https://github.com/luchris429/s5rl">https://github.com/luchris429/s5rl</a> 。</p>
<p><img src="s4rl-algo.png" alt=""></p>
<h3 id="GateLoop-Fully-Data-Controlled-Linear-Recurrence-for-Sequence-Modeling-23-11">GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling (23/11)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.01927">论文地址</a><br>
核心思想：线性递归已被证明是有效建模长序列的强大工具。在这项工作中，我们表明现有的模型未能充分利用其潜力。受这一发现的启发，我们开发了GateLoop，这是一种基础序列模型，通过采用<strong>数据控制的状态转换</strong>来推广线性递归模型，如S4、S5、LRU和RetNet。利用这一理论进步，GateLoop在经验上优于现有的自回归语言建模模型。我们的方法具有低成本的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(l)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span></span></span></span> 循环模式和高效的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi mathvariant="script">l</mi><msub><mtext>log</mtext><mn>2</mn></msub><mi mathvariant="script">l</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\mathcal{l}\text{log}_2\mathcal{l})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord"><span class="mord text"><span class="mord">log</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span></span></span></span> 并行模式，其中l是序列长度，利用了<strong>高度优化的关联扫描</strong>实现。此外，我们推导出了一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>l</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(l^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> <strong>替代(surrogate)注意力</strong>模式，揭示了它对Transformer和最近提出的架构的显著影响。具体来说，我们证明我们的方法可以被解释为<strong>向注意力提供数据控制的相对位置信息</strong>。虽然许多现有的模型仅依赖于数据控制的累计和进行上下文聚合，但我们的研究结果表明，整合数据控制的复杂累积乘积可能是迈向更强大的序列模型的关键一步。<br>
<img src="GateLoop.png" alt=""></p>
<h2 id="Mamba模型">Mamba模型</h2>
<h3 id="Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces-23-12">Mamba: Linear-Time Sequence Modeling with Selective State Spaces (23/12)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">论文地址</a><br>
<a target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">代码</a></p>
<p>核心思想：大多数预训练基础模型都是基于Transformer，自注意力能够在上下文窗口中密集地路由信息，允许其建模复杂数据。但，无法建模有限窗口外的事物，且随着窗口长度二次缩放。结构化状态空间序列模型（SSMs）可视为循环神经网络（RNNs）和卷积神经网络（CNNs）的组合，可以作为循环（常量推理）或者卷积（并行训练）高效计算，具有线性或近线性的序列长度缩放。此外，它们有原则性的机制来模拟某些数据模式中的长期依赖关系。<br>
主要贡献：</p>
<ul>
<li>选择机制。识别先前模型的关键限制：无法以输入依赖的方式有效选择数据（即，关注或忽略特定的输入），通过基于输入参数化SSM参数，设计了简单的选择机制，允许模型过滤不相干信息，并无限期地记住相关信息。</li>
<li>硬件感知算法。在结构化状态空间模型中加入选择机制，对模型计算提出挑战。事实上，所有先前的SSMs模型必须是时间和输入不变的，从而高效计算。为此，提出硬件感知算法，使用扫描循环计算模型，而不是卷积，而且不实体化扩展状态以避免不同GPU内存层次结构级别的IO访问。</li>
<li>架构。通过组合先前SSM架构的设计和Transformers中的MLP模块到单一模块，简化了先前深度序列模型架构，形成一个结合选择性状态空间的简单同质的架构设计（Mamba）。</li>
</ul>
<p><img src="/images/mamba.png" alt=""></p>
<h3 id="Transformers-are-SSMs-Generalized-Models-and-Efficient-Algorithms-Through-Structured-State-Space-Duality-24-05">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (24/05)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.21060">论文地址</a></p>
<p>核心思想：虽然Transformers已经成为深度学习在语言建模中成功的主要架构，状态空间模型（SSMs），如Mamba，近期被证明在小到中尺寸匹配或超越Transformers。我们证明，这些模型系列实际上是非常紧密相关的，并在SSMs和注意力变体之间开发一个丰富的理论联系框架，通过对一类研究良好的<strong>结构化半可分矩阵的各种分解</strong>来联系。我们的状态空间对偶性（SSD）框架允许我们设计新的架构（Mamba-2），其核心层是Mamba的选择性SSM的优化，速度快2-8倍，同时继续在语言建模中与Transformers具有可比性。<br>
贡献：</p>
<ul>
<li>证明，状态空间模型和良好定义的一组称为半可分矩阵的结构化矩阵之间的等价性。该联系是SSD框架的核心，揭示了SSMs新特性和算法。本文的核心信息是：计算状态空间模型的不同方法，可以重塑为，在半结构化矩阵上的，各种矩阵乘法算法。</li>
<li>显著改进线性注意力（Transformer are RNNs）的理论。首先通过张量收缩语言提供其循环形式的有利证明，并且将其推广到一组新的结构化掩码注意力（SMA）。</li>
<li>连接SSMs和SMA，表明，它们具有大量交互，彼此对偶，同时拥有类似SSM的线性和注意力的二次形式。还证明，任意具有快速循环形式的核注意力方法必须是SSM。</li>
</ul>
<h3 id="STUFFED-MAMBA-State-Collapse-and-State-Capacity-of-Rnn-Based-Long-Context-Modeling-24-10">STUFFED MAMBA: State Collapse and State Capacity of Rnn-Based Long-Context Modeling (24/10)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.07145">论文地址</a><br>
核心思想：循环神经网络（RNNs）超越基于transformer的语言模型的关键优势在于它们关于序列长度的线性计算复杂度，使得它们在处理长序列推理中更加快速。然而，大多公开可用的RNNs（如，Mamba和RWKV）在少于10K标记的序列上训练，它们在更长上下文中的有效性目前仍极为不满意。在本文中，我们研究了RNN无法处理长上下文的原因，并提出了关键的缓解措施。我们研究了将最先进的RNN应用于长上下文时的两个实际问题：（1）无法推断长于训练长度的输入；（2）内存容量的上限。为了解决第一个问题，我们首先研究了状态崩溃（SC），这是一种在训练过程中没有遇到的序列长度上导致严重性能下降的现象。通过对照实验，我们将其归因于训练长度对循环状态的过度拟合。对于第二个问题，我们在长文档上训练了一系列Mamba-2模型，以实证估计语言建模和密钥检索中的循环状态容量。然后，提出了三种SC缓解方法来提高Mamba-2的长度泛化能力，使模型在没有SC的情况下处理超过1M个令牌。我们还发现，密钥检索中的循环状态容量与状态大小呈指数关系，我们在256K上下文长度上实证训练了一个具有接近完美密钥检索精度的Mamba-2 370M。这表明基于RNN的长上下文建模前景广阔。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SergeWang.github.io">Serge Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9AMamba%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/">https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9AMamba%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SergeWang.github.io" target="_blank">Model The World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a><a class="post-meta__tags" href="/tags/Mamba/">Mamba</a></div><div class="post-share"><div class="social-share" data-image="/images/mamba.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9ALLM/" title="综述九：LLM"><img class="cover" src="/images/mamba.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">综述九：LLM</div></div><div class="info-2"><div class="info-item-1">提示（Prompts） Large Lanuage Models Can Self-Improve in Long-context Reasoning (24/10) 论文地址 核心思想：大型语言模型（LLM）在处理长上下文方面取得了实质性进展，但在长上下文推理方面仍存在困难。现有的方法通常涉及使用合成数据对LLM进行微调，这取决于人类专家的注释或GPT-4等高级模型，从而限制了进一步的进步。为了解决这个问题，研究了LLM在长上下文推理中自我改进的潜力，并提出了专门为此目的设计的SEALONG方法。这种方法很简单：对每个问题的多个输出进行采样，用最小贝叶斯风险对其进行评分，然后根据这些输出进行监督微调或偏好优化。在几个领先的LLM上进行的广泛实验证明了SEALONG的有效性，Llama-3.1-8B-Instruct的绝对提高了4.2分。此外，与依赖于人类专家或高级模型生成的数据的先前方法相比，SEALONG实现了更优的性能。我们预计，这项工作将为长期情景下的自我提升技术开辟新的途径，这对LLM的持续发展至关重要。 </div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%BA%8C%EF%BC%9ADiTs%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述二：DiTs及其变体"><img class="cover" src="/images/SIT.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">综述二：DiTs及其变体</div></div><div class="info-2"><div class="info-item-1">**DiT（去噪扩散Transformer模型）**是结合了Transformer架构和扩散模型的一类生成模型，特别专注于在扩散框架内的去噪过程。扩散模型通过逐步添加和去除噪声的过程来建模复杂的分布，近年来在生成任务中非常流行。 下面是一些著名的DiT变种，它们在不同方面扩展了原始的DiT模型： 1. DiT（原始版本）  关键特性：原始的DiT模型将Transformer架构与去噪扩散过程结合，利用Transformer的注意力机制改进生成质量和训练的可扩展性。 目的：生成高质量的图像，并与传统的卷积神经网络相比，提高训练效率。  2. DiT++（增强版DiT）  关键特性：DiT++是在原始DiT基础上进行增强的版本，可能包括模型架构的改进、训练方法的优化或额外的正则化技术。 目的：通过改进Transformer架构和扩散过程中的噪声调度，提升生成稳定性和性能。  3....</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%80%EF%BC%9ATransformer%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述一：Transformer及其变体"><img class="cover" src="/images/transformer.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述一：Transformer及其变体</div></div><div class="info-2"><div class="info-item-1">Transformer架构，因其自注意力机制而闻名，能够让模型根据输入序列中不同的标记之间的关系进行加权。这种机制消除了循环神经网络的需求，使得训练变得更加高效。自从2017年原始Transformer的提出以来，已经出现了多个变种，旨在优化性能、扩展其适用范围或解决一些挑战。以下是一些著名的Transformer变种，特别是那些专注于自注意力机制的： 1. 原始Transformer (Vanilla Transformer)  关键特性：原始Transformer架构包括一个编码器-解码器结构，采用自注意力机制。 目的：消除了递归神经网络的需求，使得模型训练更加并行化和高效。 自注意力机制：多头自注意力，通过计算输入序列中所有标记之间的关系来决定重要性。  2. BERT（双向编码器表示）  关键特性：BERT只使用Transformer的编码器部分，采用双向自注意力机制来捕获标记的左右上下文。 目的：通过掩蔽语言模型（MLM）进行预训练，并可以通过微调来提升在下游NLP任务中的表现。  3....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%89%EF%BC%9A%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/" title="综述三：持续学习及其方法"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述三：持续学习及其方法</div></div><div class="info-2"><div class="info-item-1">持续学习（Continual Learning，CL），也称为终身学习（Lifelong Learning），指的是模型能够从持续不断的数据流中学习，并随着时间的推移不断适应和获得新知识，而不会遗忘先前学习的内容。持续学习面临的一个主要挑战是灾难性遗忘（Catastrophic Forgetting），即在学习新任务时，模型容易遗忘之前学习的任务。 为了应对这些挑战，提出了多种方法，这些方法可以根据它们如何处理遗忘、如何存储知识以及如何使用新数据进行分类。 下面是主要的持续学习方法，按它们所采用的主要策略进行组织： 1....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%83%EF%BC%9ACNN%E6%A8%A1%E5%9E%8B/" title="综述七：CNN模型"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述七：CNN模型</div></div><div class="info-2"><div class="info-item-1">Here’s a list of major CNN (Convolutional Neural Network) variants ordered by the time of their publication. CNNs have evolved over the years to improve accuracy, reduce computational costs, and adapt to different domains.  1. LeNet (1998)  Key Idea: One of the first CNN architectures, designed for handwritten digit recognition (MNIST). It used convolutional layers followed by pooling and fully connected layers. Application: Digit classification. Notable Paper: Yann LeCun et al.,...</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9ALLM/" title="综述九：LLM"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述九：LLM</div></div><div class="info-2"><div class="info-item-1">提示（Prompts） Large Lanuage Models Can Self-Improve in Long-context Reasoning (24/10) 论文地址 核心思想：大型语言模型（LLM）在处理长上下文方面取得了实质性进展，但在长上下文推理方面仍存在困难。现有的方法通常涉及使用合成数据对LLM进行微调，这取决于人类专家的注释或GPT-4等高级模型，从而限制了进一步的进步。为了解决这个问题，研究了LLM在长上下文推理中自我改进的潜力，并提出了专门为此目的设计的SEALONG方法。这种方法很简单：对每个问题的多个输出进行采样，用最小贝叶斯风险对其进行评分，然后根据这些输出进行监督微调或偏好优化。在几个领先的LLM上进行的广泛实验证明了SEALONG的有效性，Llama-3.1-8B-Instruct的绝对提高了4.2分。此外，与依赖于人类专家或高级模型生成的数据的先前方法相比，SEALONG实现了更优的性能。我们预计，这项工作将为长期情景下的自我提升技术开辟新的途径，这对LLM的持续发展至关重要。 </div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%BA%8C%EF%BC%9ADiTs%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述二：DiTs及其变体"><img class="cover" src="/images/SIT.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述二：DiTs及其变体</div></div><div class="info-2"><div class="info-item-1">**DiT（去噪扩散Transformer模型）**是结合了Transformer架构和扩散模型的一类生成模型，特别专注于在扩散框架内的去噪过程。扩散模型通过逐步添加和去除噪声的过程来建模复杂的分布，近年来在生成任务中非常流行。 下面是一些著名的DiT变种，它们在不同方面扩展了原始的DiT模型： 1. DiT（原始版本）  关键特性：原始的DiT模型将Transformer架构与去噪扩散过程结合，利用Transformer的注意力机制改进生成质量和训练的可扩展性。 目的：生成高质量的图像，并与传统的卷积神经网络相比，提高训练效率。  2. DiT++（增强版DiT）  关键特性：DiT++是在原始DiT基础上进行增强的版本，可能包括模型架构的改进、训练方法的优化或额外的正则化技术。 目的：通过改进Transformer架构和扩散过程中的噪声调度，提升生成稳定性和性能。  3....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%BA%94%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%85%B6%E5%88%86%E7%B1%BB/" title="综述五：强化学习及其分类"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述五：强化学习及其分类</div></div><div class="info-2"><div class="info-item-1">强化学习的变种 强化学习（Reinforcement Learning，简称RL）是一种机器学习方法，其中代理（Agent）通过与环境的交互来学习做出决策。代理的目标是通过采取适当的行动，最大化长期累积的奖励。强化学习是一个广泛的领域，许多不同的变种和算法已被开发出来，以解决学习、探索和决策等不同方面的问题。 以下是强化学习的主要变种及其子类别：  1. 无模型 vs 有模型强化学习  无模型强化学习（Model-Free RL）：代理不构建或使用环境的动态模型，而是直接通过与环境的交互来学习。  示例：  Q学习（Q-Learning，包括表格化和深度Q学习） 策略梯度方法（例如，REINFORCE） Actor-Critic方法（例如A3C，PPO）     有模型强化学习（Model-Based RL）：代理试图学习环境的转移动态和奖励函数，并利用这些模型来做出更有信息量的决策。  示例：  Dyna-Q（Q学习与规划结合） World Models 蒙特卡洛树搜索（MCTS）       2....</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/newlogo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Serge Wang</div><div class="author-info-description">今日事，今日毕</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SergeWang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SergeWang" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sergew027@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#SSM%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">SSM模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformers-are-RNNs-Fast-Autoregressive-Transformers-with-Linear-Attention-20-06"><span class="toc-number">1.1.</span> <span class="toc-text">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (20&#x2F;06)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hungry-Hungry-Hippos-Towards-Language-Modeling-with-State-Space-Models-22-12"><span class="toc-number">1.2.</span> <span class="toc-text">Hungry Hungry Hippos: Towards Language Modeling with State Space Models (22&#x2F;12)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hyena-Hierarchy-Towards-Larger-Convolutional-Language-Models-23-02"><span class="toc-number">1.3.</span> <span class="toc-text">Hyena Hierarchy: Towards Larger Convolutional Language Models (23&#x2F;02)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models-23-07"><span class="toc-number">1.4.</span> <span class="toc-text">Retentive Network: A Successor to Transformer for Large Language Models (23&#x2F;07)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RWKV-Reinventing-RNNs-for-the-Transformer-Era-23-05"><span class="toc-number">1.5.</span> <span class="toc-text">RWKV: Reinventing RNNs for the Transformer Era (23&#x2F;05)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-the-Parameterization-and-Initialization-of-Diagonal-State-Space-Models-22-06"><span class="toc-number">1.6.</span> <span class="toc-text">On the Parameterization and Initialization of Diagonal State Space Models (22&#x2F;06)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Effectively-Modeling-Time-Series-with-Simple-Discrete-State-Spaces-23-03"><span class="toc-number">1.7.</span> <span class="toc-text">Effectively Modeling Time Series with Simple Discrete State Spaces (23&#x2F;03)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Efficiently-Modeling-Long-Sequences-with-Structured-State-Spaces-21-11"><span class="toc-number">1.8.</span> <span class="toc-text">Efficiently Modeling Long Sequences with Structured State Spaces (21&#x2F;11)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Simplified-State-Space-Layers-for-Sequence-Modeling-22-08"><span class="toc-number">1.9.</span> <span class="toc-text">Simplified State Space Layers for Sequence Modeling (22&#x2F;08)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Structured-State-Space-Models-for-In-Context-Reinforcement-Learning-23-03"><span class="toc-number">1.10.</span> <span class="toc-text">Structured State Space Models for In-Context Reinforcement Learning (23&#x2F;03)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GateLoop-Fully-Data-Controlled-Linear-Recurrence-for-Sequence-Modeling-23-11"><span class="toc-number">1.11.</span> <span class="toc-text">GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling (23&#x2F;11)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mamba%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">Mamba模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mamba-Linear-Time-Sequence-Modeling-with-Selective-State-Spaces-23-12"><span class="toc-number">2.1.</span> <span class="toc-text">Mamba: Linear-Time Sequence Modeling with Selective State Spaces (23&#x2F;12)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformers-are-SSMs-Generalized-Models-and-Efficient-Algorithms-Through-Structured-State-Space-Duality-24-05"><span class="toc-number">2.2.</span> <span class="toc-text">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (24&#x2F;05)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#STUFFED-MAMBA-State-Collapse-and-State-Capacity-of-Rnn-Based-Long-Context-Modeling-24-10"><span class="toc-number">2.3.</span> <span class="toc-text">STUFFED MAMBA: State Collapse and State Capacity of Rnn-Based Long-Context Modeling (24&#x2F;10)</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%EF%BC%9A%E5%AD%97%E8%8A%82%E6%BD%9C%E5%9C%A8Transformer%EF%BC%9APatches%E6%AF%94Tokens%E6%89%A9%E5%B1%95%E6%80%A7%E5%A5%BD/" title="论文阅读五十：字节潜在Transformer：Patches比Tokens扩展性好"><img src="/images/BLT.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读五十：字节潜在Transformer：Patches比Tokens扩展性好"/></a><div class="content"><a class="title" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%EF%BC%9A%E5%AD%97%E8%8A%82%E6%BD%9C%E5%9C%A8Transformer%EF%BC%9APatches%E6%AF%94Tokens%E6%89%A9%E5%B1%95%E6%80%A7%E5%A5%BD/" title="论文阅读五十：字节潜在Transformer：Patches比Tokens扩展性好">论文阅读五十：字节潜在Transformer：Patches比Tokens扩展性好</a><time datetime="2024-12-17T11:15:34.000Z" title="发表于 2024-12-17 19:15:34">2024-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B9%9D%EF%BC%9A%E5%A4%A7%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9E%8B%EF%BC%9A%E5%8F%A5%E5%AD%90%E8%A1%A8%E7%A4%BA%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1/" title="论文阅读四十九：大型概念模型：句子表示空间中的语言建模"><img src="/images/LCM.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十九：大型概念模型：句子表示空间中的语言建模"/></a><div class="content"><a class="title" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B9%9D%EF%BC%9A%E5%A4%A7%E5%9E%8B%E6%A6%82%E5%BF%B5%E6%A8%A1%E5%9E%8B%EF%BC%9A%E5%8F%A5%E5%AD%90%E8%A1%A8%E7%A4%BA%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1/" title="论文阅读四十九：大型概念模型：句子表示空间中的语言建模">论文阅读四十九：大型概念模型：句子表示空间中的语言建模</a><time datetime="2024-12-17T11:13:10.000Z" title="发表于 2024-12-17 19:13:10">2024-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%85%AB%EF%BC%9A%E5%85%8D%E8%AE%AD%E7%BB%83%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%A0%87%E7%AD%BE%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E7%9A%84%E5%8A%9B%E9%87%8F/" title="论文阅读四十八：免训练图神经网络和标签作为特征的力量"><img src="/images/tfgnn.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十八：免训练图神经网络和标签作为特征的力量"/></a><div class="content"><a class="title" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%85%AB%EF%BC%9A%E5%85%8D%E8%AE%AD%E7%BB%83%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%A0%87%E7%AD%BE%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E7%9A%84%E5%8A%9B%E9%87%8F/" title="论文阅读四十八：免训练图神经网络和标签作为特征的力量">论文阅读四十八：免训练图神经网络和标签作为特征的力量</a><time datetime="2024-12-17T11:09:52.000Z" title="发表于 2024-12-17 19:09:52">2024-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%9A3DGS-zip-3D%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/" title="论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述"><img src="/images/3dgszip.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述"/></a><div class="content"><a class="title" href="/2024/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%9A3DGS-zip-3D%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/" title="论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述">论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述</a><time datetime="2024-12-07T03:44:18.000Z" title="发表于 2024-12-07 11:44:18">2024-12-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%EF%BC%9A%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/" title="综述十：多模态模型"><img src="/images/mamba.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="综述十：多模态模型"/></a><div class="content"><a class="title" href="/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%EF%BC%9A%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/" title="综述十：多模态模型">综述十：多模态模型</a><time datetime="2024-12-01T11:04:52.000Z" title="发表于 2024-12-01 19:04:52">2024-12-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Serge Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>
<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>综述七：CNN模型 | Model The World</title><meta name="author" content="Serge Wang"><meta name="copyright" content="Serge Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Here’s a list of major CNN (Convolutional Neural Network) variants ordered by the time of their publication. CNNs have evolved over the years to improve accuracy, reduce computational costs, and adapt">
<meta property="og:type" content="article">
<meta property="og:title" content="综述七：CNN模型">
<meta property="og:url" content="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%83%EF%BC%9ACNN%E6%A8%A1%E5%9E%8B/">
<meta property="og:site_name" content="Model The World">
<meta property="og:description" content="Here’s a list of major CNN (Convolutional Neural Network) variants ordered by the time of their publication. CNNs have evolved over the years to improve accuracy, reduce computational costs, and adapt">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sergewang.github.io/images/mamba.png">
<meta property="article:published_time" content="2024-11-24T11:04:52.000Z">
<meta property="article:modified_time" content="2024-12-03T12:34:17.150Z">
<meta property="article:author" content="Serge Wang">
<meta property="article:tag" content="综述">
<meta property="article:tag" content="CNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sergewang.github.io/images/mamba.png"><link rel="shortcut icon" href="/img/newlogo.png"><link rel="canonical" href="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%83%EF%BC%9ACNN%E6%A8%A1%E5%9E%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IDUolVgWkW_cmu1mtW5hsxrrIjQfCHvq5hOTOkXeVNE"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9dcb7eb7a8a6225c2b1f242f3b0894bf";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XERFYF0N5K"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-XERFYF0N5K')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-XERFYF0N5K', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '综述七：CNN模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-03 20:34:17'
}</script></head><body><div id="web_bg" style="background-image: url(/img/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/newlogo.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">70</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">40</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/mamba.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/newlogo.png" alt="Logo"><span class="site-name">Model The World</span></a><a class="nav-page-title" href="/"><span class="site-name">综述七：CNN模型</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">综述七：CNN模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-24T11:04:52.000Z" title="发表于 2024-11-24 19:04:52">2024-11-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-03T12:34:17.150Z" title="更新于 2024-12-03 20:34:17">2024-12-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/CNN/">CNN</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Here’s a list of major <strong>CNN (Convolutional Neural Network)</strong> variants ordered by the time of their publication. CNNs have evolved over the years to improve accuracy, reduce computational costs, and adapt to different domains.</p>
<hr>
<h3 id="1-LeNet-1998">1. <strong>LeNet (1998)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: One of the first CNN architectures, designed for handwritten digit recognition (MNIST). It used convolutional layers followed by pooling and fully connected layers.</li>
<li><strong>Application</strong>: Digit classification.</li>
<li><strong>Notable Paper</strong>: Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition”, 1998.</li>
</ul>
<hr>
<h3 id="2-AlexNet-2012">2. <strong>AlexNet (2012)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: A deep CNN with 8 layers (5 convolutional and 3 fully connected layers), which demonstrated the power of CNNs by winning the 2012 ImageNet competition. It introduced ReLU activation functions and used GPU acceleration.</li>
<li><strong>Application</strong>: Image classification (ImageNet).</li>
<li><strong>Notable Paper</strong>: Alex Krizhevsky et al., “ImageNet Classification with Deep Convolutional Neural Networks”, 2012.</li>
</ul>
<hr>
<h3 id="3-ZfNet-2013">3. <strong>ZfNet (2013)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Improved upon AlexNet by reducing the size of the kernel and adding extra layers. It also used a deeper network with more filters.</li>
<li><strong>Application</strong>: Image classification (ImageNet).</li>
<li><strong>Notable Paper</strong>: Matthew Zeiler &amp; Rob Fergus, “Visualizing and Understanding Convolutional Networks”, 2013.</li>
</ul>
<hr>
<h3 id="4-VGGNet-2014">4. <strong>VGGNet (2014)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Introduced the use of very small (3x3) convolution filters and deep architectures (16–19 layers). Focused on using uniformity in the architecture, with consistent filter sizes.</li>
<li><strong>Application</strong>: Image classification (ImageNet).</li>
<li><strong>Notable Paper</strong>: Simonyan &amp; Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, 2014.</li>
</ul>
<hr>
<h3 id="5-GoogleNet-Inception-v1-2014">5. <strong>GoogleNet (Inception v1) (2014)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Used an inception module that allows the network to learn different kernel sizes simultaneously. Also introduced global average pooling to reduce overfitting.</li>
<li><strong>Application</strong>: Image classification (ImageNet).</li>
<li><strong>Notable Paper</strong>: Christian Szegedy et al., “Going Deeper with Convolutions”, 2014.</li>
</ul>
<hr>
<h3 id="6-ResNet-2015">6. <strong>ResNet (2015)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Introduced residual connections (skip connections) that allow the model to learn residuals instead of direct mappings. This solved the vanishing gradient problem and allowed for much deeper networks (up to 152 layers).</li>
<li><strong>Application</strong>: Image classification, object detection.</li>
<li><strong>Notable Paper</strong>: Kaiming He et al., “Deep Residual Learning for Image Recognition”, 2015.</li>
</ul>
<hr>
<h3 id="7-Inception-v2-and-v3-2015-2016">7. <strong>Inception v2 and v3 (2015-2016)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Further refinements to the original Inception network, including factorization ideas to reduce the number of parameters, and improvements in training techniques (e.g., batch normalization).</li>
<li><strong>Application</strong>: Image classification.</li>
<li><strong>Notable Paper</strong>: Christian Szegedy et al., “Rethinking the Inception Architecture for Computer Vision”, 2015.</li>
</ul>
<hr>
<h3 id="8-DenseNet-2016">8. <strong>DenseNet (2016)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Introduced dense connections where each layer is connected to every other layer in a feed-forward fashion, improving gradient flow and encouraging feature reuse.</li>
<li><strong>Application</strong>: Image classification, segmentation.</li>
<li><strong>Notable Paper</strong>: Gao Huang et al., “Densely Connected Convolutional Networks”, 2016.</li>
</ul>
<hr>
<h3 id="9-Xception-2017">9. <strong>Xception (2017)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Extreme version of Inception, which replaces standard convolutions with depthwise separable convolutions, improving efficiency while maintaining performance.</li>
<li><strong>Application</strong>: Image classification.</li>
<li><strong>Notable Paper</strong>: François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions”, 2017.</li>
</ul>
<hr>
<h3 id="10-MobileNet-2017">10. <strong>MobileNet (2017)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Designed for mobile and embedded devices, MobileNet uses depthwise separable convolutions to significantly reduce the number of parameters while maintaining performance.</li>
<li><strong>Application</strong>: Mobile and embedded vision tasks.</li>
<li><strong>Notable Paper</strong>: Andrew G. Howard et al., “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications”, 2017.</li>
</ul>
<hr>
<h3 id="11-U-Net-2015">11. <strong>U-Net (2015)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: A CNN architecture specifically designed for image segmentation tasks. U-Net uses a symmetric encoder-decoder structure with skip connections to better retain spatial information.</li>
<li><strong>Application</strong>: Medical image segmentation.</li>
<li><strong>Notable Paper</strong>: Olaf Ronneberger et al., “U-Net: Convolutional Networks for Biomedical Image Segmentation”, 2015.</li>
</ul>
<hr>
<h3 id="12-Mask-R-CNN-2017">12. <strong>Mask R-CNN (2017)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: An extension of Faster R-CNN for instance segmentation, Mask R-CNN adds a branch for predicting segmentation masks alongside object detection.</li>
<li><strong>Application</strong>: Object detection and segmentation.</li>
<li><strong>Notable Paper</strong>: Kaiming He et al., “Mask R-CNN”, 2017.</li>
</ul>
<hr>
<h3 id="13-EfficientNet-2019">13. <strong>EfficientNet (2019)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Introduced a systematic way to scale CNNs by balancing network depth, width, and resolution. EfficientNet achieves state-of-the-art accuracy with fewer parameters.</li>
<li><strong>Application</strong>: Image classification.</li>
<li><strong>Notable Paper</strong>: Mingxing Tan &amp; Quoc V. Le, “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”, 2019.</li>
</ul>
<hr>
<h3 id="14-RegNet-2020">14. <strong>RegNet (2020)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Introduced a simple design space for designing networks, RegNet emphasizes scalability and simplicity, resulting in models with better performance and efficiency.</li>
<li><strong>Application</strong>: Image classification, detection tasks.</li>
<li><strong>Notable Paper</strong>: Radoslav M. K. et al., “RegNet: A Family of Efficient and Scalable Convolutional Networks”, 2020.</li>
</ul>
<hr>
<h3 id="15-Vision-Transformers-ViT-2020">15. <strong>Vision Transformers (ViT) (2020)</strong></h3>
<ul>
<li><strong>Key Idea</strong>: Introduced the transformer architecture to vision tasks, showing that transformers can outperform CNNs when trained on large datasets. ViT divides images into patches and treats them as sequences.</li>
<li><strong>Application</strong>: Image classification.</li>
<li><strong>Notable Paper</strong>: Alexey Dosovitskiy et al., “Image Transformer”, 2020.</li>
</ul>
<hr>
<h3 id="Summary-of-CNN-Variants">Summary of CNN Variants:</h3>
<ol>
<li><strong>LeNet</strong> (1998)</li>
<li><strong>AlexNet</strong> (2012)</li>
<li><strong>ZfNet</strong> (2013)</li>
<li><strong>VGGNet</strong> (2014)</li>
<li><strong>GoogleNet (Inception v1)</strong> (2014)</li>
<li><strong>ResNet</strong> (2015)</li>
<li><strong>Inception v2/v3</strong> (2015-2016)</li>
<li><strong>DenseNet</strong> (2016)</li>
<li><strong>Xception</strong> (2017)</li>
<li><strong>MobileNet</strong> (2017)</li>
<li><strong>U-Net</strong> (2015)</li>
<li><strong>Mask R-CNN</strong> (2017)</li>
<li><strong>EfficientNet</strong> (2019)</li>
<li><strong>RegNet</strong> (2020)</li>
<li><strong>Vision Transformers (ViT)</strong> (2020)</li>
</ol>
<p>These CNN variants reflect the continuous advancements in architecture design, from the early simple models like LeNet to more sophisticated networks like EfficientNet and Vision Transformers. Each variant introduced innovations that addressed specific limitations of earlier models, leading to improvements in accuracy, efficiency, and the ability to scale to larger datasets and more complex tasks.</p>
<h2 id="模型">模型</h2>
<h3 id="An-Introduction-to-Convolutional-Neural-Networks-15-11">An Introduction to Convolutional Neural Networks (15/11)</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.08458">论文地址</a><br>
核心思想：CNN主要用于解决困难的图像驱动模式识别任务，其精确而简单的架构提供了一种简化的ANN入门方法。本文简要介绍了CNN，讨论了最近发表的论文和开发这些出色的图像识别模型的新技术。<br>
<a href="CNN.png"></a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SergeWang.github.io">Serge Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%83%EF%BC%9ACNN%E6%A8%A1%E5%9E%8B/">https://sergewang.github.io/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%83%EF%BC%9ACNN%E6%A8%A1%E5%9E%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SergeWang.github.io" target="_blank">Model The World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a><a class="post-meta__tags" href="/tags/CNN/">CNN</a></div><div class="post-share"><div class="social-share" data-image="/images/mamba.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%BA%8C%EF%BC%9ADiTs%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述二：DiTs及其变体"><img class="cover" src="/images/SIT.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">综述二：DiTs及其变体</div></div><div class="info-2"><div class="info-item-1">**DiT（去噪扩散Transformer模型）**是结合了Transformer架构和扩散模型的一类生成模型，特别专注于在扩散框架内的去噪过程。扩散模型通过逐步添加和去除噪声的过程来建模复杂的分布，近年来在生成任务中非常流行。 下面是一些著名的DiT变种，它们在不同方面扩展了原始的DiT模型： 1. DiT（原始版本）  关键特性：原始的DiT模型将Transformer架构与去噪扩散过程结合，利用Transformer的注意力机制改进生成质量和训练的可扩展性。 目的：生成高质量的图像，并与传统的卷积神经网络相比，提高训练效率。  2. DiT++（增强版DiT）  关键特性：DiT++是在原始DiT基础上进行增强的版本，可能包括模型架构的改进、训练方法的优化或额外的正则化技术。 目的：通过改进Transformer架构和扩散过程中的噪声调度，提升生成稳定性和性能。  3....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%BA%94%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%85%B6%E5%88%86%E7%B1%BB/" title="综述五：强化学习及其分类"><img class="cover" src="/images/mamba.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">综述五：强化学习及其分类</div></div><div class="info-2"><div class="info-item-1">强化学习的变种 强化学习（Reinforcement Learning，简称RL）是一种机器学习方法，其中代理（Agent）通过与环境的交互来学习做出决策。代理的目标是通过采取适当的行动，最大化长期累积的奖励。强化学习是一个广泛的领域，许多不同的变种和算法已被开发出来，以解决学习、探索和决策等不同方面的问题。 以下是强化学习的主要变种及其子类别：  1. 无模型 vs 有模型强化学习  无模型强化学习（Model-Free RL）：代理不构建或使用环境的动态模型，而是直接通过与环境的交互来学习。  示例：  Q学习（Q-Learning，包括表格化和深度Q学习） 策略梯度方法（例如，REINFORCE） Actor-Critic方法（例如A3C，PPO）     有模型强化学习（Model-Based RL）：代理试图学习环境的转移动态和奖励函数，并利用这些模型来做出更有信息量的决策。  示例：  Dyna-Q（Q学习与规划结合） World Models 蒙特卡洛树搜索（MCTS）       2....</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%80%EF%BC%9ATransformer%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述一：Transformer及其变体"><img class="cover" src="/images/transformer.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述一：Transformer及其变体</div></div><div class="info-2"><div class="info-item-1">Transformer架构，因其自注意力机制而闻名，能够让模型根据输入序列中不同的标记之间的关系进行加权。这种机制消除了循环神经网络的需求，使得训练变得更加高效。自从2017年原始Transformer的提出以来，已经出现了多个变种，旨在优化性能、扩展其适用范围或解决一些挑战。以下是一些著名的Transformer变种，特别是那些专注于自注意力机制的： 1. 原始Transformer (Vanilla Transformer)  关键特性：原始Transformer架构包括一个编码器-解码器结构，采用自注意力机制。 目的：消除了递归神经网络的需求，使得模型训练更加并行化和高效。 自注意力机制：多头自注意力，通过计算输入序列中所有标记之间的关系来决定重要性。  2. BERT（双向编码器表示）  关键特性：BERT只使用Transformer的编码器部分，采用双向自注意力机制来捕获标记的左右上下文。 目的：通过掩蔽语言模型（MLM）进行预训练，并可以通过微调来提升在下游NLP任务中的表现。  3....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%89%EF%BC%9A%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/" title="综述三：持续学习及其方法"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述三：持续学习及其方法</div></div><div class="info-2"><div class="info-item-1">持续学习（Continual Learning，CL），也称为终身学习（Lifelong Learning），指的是模型能够从持续不断的数据流中学习，并随着时间的推移不断适应和获得新知识，而不会遗忘先前学习的内容。持续学习面临的一个主要挑战是灾难性遗忘（Catastrophic Forgetting），即在学习新任务时，模型容易遗忘之前学习的任务。 为了应对这些挑战，提出了多种方法，这些方法可以根据它们如何处理遗忘、如何存储知识以及如何使用新数据进行分类。 下面是主要的持续学习方法，按它们所采用的主要策略进行组织： 1....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9ALLM/" title="综述九：LLM"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述九：LLM</div></div><div class="info-2"><div class="info-item-1">提示（Prompts） Large Lanuage Models Can Self-Improve in Long-context Reasoning (24/10) 论文地址 核心思想：大型语言模型（LLM）在处理长上下文方面取得了实质性进展，但在长上下文推理方面仍存在困难。现有的方法通常涉及使用合成数据对LLM进行微调，这取决于人类专家的注释或GPT-4等高级模型，从而限制了进一步的进步。为了解决这个问题，研究了LLM在长上下文推理中自我改进的潜力，并提出了专门为此目的设计的SEALONG方法。这种方法很简单：对每个问题的多个输出进行采样，用最小贝叶斯风险对其进行评分，然后根据这些输出进行监督微调或偏好优化。在几个领先的LLM上进行的广泛实验证明了SEALONG的有效性，Llama-3.1-8B-Instruct的绝对提高了4.2分。此外，与依赖于人类专家或高级模型生成的数据的先前方法相比，SEALONG实现了更优的性能。我们预计，这项工作将为长期情景下的自我提升技术开辟新的途径，这对LLM的持续发展至关重要。 </div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%BA%8C%EF%BC%9ADiTs%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述二：DiTs及其变体"><img class="cover" src="/images/SIT.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述二：DiTs及其变体</div></div><div class="info-2"><div class="info-item-1">**DiT（去噪扩散Transformer模型）**是结合了Transformer架构和扩散模型的一类生成模型，特别专注于在扩散框架内的去噪过程。扩散模型通过逐步添加和去除噪声的过程来建模复杂的分布，近年来在生成任务中非常流行。 下面是一些著名的DiT变种，它们在不同方面扩展了原始的DiT模型： 1. DiT（原始版本）  关键特性：原始的DiT模型将Transformer架构与去噪扩散过程结合，利用Transformer的注意力机制改进生成质量和训练的可扩展性。 目的：生成高质量的图像，并与传统的卷积神经网络相比，提高训练效率。  2. DiT++（增强版DiT）  关键特性：DiT++是在原始DiT基础上进行增强的版本，可能包括模型架构的改进、训练方法的优化或额外的正则化技术。 目的：通过改进Transformer架构和扩散过程中的噪声调度，提升生成稳定性和性能。  3....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%BA%94%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%85%B6%E5%88%86%E7%B1%BB/" title="综述五：强化学习及其分类"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述五：强化学习及其分类</div></div><div class="info-2"><div class="info-item-1">强化学习的变种 强化学习（Reinforcement Learning，简称RL）是一种机器学习方法，其中代理（Agent）通过与环境的交互来学习做出决策。代理的目标是通过采取适当的行动，最大化长期累积的奖励。强化学习是一个广泛的领域，许多不同的变种和算法已被开发出来，以解决学习、探索和决策等不同方面的问题。 以下是强化学习的主要变种及其子类别：  1. 无模型 vs 有模型强化学习  无模型强化学习（Model-Free RL）：代理不构建或使用环境的动态模型，而是直接通过与环境的交互来学习。  示例：  Q学习（Q-Learning，包括表格化和深度Q学习） 策略梯度方法（例如，REINFORCE） Actor-Critic方法（例如A3C，PPO）     有模型强化学习（Model-Based RL）：代理试图学习环境的转移动态和奖励函数，并利用这些模型来做出更有信息量的决策。  示例：  Dyna-Q（Q学习与规划结合） World Models 蒙特卡洛树搜索（MCTS）       2....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9AMamba%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述九：Mamba及其变体"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述九：Mamba及其变体</div></div><div class="info-2"><div class="info-item-1">SSM模型 Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (20/06) 论文地址 核心思想：将自注意表示为核特征图的线性点积，并使用矩阵乘法的结合属性将复杂度从 O(N2)\mathcal{O}(N^2)O(N2) 减少到 O(N)\mathcal{O}(N)O(N) ，其中N是序列长度。我们证明，这个公式允许迭代实现，大大加速了自回归Transformers，并揭示了它们与循环神经网络的关系。(一种涉及循环的自我注意近似，可以看作是退化的线性SSM)  Hungry Hungry Hippos: Towards Language Modeling with State Space Models...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/newlogo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Serge Wang</div><div class="author-info-description">今日事，今日毕</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">70</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">40</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SergeWang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SergeWang" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sergew027@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-LeNet-1998"><span class="toc-number">1.</span> <span class="toc-text">1. LeNet (1998)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-AlexNet-2012"><span class="toc-number">2.</span> <span class="toc-text">2. AlexNet (2012)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-ZfNet-2013"><span class="toc-number">3.</span> <span class="toc-text">3. ZfNet (2013)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-VGGNet-2014"><span class="toc-number">4.</span> <span class="toc-text">4. VGGNet (2014)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-GoogleNet-Inception-v1-2014"><span class="toc-number">5.</span> <span class="toc-text">5. GoogleNet (Inception v1) (2014)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-ResNet-2015"><span class="toc-number">6.</span> <span class="toc-text">6. ResNet (2015)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Inception-v2-and-v3-2015-2016"><span class="toc-number">7.</span> <span class="toc-text">7. Inception v2 and v3 (2015-2016)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-DenseNet-2016"><span class="toc-number">8.</span> <span class="toc-text">8. DenseNet (2016)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Xception-2017"><span class="toc-number">9.</span> <span class="toc-text">9. Xception (2017)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-MobileNet-2017"><span class="toc-number">10.</span> <span class="toc-text">10. MobileNet (2017)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-U-Net-2015"><span class="toc-number">11.</span> <span class="toc-text">11. U-Net (2015)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Mask-R-CNN-2017"><span class="toc-number">12.</span> <span class="toc-text">12. Mask R-CNN (2017)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-EfficientNet-2019"><span class="toc-number">13.</span> <span class="toc-text">13. EfficientNet (2019)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-RegNet-2020"><span class="toc-number">14.</span> <span class="toc-text">14. RegNet (2020)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Vision-Transformers-ViT-2020"><span class="toc-number">15.</span> <span class="toc-text">15. Vision Transformers (ViT) (2020)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-of-CNN-Variants"><span class="toc-number">16.</span> <span class="toc-text">Summary of CNN Variants:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number"></span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#An-Introduction-to-Convolutional-Neural-Networks-15-11"><span class="toc-number">1.</span> <span class="toc-text">An Introduction to Convolutional Neural Networks (15&#x2F;11)</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%9A3DGS-zip-3D%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/" title="论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述"><img src="/images/3dgszip.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述"/></a><div class="content"><a class="title" href="/2024/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%9A3DGS-zip-3D%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/" title="论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述">论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述</a><time datetime="2024-12-07T03:44:18.000Z" title="发表于 2024-12-07 11:44:18">2024-12-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%85%AD%EF%BC%9AStuffedMamba%EF%BC%9A%E5%9F%BA%E4%BA%8ERNN%E7%9A%84%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1%E7%9A%84%E7%8A%B6%E6%80%81%E5%B4%A9%E6%BA%83%E5%92%8C%E7%8A%B6%E6%80%81%E5%AE%B9%E9%87%8F/" title="论文阅读四十六：StuffedMamba：基于RNN的长上下文建模的状态崩溃和状态容量"><img src="/images/QGS.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十六：StuffedMamba：基于RNN的长上下文建模的状态崩溃和状态容量"/></a><div class="content"><a class="title" href="/2024/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%85%AD%EF%BC%9AStuffedMamba%EF%BC%9A%E5%9F%BA%E4%BA%8ERNN%E7%9A%84%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1%E7%9A%84%E7%8A%B6%E6%80%81%E5%B4%A9%E6%BA%83%E5%92%8C%E7%8A%B6%E6%80%81%E5%AE%B9%E9%87%8F/" title="论文阅读四十六：StuffedMamba：基于RNN的长上下文建模的状态崩溃和状态容量">论文阅读四十六：StuffedMamba：基于RNN的长上下文建模的状态崩溃和状态容量</a><time datetime="2024-12-01T09:32:44.000Z" title="发表于 2024-12-01 17:32:44">2024-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%BA%94%EF%BC%9A%E6%B5%81%E5%BC%8F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="论文阅读四十五：流式深度强化学习"><img src="/images/QGS.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十五：流式深度强化学习"/></a><div class="content"><a class="title" href="/2024/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%BA%94%EF%BC%9A%E6%B5%81%E5%BC%8F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="论文阅读四十五：流式深度强化学习">论文阅读四十五：流式深度强化学习</a><time datetime="2024-12-01T04:03:19.000Z" title="发表于 2024-12-01 12:03:19">2024-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%9B%9B%EF%BC%9A%E7%94%A8%E4%BA%8E%E9%AB%98%E6%95%88%E7%BB%86%E8%87%B4%E8%A1%A8%E9%9D%A2%E9%87%8D%E5%BB%BA%E7%9A%84%E4%BA%8C%E6%AC%A1%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85/" title="论文阅读四十四：用于高效细致曲面重建的二次高斯泼溅"><img src="/images/QGS.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十四：用于高效细致曲面重建的二次高斯泼溅"/></a><div class="content"><a class="title" href="/2024/11/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E5%9B%9B%EF%BC%9A%E7%94%A8%E4%BA%8E%E9%AB%98%E6%95%88%E7%BB%86%E8%87%B4%E8%A1%A8%E9%9D%A2%E9%87%8D%E5%BB%BA%E7%9A%84%E4%BA%8C%E6%AC%A1%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85/" title="论文阅读四十四：用于高效细致曲面重建的二次高斯泼溅">论文阅读四十四：用于高效细致曲面重建的二次高斯泼溅</a><time datetime="2024-11-27T02:15:57.000Z" title="发表于 2024-11-27 10:15:57">2024-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%89%EF%BC%9A%E6%B5%8B%E8%AF%95%E6%97%B6%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A0%EF%BC%9ALLMs%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%BE%AE%E8%B0%83/" title="论文阅读四十三：测试时高效学习：LLMs的主动微调"><img src="/images/SIFT.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四十三：测试时高效学习：LLMs的主动微调"/></a><div class="content"><a class="title" href="/2024/11/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%89%EF%BC%9A%E6%B5%8B%E8%AF%95%E6%97%B6%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A0%EF%BC%9ALLMs%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%BE%AE%E8%B0%83/" title="论文阅读四十三：测试时高效学习：LLMs的主动微调">论文阅读四十三：测试时高效学习：LLMs的主动微调</a><time datetime="2024-11-26T13:05:57.000Z" title="发表于 2024-11-26 21:05:57">2024-11-26</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Serge Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>
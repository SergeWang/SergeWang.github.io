<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调 | Model The World</title><meta name="author" content="Serge Wang"><meta name="copyright" content="Serge Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="摘要 高效的微调对于使大型语言模型（LLM）适应下游任务至关重要。然而，在不同的模型上实现这些方法需要付出巨大的努力。我们介绍LLAMAFACTORY，这是一个整合了一套尖端高效培训方法的统一框架。它提供了一种解决方案，可以灵活地定制100多个LLM的微调，而无需通过内置的web UI LLAMABOARD进行编码。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。它已发布于 ht">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调">
<meta property="og:url" content="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/">
<meta property="og:site_name" content="Model The World">
<meta property="og:description" content="摘要 高效的微调对于使大型语言模型（LLM）适应下游任务至关重要。然而，在不同的模型上实现这些方法需要付出巨大的努力。我们介绍LLAMAFACTORY，这是一个整合了一套尖端高效培训方法的统一框架。它提供了一种解决方案，可以灵活地定制100多个LLM的微调，而无需通过内置的web UI LLAMABOARD进行编码。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。它已发布于 ht">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/architecture.png">
<meta property="article:published_time" content="2024-11-03T11:40:36.000Z">
<meta property="article:modified_time" content="2024-11-03T13:46:38.593Z">
<meta property="article:author" content="Serge Wang">
<meta property="article:tag" content="论文阅读">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/architecture.png"><link rel="shortcut icon" href="/img/newlogo.png"><link rel="canonical" href="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IDUolVgWkW_cmu1mtW5hsxrrIjQfCHvq5hOTOkXeVNE"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9dcb7eb7a8a6225c2b1f242f3b0894bf";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XERFYF0N5K"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-XERFYF0N5K')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-XERFYF0N5K', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-03 21:46:38'
}</script></head><body><div id="web_bg" style="background-image: url(/img/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/newlogo.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(2024/11/03/论文阅读七：LLAMAFACTORY：100多种语言模型的统一高效微调/architecture.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/newlogo.png" alt="Logo"><span class="site-name">Model The World</span></a><a class="nav-page-title" href="/"><span class="site-name">论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-03T11:40:36.000Z" title="发表于 2024-11-03 19:40:36">2024-11-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-03T13:46:38.593Z" title="更新于 2024-11-03 21:46:38">2024-11-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>摘要</h1>
<p>高效的微调对于使大型语言模型（LLM）适应下游任务至关重要。然而，在不同的模型上实现这些方法需要付出巨大的努力。我们介绍LLAMAFACTORY，这是一个整合了一套尖端高效培训方法的统一框架。它提供了一种解决方案，可以灵活地定制100多个LLM的微调，而无需通过内置的web UI LLAMABOARD进行编码。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。它已发布于 <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a> ，并获得了25000多颗星和3000个fork。</p>
<h1>引言</h1>
<p>大型语言模型（LLM）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.19341">赵等人，2023</a>）具有显著的推理能力，并赋予了广泛的应用，如问答（<a target="_blank" rel="noopener" href="https://aclanthology.org/2023.emnlp-main.228.pdf">Jiang等人，2023b</a>）、机器翻译（<a target="_blank" rel="noopener" href="https://aclanthology.org/2023.emnlp-main.1036.pdf">Wang等人，2023c</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.02426">Jiao等人，2023a</a>）和信息提取（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.16040">Jiao等人（2023b）</a>）。随后，大量LLM被开发出来，并可通过开源社区访问。例如，Hugging Face的开放式LLM排行榜（<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">Beeching等人，2023</a>）拥有5000多个模特，为寻求利用LLM力量的个人提供了便利。</p>
<p>用有限的资源微调大量参数成为使LLM适应下游任务的主要挑战。一种流行的解决方案是高效的微调（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751">Houlsby等人，2019</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">Hu等人，2022</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">Dettmers等人，2023</a>），这降低了LLM在适应各种任务时的训练成本。然而，社区为高效的微调提供了各种方法，缺乏一个系统框架来适应和统一这些方法到不同的LLM，并为用户定制提供友好的界面。</p>
<p>为了解决上述问题，我们开发了LLAMAFACTORY，这是一个使LLM微调民主化的框架。它通过可扩展的模块统一了各种高效的微调方法，能够以最少的资源和高吞吐量对数百个LLM进行微调。此外，它简化了常用的训练方法，包括生成预训练（<a target="_blank" rel="noopener" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Radford等人，2018</a>）、监督微调（SFT）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.01652">Wei等人，2022</a>）、基于人类反馈的强化学习（RLHF）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Ouyang等人，2022年</a>）和直接偏好优化（DPO）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18290">Rafailov等人，2023年</a>）。用户可以利用命令行或web界面来定制和微调他们的LLM，只需很少或根本不需要编码工作。</p>
<p>LLAMAFACTORY由三个主要模块组成：模型加载器（Model Loader）、数据工作者（Data Worker）和训练器（Trainer）。我们最大限度地减少了这些模块对特定模型和数据集的依赖，使框架能够灵活地扩展到数百个模型和数据集中。具体来说，我们首先建立一个模型注册表，模型加载器可以通过识别精确的层将适配器精确地连接到预训练的模型。然后，我们开发了一个数据描述规范，允许数据工作者通过对齐相应的列来收集数据集。此外，我们提供最先进的高效微调方法的即插即用实现，使培训师能够通过替换默认方法来激活。我们的设计允许这些模块在不同的训练方法中重复使用，从而显著降低了集成成本。</p>
<p>LLAMAFACTORY使用PyTorch实现（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.01703">Paszke等人，2019</a>），并从开源库中受益匪浅，如Transformers（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.03771">Wolf等人，2020</a>）、PEFT（<a href="(https://github.com/huggingface/peft)">Mangrulkar等人，2022</a>）和TRL（<a target="_blank" rel="noopener" href="https://github.com/huggingface/trl">von Werra等人，2020年</a>）。在此基础上，我们提供了一个具有更高抽象级别的开箱即用框架。此外，我们使用Gradio（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.02569">Abid等人，2019</a>）构建了LLAMABOARD，可以在不需要编码的情况下对LLM进行微调。</p>
<p>LLAMAFACTORY是根据Apache-2.0许可证开源的。它已经在GitHub上获得了25000多颗星和3000个分支，并且在Hugging Face Hub1的LLAMAFACTORY上构建了数百个开源模型。例如，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.02715">Truong等人（2024）</a>基于LLAMAFACTORY构建了GemSUra-7B，揭示了Gemma（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.08295">Mesnard等人，2024</a>）的跨语言能力。此外，数十项研究利用我们的框架来探索LLM（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.02223">Wang等人，2023a</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.10092">Yu等人，2023</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.11746">Bhardwaj等人，2024</a>）。</p>
<h1>相关工作</h1>
<p>随着对微调LLM的需求迅速增加，已经开发了许多使LLM适应特定目的的框架。LLaMAAdapter（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.16199">Zhang等人，2024</a>）使用<strong>零初始化注意力</strong>有效地微调Llama模型（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13971">Touvron等人，2023a</a>）。FastChat（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.05685">Zheng等人，2023</a>）是一个专注于训练和评估LLM以完成聊天的框架。LitGPT（<a target="_blank" rel="noopener" href="https://github.com/Lightning-AI/litgpt">AI，2023</a>）提供生成模型的实现，并支持各种训练方法。Open Instruct（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.04751">Wang等人，2023d</a>）提供了训练指令模型的配方。Colossal AI（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.14883">Li等人，2023b</a>）采用先进的并行策略进行分布式训练。LMFlow（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.12420">Diao等人，2024</a>）支持为专业领域或任务训练LLM。GPT4All（<a target="_blank" rel="noopener" href="https://github.com/nomic-ai/gpt4all">Anand等人，2023</a>）允许LLM在消费设备上运行，同时还提供微调功能。与现有的竞争框架相比，LLAMAFACTORY支持更广泛的高效微调技术和培训方法。我们在表1中列出了代表性框架中的特征。<br>
<img src="frameworks.png" alt="LLAMAFACTORY中的功能与常用的微调LLM框架的比较。"></p>
<h1>高效的微调技术</h1>
<p>高效的LLM微调技术可分为两大类：侧重于优化的技术和旨在计算的技术。高效优化技术的主要目标是微调LLM的参数，同时将成本降至最低。另一方面，高效的计算方法寻求减少LLM中所需计算的时间或空间。LLAMAFACTORY中包含的方法列于表2中。我们将在以下部分介绍这些高效的微调技术，并展示将其纳入我们的框架所实现的显著效率提高。<br>
<img src="technique.png" alt="LLAMAFACTORY中的微调技术之间的兼容性。"></p>
<h2 id="高效优化">高效优化</h2>
<p>首先，我们概述了LLAMAFACTORY中使用的高效优化技术。冻结调谐方法（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751">Houlsby等人，2019</a>）涉及<strong>冻结大多数参数</strong>，同时微调解码器层一小部分中的其余参数。另一种称为<strong>梯度低阶投影</strong>（GaLore）的方法（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.03507">赵等人，2024</a>）将梯度投影到低维空间中，以高效记忆的方式促进全参数学习。同样，BAdam（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.02827">Luo等人，2024</a>）利用<strong>块坐标下降</strong>（BCD）有效地优化了广泛的参数。相反，低秩自适应（LoRA）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">Hu等人，2022</a>）方法<strong>冻结所有预训练的权重</strong>，并将一对可训练的低秩矩阵引入指定层。当与量化相结合时，这种方法被称为QLoRA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">Dettmers等人，2023</a>），它还减少了内存使用。DoRA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.09353">Liu等人，2024</a>）将预先训练的<strong>权重分解为幅度和方向分量</strong>，并更新方向分量以提高性能。LoRA+（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.12354">Hayou等人，2024</a>）被提出以克服LoRA的次优性。PiSSA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.02948">Meng等人，2024</a>）<strong>使用预训练权重的主成分初来始化适配器</strong>，以实现更快的收敛。</p>
<h2 id="高效计算">高效计算</h2>
<p>在LLAMAFACTORY中，我们集成了一系列高效计算技术。常用的技术包括<strong>混合精度训练</strong>（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.03740">Micikevicius等人，2018</a>）和<strong>激活检查点</strong>（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.06174">Chen等人，2016</a>）。通过对注意力层的输入输出（IO）费用的检查，flash attention（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135">Dao等人，2022</a>）引入了一种硬件友好的方法来增强注意力计算。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 注意力（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.12307">Chen等人，2024b</a>）通过转移稀疏注意力来应对扩展上下文的挑战，从而减少了微调长上下文LLM时的内存使用。各种量化策略（<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html">Dettmers等人，2022a</a>；<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=tcbBPnfwxS">Frantar等人，2023</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.00978">Lin等人，2023</a>，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.06118">Egiazarian等人，2024</a>）通过利用较低精度的权重表示来降低大型语言模型（LLM）中的内存需求。然而，量化模型的微调仅限于基于适配器的技术，如LoRA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">Hu等人，2022</a>）。Unsloth（<a target="_blank" rel="noopener" href="https://github.com/unslothai/unsloth">Han和Han，2023</a>）采用Triton（<a target="_blank" rel="noopener" href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Tillet等人，2019</a>）来实现LoRA的反向传播，这减少了梯度下降过程中的浮点运算（FLOP），并加快了LoRA训练。</p>
<p>LLAMAFACTORY将这些技术无缝结合成一个有凝聚力的结构，以提高LLM微调的效率。这导致内存占用从混合精度训练期间的每个参数18个字节（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.03740">Micikevicius等人，2018</a>）或半精度训练中的每个参数8个字节（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.05100">Le Scao等人，2022</a>）减少到每个参数仅0.6个字节。后续章节将对LLAMAFACTORY中的组件进行进一步阐述。</p>
<h1>LLAMAFACTORY框架</h1>
<p>LLAMAFACTORY由三个主要模块组成：模型加载器、数据工作者和训练器。模型加载器操纵各种模型架构进行微调，支持大型语言模型（LLM）和视觉语言模型（VLM）。Data Worker通过精心设计的管道处理来自不同任务的数据，支持单轮和多轮对话。培训师将高效的微调技术应用于不同的培训方法，支持预培训、指令调整和偏好优化。除此之外，LLAMABOARD提供了一个友好的视觉界面来访问这些模块，使用户能够无代码地配置和启动单个LLM微调实例，并同步监控训练状态。我们在图1中说明了这些模块与LLAMAFACTORY的整体架构之间的关系。</p>
<p><img src="architecture.png" alt="LLAMAFACTORY的架构"></p>
<h2 id="模型加载器">模型加载器</h2>
<p>本节首先介绍了模型加载器中的四个组件：模型初始化、模型拼接、模型量化和适配器连接，然后描述了我们通过在微调过程中处理参数浮点精度来适应各种设备的方法。</p>
<p><strong>模型初始化</strong>我们利用Transformers的自动分类（Wolf等人，2020）来加载预训练模型并初始化参数。具体来说，我们使用AutoModelForVision2Seq类加载视觉语言模型，而其余的则使用AutoModelForCausalLM类加载。tokenizer与模型一起使用AutoTokenizer类加载。在tokenizer的词汇量超过嵌入层容量的情况下，我们调整层的大小，并使用噪声均值初始化来初始化新参数。为了确定RoPE缩放的缩放因子（Chen等人，2023），我们将其计算为最大输入序列长度与模型上下文长度的比率。</p>
<p><strong>模型拼接</strong>为了实现S2注意力，我们使用猴子补丁来代替模型的前向计算。然而，我们使用本地类来启用flash注意力，因为自Transformers 4.34.0以来，它得到了广泛的支持。为了防止动态层的过度划分，我们在DeepSpeed ZeRO第三阶段优化MoE模型时，将专家混合（MoE）块设置为叶子模块（Rasley等人，2020）。</p>
<p><strong>模型量化</strong>可以通过bitsandbytes库（Dettmers，2021）使用LLM.int8（Dettmer等人，2022a）将模型动态量化为8位或4位。对于4位量化，我们使用双量化和4位正常浮点数作为QLoRA（Dettmers等人，2023）。我们还支持对训练后量化（PTQ）方法量化的模型进行微调，包括GPTQ（Frantar等人，2023）、AWQ（Lin等人，2023年）和AQLM（Egiazarian等人，2024年）。请注意，我们不能直接微调量化权重；因此，量化模型仅与基于适配器的方法兼容。</p>
<p><strong>适配器连接</strong>我们通过遍历模型层自动识别要连接适配器的适当层。低秩适配器连接到所有线性层，以实现更好的收敛，如（Dettmers等人，2023）所述。PEFT（Mangrulkar等人，2022）库提供了一种极其方便的方法来实现基于适配器的方法，如LoRA（Hu等人，2022年）、rsLoRA（Kalajdzievski，2023年）、DoRA（Liu等人，2024年）和PiSSA（Meng等人，2024年）。我们用Unsloth（Han和Han，2023）的反向计算代替，以加速训练。为了执行基于人类反馈的强化学习（RLHF），在变换器模型的顶部附加一个值头层（value head layer)，将每个token的表示映射到标量。</p>
<p><strong>精度自适应</strong>我们根据计算设备的能力处理预训练模型的浮点精度。对于NVIDIA GPU，如果计算能力为8.0或更高，我们采用bfloat16精度。否则，采用float16。此外，我们为Ascend NPU和AMD GPU采用float16，为非CUDA设备采用float32。在混合精度训练中，我们将所有可训练参数设置为float32，以提高训练稳定性。然而，在半精度训练中，我们将可训练参数保留为bfloat16。</p>
<h2 id="数据工作者">数据工作者</h2>
<p>我们开发了一个数据处理管道，包括数据集加载、数据集对齐、数据集合并和数据集预处理。它将不同任务的数据集标准化为统一的格式，使我们能够在各种格式的数据集上微调模型。</p>
<p><strong>数据集加载</strong>我们利用Datasets（Lhoest等人，2021）库加载数据，这允许用户从Hugging Face Hub加载远程数据集，或通过脚本或文件读取本地数据集。Datasets库显著降低了数据处理过程中的内存开销，并使用Arrow加速了样本查询（Apache，2016）。默认情况下，整个数据集被下载到本地磁盘。然而，如果数据集太大而无法存储，我们的框架会提供数据流来迭代它，而无需下载。</p>
<p><strong>数据集对齐</strong>为了统一数据集格式，我们设计了一个数据描述规范来表征数据集的结构。例如，羊驼数据集有三列：指令、输入和输出（Taori等人，2023）。我们根据数据描述规范将数据集转换为与各种任务兼容的标准结构。数据集结构的一些示例如表3所示。</p>
<p><strong>数据集合并</strong>统一的数据集结构为合并多个数据集提供了一种有效的方法。对于非流模式的数据集，我们只需在训练期间对数据集进行混洗之前将它们连接起来。然而，在流模式下，简单地连接数据集会阻碍数据洗牌。因此，我们提供了交替读取不同数据集数据的方法。</p>
<p><strong>数据集预处理</strong>LLAMAFACTORY旨在微调文本生成模型，主要用于聊天完成。聊天模板是这些模型中的关键组成部分，因为它与这些模型的指令遵循能力高度相关。因此，我们提供了数十个聊天模板，可以根据模型类型自动选择。我们使用标记器应用聊天模板后对句子进行编码。默认情况下，我们只计算完井损失，而忽略提示（Taori等人，2023）。可选地，我们可以利用序列打包（Krell等人，2021）来减少训练时间，这在执行生成预训练时会自动启用。</p>
<h2 id="训练器">训练器</h2>
<p><strong>高效训练</strong>我们通过替换默认组件，将最先进的高效微调方法，包括LoRA+（Hayou et al.，2024）、GaLore（赵et al.，424）和BAdam（Luo et al.，024）集成到培训师中。这些微调方法独立于培训师，使其易于应用于各种任务。我们使用Transformers（Wolf等人，2020）的培训师进行预培训和SFT，同时采用TRL（von Werra等人，2020年）的培训员进行RLHF和DPO。我们还包括来自TRL库的高级偏好优化方法的培训师，如KTO（Ethayarajh等人，2024）和ORPO（Hong等人，2024）。利用定制的数据整理器来区分各种培训方法的培训师。为了匹配训练器对偏好数据的输入格式，我们在一批中构建2n个样本，其中前n个样本是被选中的样本，最后n个样本则是被拒绝的样本。</p>
<p><strong>模型共享RLHF</strong>允许在消费设备上进行RLHF训练对于LLM微调的民主化至关重要。然而，这很困难，因为RLHF训练需要四种不同的模型。为了解决这个问题，我们提出了模型共享RLHF，使整个RLHF训练不超过一个预训练模型。具体来说，我们首先使用奖励建模的目标函数训练一个适配器和一个值头，使模型能够计算奖励分数。然后，我们初始化另一个适配器和值头，并用PPO算法对其进行训练（Ouyang等人，2022）。在训练过程中，适配器和价值头通过PEFT的set_adapter和disable_adapter方法动态切换（Mangrulkar等人，2022），允许单个预训练模型同时用作策略模型、价值模型、参考模型和奖励模型。据我们所知，这是第一种支持消费设备上RLHF训练的方法。</p>
<p><strong>分布式训练</strong>我们可以将上述训练器与DeepSpeed（Rasley等人，2020；Ren等人，2021）结合起来进行分布式训练。我们采用数据并行来充分利用计算设备的能力。利用DeepSpeed ZeRO优化器，可以通过分区或卸载进一步减少内存消耗。</p>
<h2 id="组件-Utilities">组件(Utilities)</h2>
<p><strong>模型推理</strong>在推理过程中，我们重用数据工作者中的聊天模板来构建模型输入。我们支持使用Transformer（Wolf等人，2020）和vLLM（Kwon等人，2023）对模型输出进行采样，这两种方法都支持流解码。此外，我们实现了一个OpenAI风格的API，该API利用异步LLM引擎和vLLM的分页注意力，提供高吞吐量的并发推理服务，有助于将经过微调的LLM部署到各种应用程序中。</p>
<p><strong>模型评估</strong>我们包括评估LLM的几个指标，包括多项选择任务，如MMLU（Hendrycks等人，2021）、CMMLU（Li等人，2023a）和C-Eval（Huang等人，2023），以及计算BLEU-4（Papineni等人，2002）和ROUGE（Lin，2004）等文本相似性得分。此功能便于用户衡量微调模型的能力。</p>
<h2 id="LLAMABOARD：LLAMAFACTORY的统一接口">LLAMABOARD：LLAMAFACTORY的统一接口</h2>
<p>LLAMABOARD是基于Gradio（Abid等人，2019）的统一用户界面，允许用户在不编写任何代码的情况下自定义LLM的微调。它提供了一种简化的模型微调和推理服务，使用户能够轻松探索LLM在其环境中的潜力。LLAMABOARD具有以下显著特点。</p>
<p><strong>易于配置</strong>LLAMABOARD允许我们通过与web界面的交互来定制微调参数。我们为大多数用户推荐的大多数参数提供默认值，简化了配置过程。此外，用户可以在web UI上预览数据集以进行验证。</p>
<p><strong>可监控训练</strong>在培训过程中，培训日志和损失曲线实时可视化和更新，使用户能够监控培训进度。此功能为分析微调过程提供了宝贵的见解。<br>
<strong>灵活评估</strong>LLAMABOARD支持计算数据集上的文本相似性得分，以自动评估模型或通过与模型聊天进行人工评估。<br>
<strong>多语言支持</strong>LLAMABOARD提供本地化文件，便于集成新语言来呈现界面。目前，我们支持三种语言：英语、俄语和中文，这允许更广泛的用户使用LLAMABARD来微调LLM。</p>
<h1>实证研究</h1>
<p>我们从两个角度系统地评估了LLAMAFACTORY：1）在内存使用、吞吐量和困惑度方面的训练效率。2） 适应下游任务的有效性</p>
<h2 id="训练效率">训练效率</h2>
<p><strong>实验设置</strong>我们使用PubMed数据集（Canese和Weis，2013），其中包含3600多万条生物医学文献记录。我们从文献摘要中提取了大约40万个标记来构建训练语料库。然后，我们使用生成预训练目标和各种有效的微调方法对Gemma-2B（Mesnard等人，2024）、Llama2-7B和Llama2-13B（Touvron等人，2023b）模型进行微调。我们比较了全调谐、冷冻调谐、GaLore、LoRA和4位QLoRA的结果。经过微调后，我们计算训练语料库的困惑度，以评估不同方法的效率。我们还将预训练模型的困惑作为基线。</p>
<p>在这个实验中，我们采用10-5的学习率，512的令牌批大小。我们使用具有激活检查点的bfloat16精度的8位AdamW优化器（Dettmers等人，2022b）对这些模型进行微调，以减少内存占用。在冻结调优中，我们只对模型的最后3个解码器层进行微调。对于GaLore，我们将rank和scale分别设置为128和2.0。对于LoRA和QLoRA，我们将适配器连接到所有线性层，并将秩和alpha分别设置为128和256。所有实验均在单个NVIDIA A100 40GB GPU上进行。我们在所有实验中启用闪光注意力，在LoRA和QLoRA实验中启用Uncloth。</p>
<p><strong>结果</strong>关于训练效率的结果如表4所示，其中内存是指训练过程中消耗的峰值内存，吞吐量是指每秒训练的令牌数量，PPL表示模型在训练语料库上的困惑度。由于满调Llama2-13B会导致内存溢出，因此不会记录结果。我们观察到QLoRA始终具有最低的内存占用，因为预训练的权重以较低的精度表示。利用Unloth在LoRA层中的优化，LoRA显示出更高的吞吐量。GaLore在大型型号上实现了较低的PPL，而LoRA在小型型号上具有优势。<br>
<img src="efficiency.png" alt="LLAMAFACTORY中使用不同微调方法的训练效率比较。每种型号的GaLore、LoRA和QLoRA中最好的结果都是粗体。"></p>
<h2 id="对下游任务进行微调">对下游任务进行微调</h2>
<p><strong>实验设置</strong>为了评估不同高效微调方法的有效性，我们比较了下游任务微调后各种模型的性能。我们使用来自三个代表性文本生成任务的2000个示例和1000个示例构建非重叠训练集和测试集，分别包括CNN/DM（Nallapati等人，2016）、XSum（Narayan等人，2018）和AdGen（Shao等人，2019）。我们选择了几个指令调优模型，并使用不同的微调方法按照顺序对任务进行微调。然后我们比较了全调谐（FT）、GaLore、LoRA和4位QLoRA的结果。经过微调后，我们计算了每个任务测试集的ROUGE分数（Lin，2004）。我们还将原始指令调整模型的分数作为基线。</p>
<p>在这个实验中，我们将学习率设置为10−5，批处理大小设置为4，最大输入长度设置为2048。我们使用具有激活检查点的bfloat16精度的8位AdamW优化器（Dettmers等人，2022b）对这些模型进行微调。对于GaLore，我们将排名和规模分别设置为128和2.0。对于LoRA和QLoRA，我们将适配器连接到所有线性层，并将秩和alpha分别设置为128和256。所有实验均在NVIDIA A100 40GB GPU上进行。</p>
<p><strong>结果</strong>下游任务的评估结果如表5所示。我们报告了ROUGE-1、ROUGE-2和ROUGEL的平均得分。Gemma-7B和Qwen2-7B（Bai等人，2023）模型的一些结果未包含在表中，因为GaLore方法可能不适用于它们。结果中的一个有趣发现是，除了CNN/DM和AdGen数据集上的ChatGLM3-6B（Zeng等人，2024）和Llama2-7B模型外，LoRA和QLoRA在大多数情况下都达到了最佳性能。这一现象突显了这些有效的微调方法在使LLM适应特定任务方面的有效性。此外，我们观察到Llama3-8B在这些模型中表现最佳，而Yi-6B（Young等人，2024）和Mistral-7B（Jiang等人，2023a）在相同尺寸的模型中表现出具有竞争力的性能。<br>
<img src="performance.png" alt="在LLAMAFACTORY中使用不同微调方法对特定任务的性能（就ROUGE而言）进行比较。每个模型的最佳结果都用下划线表示，每个任务的最佳结果用粗体表示。"></p>
<h1>结论和未来工作</h1>
<p>在本文中，我们展示了LLAMAFACTORY，这是一个用于高效微调LLM的统一框架。通过模块化设计，我们最大限度地减少了模型、数据集和训练方法之间的依赖性，并提供了一种集成方法，通过各种高效的微调技术对100多种LLM进行微调。此外，我们还提供灵活的web UI LLAMABOARD，无需编码即可对LLM进行自定义微调和评估。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。</p>
<p>我们将始终如一地保持LLAMAFACTORY与最先进的模型和高效的微调技术同步。我们也欢迎开源社区的贡献。LLAMAFACTORY的路线图包括：<br>
（1） 能够对支持更广泛模态的模型进行微调，例如音频和视频模态（Zhu等人，2024a）。<br>
（2） 整合更多的并行训练策略，例如序列并行性（Jacobs等人，2023）和张量并行性（Shoeybi等人，2019）。<br>
（3） 探索会话模型的更强微调方法，例如self-play（Chen等人，2024c；袁等人，2024）</p>
<h1>更广泛的影响和负责任的使用</h1>
<p>LLAMAFACTORY吸引了大量对LLM感兴趣的人来探索定制模型的可能性。这对开源社区的发展做出了重大贡献。它正受到越来越多的关注，并作为LLM高效微调框架的代表出现在《Awesome Transformers 2》中。我们期望从业者在我们的框架上建立他们的LLM，为社会带来利益。在使用LLAMAFACTORY对LLM进行微调时，必须遵守模型许可证，从而防止任何潜在的滥用。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SergeWang.github.io">Serge Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/">https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SergeWang.github.io" target="_blank">Model The World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/architecture.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/" title="论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器"><img class="cover" src="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/diffusion-models.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器</div></div><div class="info-2"><div class="info-item-1">摘要 扩散模型采样的反演旨在找到样本的相应初始噪声，在各种任务中起着至关重要的作用。最近，已经提出了几种启发式精确反演采样器，以无需训练的方式解决不精确反演问题。然而，这些启发式采样器的理论性质仍然未知，它们的采样质量往往很差。本文介绍了一种精确反演采样器的通用公式，即双向显式线性多步（BELM）采样器，其中包括所有先前提出的启发式精确反演采样器作为特例。BELM公式是通过整合双向显式约束，从变步长变公式线性多步方法中推导出来的。我们强调这种双向显式约束是数学精确反演的关键。我们系统地研究了BELM框架内的局部截断误差（LTE），并表明现有的精确反演采样器的启发式设计产生了次优LTE。因此，我们通过LTE最小化方法提出了最优BELM（OBELM）采样器。我们进行了额外的分析，以证实所提出的最优采样器的理论稳定性和全局收敛性。综合实验表明，我们的O-BELM采样器在实现高质量采样的同时建立了精确的反演特性。图像编辑和图像插值的其他实验突出了O-BELM在不同应用中的广泛潜力。 </div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/11/02/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%89%EF%BC%9A%E7%94%9F%E6%88%90%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AF%B9%E9%BD%90%EF%BC%88REPA%EF%BC%89%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%89%A9%E6%95%A3%E5%8F%98%E6%8D%A2%E5%99%A8%E6%AF%94%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E8%A6%81%E5%AE%B9%E6%98%93/" title="论文阅读三：生成的表示对齐（REPA）：训练扩散变换器比你想象的要容易"><img class="cover" src="/2024/11/02/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%89%EF%BC%9A%E7%94%9F%E6%88%90%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AF%B9%E9%BD%90%EF%BC%88REPA%EF%BC%89%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%89%A9%E6%95%A3%E5%8F%98%E6%8D%A2%E5%99%A8%E6%AF%94%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E8%A6%81%E5%AE%B9%E6%98%93/diffusion-models.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-02</div><div class="info-item-2">论文阅读三：生成的表示对齐（REPA）：训练扩散变换器比你想象的要容易</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2024/10/31/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%80%EF%BC%9A%E9%AB%98%E6%95%88%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E8%B7%B5%E7%9A%84%E5%85%A8%E9%9D%A2%E7%BB%BC%E8%BF%B0/" title="论文阅读一：高效扩散模型：从原理到实践的全面综述"><img class="cover" src="/2024/10/31/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%80%EF%BC%9A%E9%AB%98%E6%95%88%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E8%B7%B5%E7%9A%84%E5%85%A8%E9%9D%A2%E7%BB%BC%E8%BF%B0/diffusion-models.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-31</div><div class="info-item-2">论文阅读一：高效扩散模型：从原理到实践的全面综述</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2024/11/01/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%BA%8C%EF%BC%9A%E6%88%90%E5%83%8F%E5%92%8C%E8%A7%86%E8%A7%89%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%95%99%E7%A8%8B/" title="论文阅读二：成像和视觉扩散模型教程"><img class="cover" src="/2024/11/01/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%BA%8C%EF%BC%9A%E6%88%90%E5%83%8F%E5%92%8C%E8%A7%86%E8%A7%89%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%95%99%E7%A8%8B/ELBO.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-01</div><div class="info-item-2">论文阅读二：成像和视觉扩散模型教程</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%EF%BC%9AD-FINE%EF%BC%9A%E5%B0%86DETRS%E4%B8%AD%E7%9A%84%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E4%B8%BA%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E5%B8%83%E7%BB%86%E5%8C%96/" title="论文阅读五：D-FINE：将DETRS中的回归任务重新定义为细粒度分布细化"><img class="cover" src="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%EF%BC%9AD-FINE%EF%BC%9A%E5%B0%86DETRS%E4%B8%AD%E7%9A%84%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E4%B8%BA%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E5%B8%83%E7%BB%86%E5%8C%96/D-FINE.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-02</div><div class="info-item-2">论文阅读五：D-FINE：将DETRS中的回归任务重新定义为细粒度分布细化</div></div><div class="info-2"><div class="info-item-1">摘要 我们介绍了D-FINE，这是一种强大的实时对象检测器，通过在DETR模型中重新定义边界框回归任务，实现了出色的定位精度。D-FINE包括两个关键组成部分：细粒度分布细化（FDR）和全局最优定位自蒸馏（GO-LSD）。FDR将回归过程从预测固定坐标转换为迭代细化概率分布，提供了一种细粒度的中间表示，显著提高了定位精度。GOLSD是一种双向优化策略，通过自蒸馏将定位知识从精细分布转移到较浅层，同时简化了较深层的残差预测任务。此外，D-FINE在计算密集型模块和操作中采用了轻量级优化，在速度和精度之间实现了更好的平衡。具体来说，D-FINE-L/X在NVIDIA T4 GPU上以124/78 FPS的速度在COCO数据集上实现了54.0%/55.8%的AP。在Objects365上进行预训练时，D-FINE-L/X的AP达到57.1%/59.3%，超过了所有现有的实时检测器。此外，我们的方法显著提高了各种DETR模型的性能，AP高达5.3%，额外参数和训练成本可以忽略不计。我们的代码和预训练模型：https://github.com/Peterande/D-FINE...</div></div></div></a><a class="pagination-related" href="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/" title="论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器"><img class="cover" src="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/diffusion-models.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-03</div><div class="info-item-2">论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器</div></div><div class="info-2"><div class="info-item-1">摘要 扩散模型采样的反演旨在找到样本的相应初始噪声，在各种任务中起着至关重要的作用。最近，已经提出了几种启发式精确反演采样器，以无需训练的方式解决不精确反演问题。然而，这些启发式采样器的理论性质仍然未知，它们的采样质量往往很差。本文介绍了一种精确反演采样器的通用公式，即双向显式线性多步（BELM）采样器，其中包括所有先前提出的启发式精确反演采样器作为特例。BELM公式是通过整合双向显式约束，从变步长变公式线性多步方法中推导出来的。我们强调这种双向显式约束是数学精确反演的关键。我们系统地研究了BELM框架内的局部截断误差（LTE），并表明现有的精确反演采样器的启发式设计产生了次优LTE。因此，我们通过LTE最小化方法提出了最优BELM（OBELM）采样器。我们进行了额外的分析，以证实所提出的最优采样器的理论稳定性和全局收敛性。综合实验表明，我们的O-BELM采样器在实现高质量采样的同时建立了精确的反演特性。图像编辑和图像插值的其他实验突出了O-BELM在不同应用中的广泛潜力。 </div></div></div></a><a class="pagination-related" href="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%EF%BC%9ACMs%EF%BC%9A%E7%AE%80%E5%8C%96%EF%BC%8C%E7%A8%B3%E5%AE%9A%E5%92%8C%E7%BC%A9%E6%94%BE%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="论文阅读四：CMs：简化，稳定和缩放连续时间一致性模型"><img class="cover" src="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%EF%BC%9ACMs%EF%BC%9A%E7%AE%80%E5%8C%96%EF%BC%8C%E7%A8%B3%E5%AE%9A%E5%92%8C%E7%BC%A9%E6%94%BE%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/diffusion-models.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-02</div><div class="info-item-2">论文阅读四：CMs：简化，稳定和缩放连续时间一致性模型</div></div><div class="info-2"><div class="info-item-1">摘要 一致性模型（CM）是一类强大的基于扩散的生成模型，针对快速采样进行了优化。大多数现有的CM都是使用离散化时间步长训练的，这会引入额外的超参数，并且容易出现离散化误差。虽然连续时间公式可以缓解这些问题，但它们的成功受到训练不稳定性的限制。为了解决这个问题，我们提出了一个简化的理论框架，该框架将之前扩散模型和CM的参数化统一起来，确定了不稳定的根本原因。基于这一分析，我们介绍了扩散过程参数化、网络架构和训练目标方面的关键改进。这些变化使我们能够以前所未有的规模训练连续时间CM，在ImageNet 512×512上达到1.5B参数。我们提出的训练算法仅使用两个采样步骤，在CIFAR-10上实现了2.06的FID得分，在ImageNet 64×64上实现了1.48的FID分数，在ImageNet...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/newlogo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Serge Wang</div><div class="author-info-description">今日事，今日毕</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">11</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SergeWang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SergeWang" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sergew027@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">相关工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">高效的微调技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E4%BC%98%E5%8C%96"><span class="toc-number">4.1.</span> <span class="toc-text">高效优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E8%AE%A1%E7%AE%97"><span class="toc-number">4.2.</span> <span class="toc-text">高效计算</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">LLAMAFACTORY框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">5.1.</span> <span class="toc-text">模型加载器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B7%A5%E4%BD%9C%E8%80%85"><span class="toc-number">5.2.</span> <span class="toc-text">数据工作者</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%99%A8"><span class="toc-number">5.3.</span> <span class="toc-text">训练器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%84%E4%BB%B6-Utilities"><span class="toc-number">5.4.</span> <span class="toc-text">组件(Utilities)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLAMABOARD%EF%BC%9ALLAMAFACTORY%E7%9A%84%E7%BB%9F%E4%B8%80%E6%8E%A5%E5%8F%A3"><span class="toc-number">5.5.</span> <span class="toc-text">LLAMABOARD：LLAMAFACTORY的统一接口</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">实证研究</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87"><span class="toc-number">6.1.</span> <span class="toc-text">训练效率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83"><span class="toc-number">6.2.</span> <span class="toc-text">对下游任务进行微调</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">结论和未来工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">更广泛的影响和负责任的使用</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/" title="论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调"><img src="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/architecture.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调"/></a><div class="content"><a class="title" href="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/" title="论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调">论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调</a><time datetime="2024-11-03T11:40:36.000Z" title="发表于 2024-11-03 19:40:36">2024-11-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/" title="论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器"><img src="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/diffusion-models.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器"/></a><div class="content"><a class="title" href="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/" title="论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器">论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器</a><time datetime="2024-11-03T03:30:36.000Z" title="发表于 2024-11-03 11:30:36">2024-11-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%EF%BC%9AD-FINE%EF%BC%9A%E5%B0%86DETRS%E4%B8%AD%E7%9A%84%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E4%B8%BA%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E5%B8%83%E7%BB%86%E5%8C%96/" title="论文阅读五：D-FINE：将DETRS中的回归任务重新定义为细粒度分布细化"><img src="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%EF%BC%9AD-FINE%EF%BC%9A%E5%B0%86DETRS%E4%B8%AD%E7%9A%84%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E4%B8%BA%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E5%B8%83%E7%BB%86%E5%8C%96/D-FINE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读五：D-FINE：将DETRS中的回归任务重新定义为细粒度分布细化"/></a><div class="content"><a class="title" href="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%EF%BC%9AD-FINE%EF%BC%9A%E5%B0%86DETRS%E4%B8%AD%E7%9A%84%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E4%B8%BA%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E5%B8%83%E7%BB%86%E5%8C%96/" title="论文阅读五：D-FINE：将DETRS中的回归任务重新定义为细粒度分布细化">论文阅读五：D-FINE：将DETRS中的回归任务重新定义为细粒度分布细化</a><time datetime="2024-11-02T11:16:36.000Z" title="发表于 2024-11-02 19:16:36">2024-11-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%EF%BC%9ACMs%EF%BC%9A%E7%AE%80%E5%8C%96%EF%BC%8C%E7%A8%B3%E5%AE%9A%E5%92%8C%E7%BC%A9%E6%94%BE%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="论文阅读四：CMs：简化，稳定和缩放连续时间一致性模型"><img src="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%EF%BC%9ACMs%EF%BC%9A%E7%AE%80%E5%8C%96%EF%BC%8C%E7%A8%B3%E5%AE%9A%E5%92%8C%E7%BC%A9%E6%94%BE%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/diffusion-models.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读四：CMs：简化，稳定和缩放连续时间一致性模型"/></a><div class="content"><a class="title" href="/2024/11/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%EF%BC%9ACMs%EF%BC%9A%E7%AE%80%E5%8C%96%EF%BC%8C%E7%A8%B3%E5%AE%9A%E5%92%8C%E7%BC%A9%E6%94%BE%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="论文阅读四：CMs：简化，稳定和缩放连续时间一致性模型">论文阅读四：CMs：简化，稳定和缩放连续时间一致性模型</a><time datetime="2024-11-02T10:00:36.000Z" title="发表于 2024-11-02 18:00:36">2024-11-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/02/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%89%EF%BC%9A%E7%94%9F%E6%88%90%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AF%B9%E9%BD%90%EF%BC%88REPA%EF%BC%89%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%89%A9%E6%95%A3%E5%8F%98%E6%8D%A2%E5%99%A8%E6%AF%94%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E8%A6%81%E5%AE%B9%E6%98%93/" title="论文阅读三：生成的表示对齐（REPA）：训练扩散变换器比你想象的要容易"><img src="/2024/11/02/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%89%EF%BC%9A%E7%94%9F%E6%88%90%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AF%B9%E9%BD%90%EF%BC%88REPA%EF%BC%89%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%89%A9%E6%95%A3%E5%8F%98%E6%8D%A2%E5%99%A8%E6%AF%94%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E8%A6%81%E5%AE%B9%E6%98%93/diffusion-models.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读三：生成的表示对齐（REPA）：训练扩散变换器比你想象的要容易"/></a><div class="content"><a class="title" href="/2024/11/02/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%89%EF%BC%9A%E7%94%9F%E6%88%90%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%AF%B9%E9%BD%90%EF%BC%88REPA%EF%BC%89%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%89%A9%E6%95%A3%E5%8F%98%E6%8D%A2%E5%99%A8%E6%AF%94%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E8%A6%81%E5%AE%B9%E6%98%93/" title="论文阅读三：生成的表示对齐（REPA）：训练扩散变换器比你想象的要容易">论文阅读三：生成的表示对齐（REPA）：训练扩散变换器比你想象的要容易</a><time datetime="2024-11-02T08:42:36.000Z" title="发表于 2024-11-02 16:42:36">2024-11-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Serge Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>
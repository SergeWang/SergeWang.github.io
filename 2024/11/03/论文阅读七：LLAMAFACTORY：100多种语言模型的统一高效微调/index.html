<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调 | Model The World</title><meta name="author" content="Serge Wang"><meta name="copyright" content="Serge Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="摘要 高效的微调对于使大型语言模型（LLM）适应下游任务至关重要。然而，在不同的模型上实现这些方法需要付出巨大的努力。我们介绍LLAMAFACTORY，这是一个整合了一套尖端高效训练方法的统一框架。它提供了一种解决方案，可以灵活地定制100多个LLM的微调，而无需通过内置的web UI LLAMABOARD进行编码。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。它已发布于 h">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调">
<meta property="og:url" content="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/">
<meta property="og:site_name" content="Model The World">
<meta property="og:description" content="摘要 高效的微调对于使大型语言模型（LLM）适应下游任务至关重要。然而，在不同的模型上实现这些方法需要付出巨大的努力。我们介绍LLAMAFACTORY，这是一个整合了一套尖端高效训练方法的统一框架。它提供了一种解决方案，可以灵活地定制100多个LLM的微调，而无需通过内置的web UI LLAMABOARD进行编码。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。它已发布于 h">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/architecture.png">
<meta property="article:published_time" content="2024-11-03T11:40:36.000Z">
<meta property="article:modified_time" content="2024-11-09T05:38:25.404Z">
<meta property="article:author" content="Serge Wang">
<meta property="article:tag" content="论文阅读">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/architecture.png"><link rel="shortcut icon" href="/img/newlogo.png"><link rel="canonical" href="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IDUolVgWkW_cmu1mtW5hsxrrIjQfCHvq5hOTOkXeVNE"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9dcb7eb7a8a6225c2b1f242f3b0894bf";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XERFYF0N5K"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-XERFYF0N5K')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-XERFYF0N5K', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-09 13:38:25'
}</script></head><body><div id="web_bg" style="background-image: url(/img/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/newlogo.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(2024/11/03/论文阅读七：LLAMAFACTORY：100多种语言模型的统一高效微调/architecture.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/newlogo.png" alt="Logo"><span class="site-name">Model The World</span></a><a class="nav-page-title" href="/"><span class="site-name">论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读七：LLaMA-Factory：100多种语言模型的统一高效微调</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-03T11:40:36.000Z" title="发表于 2024-11-03 19:40:36">2024-11-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-09T05:38:25.404Z" title="更新于 2024-11-09 13:38:25">2024-11-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><img src="architecture.png" alt="100多种语言模型的统一高效微调"></p>
<h2 id="摘要">摘要</h2>
<p>高效的微调对于使大型语言模型（LLM）适应下游任务至关重要。然而，在不同的模型上实现这些方法需要付出巨大的努力。我们介绍LLAMAFACTORY，这是一个整合了一套尖端高效训练方法的统一框架。它提供了一种解决方案，可以灵活地定制100多个LLM的微调，而无需通过内置的web UI LLAMABOARD进行编码。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。它已发布于 <a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a> ，并获得了25000多颗星和3000个fork。</p>
<h2 id="引言">引言</h2>
<p>大型语言模型（LLM）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.19341">赵等人，2023</a>）具有显著的推理能力，并赋予了广泛的应用，如问答（<a target="_blank" rel="noopener" href="https://aclanthology.org/2023.emnlp-main.228.pdf">Jiang等人，2023b</a>）、机器翻译（<a target="_blank" rel="noopener" href="https://aclanthology.org/2023.emnlp-main.1036.pdf">Wang等人，2023c</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.02426">Jiao等人，2023a</a>）和信息提取（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.16040">Jiao等人（2023b）</a>）。随后，大量LLM被开发出来，并可通过开源社区访问。例如，Hugging Face的开放式LLM排行榜（<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">Beeching等人，2023</a>）拥有5000多个模特，为寻求利用LLM力量的个人提供了便利。</p>
<p>用有限的资源微调大量参数成为使LLM适应下游任务的主要挑战。一种流行的解决方案是高效的微调（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751">Houlsby等人，2019</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">Hu等人，2022</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">Dettmers等人，2023</a>），这降低了LLM在适应各种任务时的训练成本。然而，社区为高效的微调提供了各种方法，缺乏一个系统框架来适应和统一这些方法到不同的LLM，并为用户定制提供友好的界面。</p>
<p>为了解决上述问题，我们开发了LLAMAFACTORY，这是一个使LLM微调民主化的框架。它通过可扩展的模块统一了各种高效的微调方法，能够以最少的资源和高吞吐量对数百个LLM进行微调。此外，它简化了常用的训练方法，包括生成预训练（<a target="_blank" rel="noopener" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Radford等人，2018</a>）、监督微调（SFT）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.01652">Wei等人，2022</a>）、基于人类反馈的强化学习（RLHF）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Ouyang等人，2022年</a>）和直接偏好优化（DPO）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.18290">Rafailov等人，2023年</a>）。用户可以利用命令行或web界面来定制和微调他们的LLM，只需很少或根本不需要编码工作。</p>
<p>LLAMAFACTORY由三个主要模块组成：模型加载器（Model Loader）、数据工作者（Data Worker）和训练器（Trainer）。我们最大限度地减少了这些模块对特定模型和数据集的依赖，使框架能够灵活地扩展到数百个模型和数据集中。具体来说，我们首先建立一个模型注册表，模型加载器可以通过识别精确的层将适配器精确地连接到预训练的模型。然后，我们开发了一个数据描述规范，允许数据工作者通过对齐相应的列来收集数据集。此外，我们提供最先进的高效微调方法的即插即用实现，使训练师能够通过替换默认方法来激活。我们的设计允许这些模块在不同的训练方法中重复使用，从而显著降低了集成成本。</p>
<p>LLAMAFACTORY使用PyTorch实现（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.01703">Paszke等人，2019</a>），并从开源库中受益匪浅，如Transformers（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.03771">Wolf等人，2020</a>）、PEFT（<a href="(https://github.com/huggingface/peft)">Mangrulkar等人，2022</a>）和TRL（<a target="_blank" rel="noopener" href="https://github.com/huggingface/trl">von Werra等人，2020年</a>）。在此基础上，我们提供了一个具有更高抽象级别的开箱即用框架。此外，我们使用Gradio（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.02569">Abid等人，2019</a>）构建了LLAMABOARD，可以在不需要编码的情况下对LLM进行微调。</p>
<p>LLAMAFACTORY是根据Apache-2.0许可证开源的。它已经在GitHub上获得了25000多颗星和3000个分支，并且在Hugging Face Hub1的LLAMAFACTORY上构建了数百个开源模型。例如，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.02715">Truong等人（2024）</a>基于LLAMAFACTORY构建了GemSUra-7B，揭示了Gemma（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.08295">Mesnard等人，2024</a>）的跨语言能力。此外，数十项研究利用我们的框架来探索LLM（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.02223">Wang等人，2023a</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.10092">Yu等人，2023</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.11746">Bhardwaj等人，2024</a>）。</p>
<h2 id="相关工作">相关工作</h2>
<p>随着对微调LLM的需求迅速增加，已经开发了许多使LLM适应特定目的的框架。LLaMAAdapter（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.16199">Zhang等人，2024</a>）使用<strong>零初始化注意力</strong>有效地微调Llama模型（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13971">Touvron等人，2023a</a>）。FastChat（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.05685">Zheng等人，2023</a>）是一个专注于训练和评估LLM以完成聊天的框架。LitGPT（<a target="_blank" rel="noopener" href="https://github.com/Lightning-AI/litgpt">AI，2023</a>）提供生成模型的实现，并支持各种训练方法。Open Instruct（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.04751">Wang等人，2023d</a>）提供了训练指令模型的配方。Colossal AI（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.14883">Li等人，2023b</a>）采用先进的并行策略进行分布式训练。LMFlow（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.12420">Diao等人，2024</a>）支持为专业领域或任务训练LLM。GPT4All（<a target="_blank" rel="noopener" href="https://github.com/nomic-ai/gpt4all">Anand等人，2023</a>）允许LLM在消费设备上运行，同时还提供微调功能。与现有的竞争框架相比，LLAMAFACTORY支持更广泛的高效微调技术和训练方法。我们在表1中列出了代表性框架中的特征。<br>
<img src="frameworks.png" alt="LLAMAFACTORY中的功能与常用的微调LLM框架的比较。"></p>
<h2 id="高效的微调技术">高效的微调技术</h2>
<p>高效的LLM微调技术可分为两大类：侧重于优化的技术和旨在计算的技术。高效优化技术的主要目标是微调LLM的参数，同时将成本降至最低。另一方面，高效的计算方法寻求减少LLM中所需计算的时间或空间。LLAMAFACTORY中包含的方法列于表2中。我们将在以下部分介绍这些高效的微调技术，并展示将其纳入我们的框架所实现的显著效率提高。<br>
<img src="technique.png" alt="LLAMAFACTORY中的微调技术之间的兼容性。"></p>
<h3 id="高效优化">高效优化</h3>
<p>首先，我们概述了LLAMAFACTORY中使用的高效优化技术。冻结调谐方法（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751">Houlsby等人，2019</a>）涉及<strong>冻结大多数参数</strong>，同时微调解码器层一小部分中的其余参数。另一种称为<strong>梯度低阶投影</strong>（GaLore）的方法（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.03507">赵等人，2024</a>）将梯度投影到低维空间中，以高效记忆的方式促进全参数学习。同样，BAdam（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.02827">Luo等人，2024</a>）利用<strong>块坐标下降</strong>（BCD）有效地优化了广泛的参数。相反，低秩自适应（LoRA）（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">Hu等人，2022</a>）方法<strong>冻结所有预训练的权重</strong>，并将一对可训练的低秩矩阵引入指定层。当与量化相结合时，这种方法被称为QLoRA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14314">Dettmers等人，2023</a>），它还减少了内存使用。DoRA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.09353">Liu等人，2024</a>）将预先训练的<strong>权重分解为幅度和方向分量</strong>，并更新方向分量以提高性能。LoRA+（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.12354">Hayou等人，2024</a>）被提出以克服LoRA的次优性。PiSSA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.02948">Meng等人，2024</a>）<strong>使用预训练权重的主成分初来始化适配器</strong>，以实现更快的收敛。</p>
<h3 id="高效计算">高效计算</h3>
<p>在LLAMAFACTORY中，我们集成了一系列高效计算技术。常用的技术包括<strong>混合精度训练</strong>（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.03740">Micikevicius等人，2018</a>）和<strong>激活检查点</strong>（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.06174">Chen等人，2016</a>）。通过对注意力层的输入输出（IO）费用的检查，flash attention（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135">Dao等人，2022</a>）引入了一种硬件友好的方法来增强注意力计算。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 注意力（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.12307">Chen等人，2024b</a>）通过转移稀疏注意力来应对扩展上下文的挑战，从而减少了微调长上下文LLM时的内存使用。各种量化策略（<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html">Dettmers等人，2022a</a>；<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=tcbBPnfwxS">Frantar等人，2023</a>；<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.00978">Lin等人，2023</a>，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.06118">Egiazarian等人，2024</a>）通过利用较低精度的权重表示来降低大型语言模型（LLM）中的内存需求。然而，量化模型的微调仅限于基于适配器的技术，如LoRA（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">Hu等人，2022</a>）。Unsloth（<a target="_blank" rel="noopener" href="https://github.com/unslothai/unsloth">Han和Han，2023</a>）采用Triton（<a target="_blank" rel="noopener" href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Tillet等人，2019</a>）来实现LoRA的反向传播，这减少了梯度下降过程中的浮点运算（FLOP），并加快了LoRA训练。</p>
<p>LLAMAFACTORY将这些技术无缝结合成一个有凝聚力的结构，以提高LLM微调的效率。这导致内存占用从混合精度训练期间的每个参数18个字节（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.03740">Micikevicius等人，2018</a>）或半精度训练中的每个参数8个字节（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.05100">Le Scao等人，2022</a>）减少到每个参数仅0.6个字节。后续章节将对LLAMAFACTORY中的组件进行进一步阐述。</p>
<h2 id="LLAMAFACTORY框架">LLAMAFACTORY框架</h2>
<p>LLAMAFACTORY由三个主要模块组成：模型加载器、数据工作者和训练器。模型加载器操纵各种模型架构进行微调，支持大型语言模型（LLM）和视觉语言模型（VLM）。Data Worker通过精心设计的管道处理来自不同任务的数据，支持单轮和多轮对话。训练师将高效的微调技术应用于不同的训练方法，支持预训练、指令调整和偏好优化。除此之外，LLAMABOARD提供了一个友好的视觉界面来访问这些模块，使用户能够无代码地配置和启动单个LLM微调实例，并同步监控训练状态。我们在图1中说明了这些模块与LLAMAFACTORY的整体架构之间的关系。</p>
<p><img src="architecture.png" alt="LLAMAFACTORY的架构"></p>
<h3 id="模型加载器">模型加载器</h3>
<p>本节首先介绍了模型加载器中的四个组件：模型初始化、模型拼接、模型量化和适配器连接，然后描述了我们通过在微调过程中处理参数浮点精度来适应各种设备的方法。</p>
<p><strong>模型初始化</strong>我们利用Transformers的自动分类（Wolf等人，2020）来加载预训练模型并初始化参数。具体来说，我们使用AutoModelForVision2Seq类加载视觉语言模型，而其余的则使用AutoModelForCausalLM类加载。tokenizer与模型一起使用AutoTokenizer类加载。在tokenizer的词汇量超过嵌入层容量的情况下，我们调整层的大小，并使用噪声均值初始化来初始化新参数。为了确定RoPE缩放的缩放因子（Chen等人，2023），我们将其计算为最大输入序列长度与模型上下文长度的比率。</p>
<p><strong>模型拼接</strong>为了实现S2注意力，我们使用猴子补丁来代替模型的前向计算。然而，我们使用本地类来启用flash注意力，因为自Transformers 4.34.0以来，它得到了广泛的支持。为了防止动态层的过度划分，我们在DeepSpeed ZeRO第三阶段优化MoE模型时，将专家混合（MoE）块设置为叶子模块（Rasley等人，2020）。</p>
<p><strong>模型量化</strong>可以通过bitsandbytes库（Dettmers，2021）使用LLM.int8（Dettmer等人，2022a）将模型动态量化为8位或4位。对于4位量化，我们使用双量化和4位正常浮点数作为QLoRA（Dettmers等人，2023）。我们还支持对训练后量化（PTQ）方法量化的模型进行微调，包括GPTQ（Frantar等人，2023）、AWQ（Lin等人，2023年）和AQLM（Egiazarian等人，2024年）。请注意，我们不能直接微调量化权重；因此，量化模型仅与基于适配器的方法兼容。</p>
<p><strong>适配器连接</strong>我们通过遍历模型层自动识别要连接适配器的适当层。低秩适配器连接到所有线性层，以实现更好的收敛，如（Dettmers等人，2023）所述。PEFT（Mangrulkar等人，2022）库提供了一种极其方便的方法来实现基于适配器的方法，如LoRA（Hu等人，2022年）、rsLoRA（Kalajdzievski，2023年）、DoRA（Liu等人，2024年）和PiSSA（Meng等人，2024年）。我们用Unsloth（Han和Han，2023）的反向计算代替，以加速训练。为了执行基于人类反馈的强化学习（RLHF），在变换器模型的顶部附加一个值头层（value head layer)，将每个token的表示映射到标量。</p>
<p><strong>精度自适应</strong>我们根据计算设备的能力处理预训练模型的浮点精度。对于NVIDIA GPU，如果计算能力为8.0或更高，我们采用bfloat16精度。否则，采用float16。此外，我们为Ascend NPU和AMD GPU采用float16，为非CUDA设备采用float32。在混合精度训练中，我们将所有可训练参数设置为float32，以提高训练稳定性。然而，在半精度训练中，我们将可训练参数保留为bfloat16。</p>
<h3 id="数据工作者">数据工作者</h3>
<p>我们开发了一个数据处理管道，包括数据集加载、数据集对齐、数据集合并和数据集预处理。它将不同任务的数据集标准化为统一的格式，使我们能够在各种格式的数据集上微调模型。</p>
<p><strong>数据集加载</strong>我们利用Datasets（Lhoest等人，2021）库加载数据，这允许用户从Hugging Face Hub加载远程数据集，或通过脚本或文件读取本地数据集。Datasets库显著降低了数据处理过程中的内存开销，并使用Arrow加速了样本查询（Apache，2016）。默认情况下，整个数据集被下载到本地磁盘。然而，如果数据集太大而无法存储，我们的框架会提供数据流来迭代它，而无需下载。</p>
<p><strong>数据集对齐</strong>为了统一数据集格式，我们设计了一个数据描述规范来表征数据集的结构。例如，羊驼数据集有三列：指令、输入和输出（Taori等人，2023）。我们根据数据描述规范将数据集转换为与各种任务兼容的标准结构。数据集结构的一些示例如表3所示。</p>
<p><strong>数据集合并</strong>统一的数据集结构为合并多个数据集提供了一种有效的方法。对于非流模式的数据集，我们只需在训练期间对数据集进行混洗之前将它们连接起来。然而，在流模式下，简单地连接数据集会阻碍数据洗牌。因此，我们提供了交替读取不同数据集数据的方法。</p>
<p><strong>数据集预处理</strong>LLAMAFACTORY旨在微调文本生成模型，主要用于聊天完成。聊天模板是这些模型中的关键组成部分，因为它与这些模型的指令遵循能力高度相关。因此，我们提供了数十个聊天模板，可以根据模型类型自动选择。我们使用标记器应用聊天模板后对句子进行编码。默认情况下，我们只计算完井损失，而忽略提示（Taori等人，2023）。可选地，我们可以利用序列打包（Krell等人，2021）来减少训练时间，这在执行生成预训练时会自动启用。</p>
<h3 id="训练器">训练器</h3>
<p><strong>高效训练</strong>我们通过替换默认组件，将最先进的高效微调方法，包括LoRA+（Hayou et al.，2024）、GaLore（赵et al.，424）和BAdam（Luo et al.，024）集成到训练师中。这些微调方法独立于训练师，使其易于应用于各种任务。我们使用Transformers（Wolf等人，2020）的训练师进行预训练和SFT，同时采用TRL（von Werra等人，2020年）的训练员进行RLHF和DPO。我们还包括来自TRL库的高级偏好优化方法的训练师，如KTO（Ethayarajh等人，2024）和ORPO（Hong等人，2024）。利用定制的数据整理器来区分各种训练方法的训练师。为了匹配训练器对偏好数据的输入格式，我们在一批中构建2n个样本，其中前n个样本是被选中的样本，最后n个样本则是被拒绝的样本。</p>
<p><strong>模型共享RLHF</strong>允许在消费设备上进行RLHF训练对于LLM微调的民主化至关重要。然而，这很困难，因为RLHF训练需要四种不同的模型。为了解决这个问题，我们提出了模型共享RLHF，使整个RLHF训练不超过一个预训练模型。具体来说，我们首先使用奖励建模的目标函数训练一个适配器和一个值头，使模型能够计算奖励分数。然后，我们初始化另一个适配器和值头，并用PPO算法对其进行训练（Ouyang等人，2022）。在训练过程中，适配器和价值头通过PEFT的set_adapter和disable_adapter方法动态切换（Mangrulkar等人，2022），允许单个预训练模型同时用作策略模型、价值模型、参考模型和奖励模型。据我们所知，这是第一种支持消费设备上RLHF训练的方法。</p>
<p><strong>分布式训练</strong>我们可以将上述训练器与DeepSpeed（Rasley等人，2020；Ren等人，2021）结合起来进行分布式训练。我们采用数据并行来充分利用计算设备的能力。利用DeepSpeed ZeRO优化器，可以通过分区或卸载进一步减少内存消耗。</p>
<h3 id="组件-Utilities">组件(Utilities)</h3>
<p><strong>模型推理</strong>在推理过程中，我们重用数据工作者中的聊天模板来构建模型输入。我们支持使用Transformer（Wolf等人，2020）和vLLM（Kwon等人，2023）对模型输出进行采样，这两种方法都支持流解码。此外，我们实现了一个OpenAI风格的API，该API利用异步LLM引擎和vLLM的分页注意力，提供高吞吐量的并发推理服务，有助于将经过微调的LLM部署到各种应用程序中。</p>
<p><strong>模型评估</strong>我们包括评估LLM的几个指标，包括多项选择任务，如MMLU（Hendrycks等人，2021）、CMMLU（Li等人，2023a）和C-Eval（Huang等人，2023），以及计算BLEU-4（Papineni等人，2002）和ROUGE（Lin，2004）等文本相似性得分。此功能便于用户衡量微调模型的能力。</p>
<h3 id="LLAMABOARD：LLAMAFACTORY的统一接口">LLAMABOARD：LLAMAFACTORY的统一接口</h3>
<p>LLAMABOARD是基于Gradio（Abid等人，2019）的统一用户界面，允许用户在不编写任何代码的情况下自定义LLM的微调。它提供了一种简化的模型微调和推理服务，使用户能够轻松探索LLM在其环境中的潜力。LLAMABOARD具有以下显著特点。</p>
<p><strong>易于配置</strong>LLAMABOARD允许我们通过与web界面的交互来定制微调参数。我们为大多数用户推荐的大多数参数提供默认值，简化了配置过程。此外，用户可以在web UI上预览数据集以进行验证。</p>
<p><strong>可监控训练</strong>在训练过程中，训练日志和损失曲线实时可视化和更新，使用户能够监控训练进度。此功能为分析微调过程提供了宝贵的见解。<br>
<strong>灵活评估</strong>LLAMABOARD支持计算数据集上的文本相似性得分，以自动评估模型或通过与模型聊天进行人工评估。<br>
<strong>多语言支持</strong>LLAMABOARD提供本地化文件，便于集成新语言来呈现界面。目前，我们支持三种语言：英语、俄语和中文，这允许更广泛的用户使用LLAMABARD来微调LLM。</p>
<h2 id="实证研究">实证研究</h2>
<p>我们从两个角度系统地评估了LLAMAFACTORY：1）在内存使用、吞吐量和困惑度方面的训练效率。2） 适应下游任务的有效性</p>
<h3 id="训练效率">训练效率</h3>
<p><strong>实验设置</strong>我们使用PubMed数据集（Canese和Weis，2013），其中包含3600多万条生物医学文献记录。我们从文献摘要中提取了大约40万个标记来构建训练语料库。然后，我们使用生成预训练目标和各种有效的微调方法对Gemma-2B（Mesnard等人，2024）、Llama2-7B和Llama2-13B（Touvron等人，2023b）模型进行微调。我们比较了全调谐、冷冻调谐、GaLore、LoRA和4位QLoRA的结果。经过微调后，我们计算训练语料库的困惑度，以评估不同方法的效率。我们还将预训练模型的困惑作为基线。</p>
<p>在这个实验中，我们采用10-5的学习率，512的令牌批大小。我们使用具有激活检查点的bfloat16精度的8位AdamW优化器（Dettmers等人，2022b）对这些模型进行微调，以减少内存占用。在冻结调优中，我们只对模型的最后3个解码器层进行微调。对于GaLore，我们将rank和scale分别设置为128和2.0。对于LoRA和QLoRA，我们将适配器连接到所有线性层，并将秩和alpha分别设置为128和256。所有实验均在单个NVIDIA A100 40GB GPU上进行。我们在所有实验中启用闪光注意力，在LoRA和QLoRA实验中启用Uncloth。</p>
<p><strong>结果</strong>关于训练效率的结果如表4所示，其中内存是指训练过程中消耗的峰值内存，吞吐量是指每秒训练的令牌数量，PPL表示模型在训练语料库上的困惑度。由于满调Llama2-13B会导致内存溢出，因此不会记录结果。我们观察到QLoRA始终具有最低的内存占用，因为预训练的权重以较低的精度表示。利用Unloth在LoRA层中的优化，LoRA显示出更高的吞吐量。GaLore在大型型号上实现了较低的PPL，而LoRA在小型型号上具有优势。<br>
<img src="efficiency.png" alt="LLAMAFACTORY中使用不同微调方法的训练效率比较。每种型号的GaLore、LoRA和QLoRA中最好的结果都是粗体。"></p>
<h3 id="对下游任务进行微调">对下游任务进行微调</h3>
<p><strong>实验设置</strong>为了评估不同高效微调方法的有效性，我们比较了下游任务微调后各种模型的性能。我们使用来自三个代表性文本生成任务的2000个示例和1000个示例构建非重叠训练集和测试集，分别包括CNN/DM（Nallapati等人，2016）、XSum（Narayan等人，2018）和AdGen（Shao等人，2019）。我们选择了几个指令调优模型，并使用不同的微调方法按照顺序对任务进行微调。然后我们比较了全调谐（FT）、GaLore、LoRA和4位QLoRA的结果。经过微调后，我们计算了每个任务测试集的ROUGE分数（Lin，2004）。我们还将原始指令调整模型的分数作为基线。</p>
<p>在这个实验中，我们将学习率设置为10−5，批处理大小设置为4，最大输入长度设置为2048。我们使用具有激活检查点的bfloat16精度的8位AdamW优化器（Dettmers等人，2022b）对这些模型进行微调。对于GaLore，我们将排名和规模分别设置为128和2.0。对于LoRA和QLoRA，我们将适配器连接到所有线性层，并将秩和alpha分别设置为128和256。所有实验均在NVIDIA A100 40GB GPU上进行。</p>
<p><strong>结果</strong>下游任务的评估结果如表5所示。我们报告了ROUGE-1、ROUGE-2和ROUGEL的平均得分。Gemma-7B和Qwen2-7B（Bai等人，2023）模型的一些结果未包含在表中，因为GaLore方法可能不适用于它们。结果中的一个有趣发现是，除了CNN/DM和AdGen数据集上的ChatGLM3-6B（Zeng等人，2024）和Llama2-7B模型外，LoRA和QLoRA在大多数情况下都达到了最佳性能。这一现象突显了这些有效的微调方法在使LLM适应特定任务方面的有效性。此外，我们观察到Llama3-8B在这些模型中表现最佳，而Yi-6B（Young等人，2024）和Mistral-7B（Jiang等人，2023a）在相同尺寸的模型中表现出具有竞争力的性能。<br>
<img src="performance.png" alt="在LLAMAFACTORY中使用不同微调方法对特定任务的性能（就ROUGE而言）进行比较。每个模型的最佳结果都用下划线表示，每个任务的最佳结果用粗体表示。"></p>
<h2 id="结论和未来工作">结论和未来工作</h2>
<p>在本文中，我们展示了LLAMAFACTORY，这是一个用于高效微调LLM的统一框架。通过模块化设计，我们最大限度地减少了模型、数据集和训练方法之间的依赖性，并提供了一种集成方法，通过各种高效的微调技术对100多种LLM进行微调。此外，我们还提供灵活的web UI LLAMABOARD，无需编码即可对LLM进行自定义微调和评估。我们实证验证了我们的框架在语言建模和文本生成任务上的效率和有效性。</p>
<p>我们将始终如一地保持LLAMAFACTORY与最先进的模型和高效的微调技术同步。我们也欢迎开源社区的贡献。LLAMAFACTORY的路线图包括：<br>
（1） 能够对支持更广泛模态的模型进行微调，例如音频和视频模态（Zhu等人，2024a）。<br>
（2） 整合更多的并行训练策略，例如序列并行性（Jacobs等人，2023）和张量并行性（Shoeybi等人，2019）。<br>
（3） 探索会话模型的更强微调方法，例如self-play（Chen等人，2024c；袁等人，2024）</p>
<h2 id="更广泛的影响和负责任的使用">更广泛的影响和负责任的使用</h2>
<p>LLAMAFACTORY吸引了大量对LLM感兴趣的人来探索定制模型的可能性。这对开源社区的发展做出了重大贡献。它正受到越来越多的关注，并作为LLM高效微调框架的代表出现在《Awesome Transformers 2》中。我们期望从业者在我们的框架上建立他们的LLM，为社会带来利益。在使用LLAMAFACTORY对LLM进行微调时，必须遵守模型许可证，从而防止任何潜在的滥用。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SergeWang.github.io">Serge Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/">https://sergewang.github.io/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SergeWang.github.io" target="_blank">Model The World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%83%EF%BC%9ALLAMAFACTORY%EF%BC%9A100%E5%A4%9A%E7%A7%8D%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/architecture.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AB%EF%BC%9A%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="论文阅读八：一致性模型"><img class="cover" src="/images/CM.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">论文阅读八：一致性模型</div></div><div class="info-2"><div class="info-item-1">摘要 虽然扩散模型在图像、音频和视频生成领域获得显著进展，但它们依赖于迭代采样过程，导致生成缓慢。为克服该限制，提出一致性模型。一致性模型直接映射噪声到数据，以生成高质量样本。通过设计，支持快速一步生成，同时允许多步采样，以平衡计算和采样质量。它们还支持零样本（zero-shot）数据编辑，例如图像恢复、着色和超分辨，而无需再这类任务上额外训练。一致性模型可以通过两种方式训练：（1）蒸馏预训练扩散模型，或者（2）作为独立生成模型。通过大量实验证明，它们优于现有扩散模型一步和几步采样的蒸馏技术，对于一步生成，获得CIFAR-10上3.55和ImageNet 64×6464\times6464×64 上6.20的先进FID水平。当独立训练时，一致性模型成为一个新系列的生成模型，在诸如CIFAR-10、ImageNet64x64和LSUN 256x256标准基准上，优于现有的一步、非对抗生成模型。 如下图，数据通过概率流常微分方程（PF-ODE）转为噪声。在PF-ODE轨迹上的任意点 xtx_txt​...</div></div></div></a><a class="pagination-related" href="/2024/11/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%85%AD%EF%BC%9ABELM%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%B2%BE%E7%A1%AE%E5%8F%8D%E6%BC%94%E7%9A%84%E5%8F%8C%E5%90%91%E6%98%BE%E5%BC%8F%E7%BA%BF%E6%80%A7%E5%A4%9A%E6%AD%A5%E9%87%87%E6%A0%B7%E5%99%A8/" title="论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器"><img class="cover" src="/images/BELM.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">论文阅读六：BELM：用于扩散模型精确反演的双向显式线性多步采样器</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/11/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%9D%EF%BC%9AOPENCODER%EF%BC%9A%E9%A1%B6%E7%BA%A7%E4%BB%A3%E7%A0%81LLM%E7%9A%84%E5%BC%80%E6%94%BE%E6%89%8B%E5%86%8C/" title="论文阅读九：OPENCODER：顶级代码LLM的开放手册"><img class="cover" src="/images/opencoder.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-10</div><div class="info-item-2">论文阅读九：OPENCODER：顶级代码LLM的开放手册</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%9B%B6%E6%A0%B7%E6%9C%AC%E7%9F%A5%E8%AF%86%E6%B5%8B%E8%AF%95%E7%9A%84LLM%E5%B9%BB%E8%A7%89%E6%8E%A8%E7%90%86/" title="论文阅读二十三：基于零样本知识测试的LLM幻觉推理"><img class="cover" src="/images/hallucination-reasoning.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-16</div><div class="info-item-2">论文阅读二十三：基于零样本知识测试的LLM幻觉推理</div></div><div class="info-2"><div class="info-item-1">摘要 LLM幻觉，LLM偶尔会产生不忠实的文本，对其实际应用构成了重大挑战。大多数现有的检测方法依赖于外部知识、LLM微调或幻觉标记的数据集，并且它们不能区分不同类型的幻觉，而幻觉对于提高检测性能至关重要。我们引入了一个新的任务，幻觉推理，它将LLM生成的文本分为三类：对齐、未对齐和伪造。我们新颖的零样本方法评估LLM是否对给定的提示和文本有足够的知识。我们在新数据集上进行的实验证明了我们的方法在幻觉推理中的有效性，并强调了它对提高检测性能的重要性。...</div></div></div></a><a class="pagination-related" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%9A%E6%9C%89%E9%99%90%E6%95%B0%E6%8D%AE%E5%BE%AE%E8%B0%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/" title="论文阅读二十二：有限数据微调语言模型实用指南"><img class="cover" src="/images/finetune-methods.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-16</div><div class="info-item-2">论文阅读二十二：有限数据微调语言模型实用指南</div></div><div class="info-2"><div class="info-item-1">摘要 使用预训练大型语言模型（LLMs）已经称为自然语言处理（NLP）中的事实标准，尽管它们需要大量数据。受最近以有限数据训练LLM为重点的研究激增的启发，特别是在低资源领域和语言中，本文调查了最近的迁移学习方法，以优化数据稀缺的下游任务中的模型性能。我们首先解决初始化和持续的预训练策略，以更好地利用未知领域和语言的先验知识。然后，我们研究如何在微调和少样本学习过程中最大限度地利用有限的数据。最后一节从特定任务的角度，回顾了适用于不同数据稀缺程度的模型和方法。我们的目标是为从业者提供实用的指导方针，以克服数据受限带来的挑战，同时突出未来研究的有前景的方向。论文地址 引言 预训练语言模型（PLMs）正在改变NLP领域，显示出学习和建模来自复杂和多样化领域的自然语言数据底层分布的出色能力（Han等人，2021）。然而，他们的训练需要大量的数据和计算资源，这在许多现实世界场景中可能是令人望而却步的（Bai et al.，2024），尤其是对于英语以外的语言和专业领域，例如医学（Crema et al.，2023；Van Veen et al.，2021）、化学（Jablonka et...</div></div></div></a><a class="pagination-related" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%9ASqueezed%20Attention%EF%BC%9A%E5%8A%A0%E9%80%9F%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6LLM%E6%8E%A8%E7%90%86/" title="论文阅读二十四：Squeezed Attention：加速长上下文长度LLM推理"><img class="cover" src="/images/squeeze-attention.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-16</div><div class="info-item-2">论文阅读二十四：Squeezed Attention：加速长上下文长度LLM推理</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2024/11/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%EF%BC%9A%E4%BC%98%E5%8C%96%E7%BC%A9%E6%94%BELLM%E6%B5%8B%E8%AF%95%E6%97%B6%E9%97%B4%E8%AE%A1%E7%AE%97%E6%AF%94%E7%BC%A9%E6%94%BE%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E6%9B%B4%E6%9C%89%E6%95%88/" title="论文阅读二十：优化缩放LLM测试时间计算比缩放模型参数更有效"><img class="cover" src="/images/test-time-inference.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-14</div><div class="info-item-2">论文阅读二十：优化缩放LLM测试时间计算比缩放模型参数更有效</div></div><div class="info-2"><div class="info-item-1">使LLM能够通过使用更多的测试时间计算来提高其输出，是构建可以在开放式自然语言上运行的一般自我改进代理的关键一步。本文研究了LLM中推理时间计算的缩放，重点回答了以下问题：如果允许LLM使用固定但非微不足道的推理时间计算，那么它在具有挑战性的提示下能提高多少性能？回答这个问题不仅对LLM的可实现性能有影响，而且对LLM预训练的未来以及如何权衡推理时间和预训练计算也有影响。尽管它很重要，但很少有研究试图了解各种测试时间推理方法的缩放行为。此外，目前的工作在很大程度上为其中一些策略提供了负面结果。在这项工作中，我们分析了两种主要的机制来扩展测试时间计算：（1）针对密集的、基于过程的验证者奖励模型进行搜索；以及（2）在测试时给出提示的情况下自适应地更新模型在响应上的分布。我们发现，在这两种情况下，缩放测试时间计算的不同方法的有效性因提示的难度而异。这一观察结果促使应用...</div></div></div></a><a class="pagination-related" href="/2024/11/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8D%81%E4%B8%80%EF%BC%9A%E7%A9%BF%E9%92%88%E5%BC%95%E7%BA%BF%EF%BC%9ALLMs%E8%83%BD%E5%90%A6%E7%A9%BF%E8%BF%87%E8%BF%91%E7%99%BE%E4%B8%87%E8%A7%84%E6%A8%A1%E7%9A%84%E5%B9%B2%E8%8D%89%E5%A0%86%EF%BC%9F/" title="论文阅读十一：穿针引线：LLMs能否穿过近百万规模的干草堆？"><img class="cover" src="/images/needlethreading.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-10</div><div class="info-item-2">论文阅读十一：穿针引线：LLMs能否穿过近百万规模的干草堆？</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/newlogo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Serge Wang</div><div class="author-info-description">今日事，今日毕</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SergeWang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SergeWang" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sergew027@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E7%9A%84%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF"><span class="toc-number">4.</span> <span class="toc-text">高效的微调技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E4%BC%98%E5%8C%96"><span class="toc-number">4.1.</span> <span class="toc-text">高效优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E8%AE%A1%E7%AE%97"><span class="toc-number">4.2.</span> <span class="toc-text">高效计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLAMAFACTORY%E6%A1%86%E6%9E%B6"><span class="toc-number">5.</span> <span class="toc-text">LLAMAFACTORY框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">5.1.</span> <span class="toc-text">模型加载器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B7%A5%E4%BD%9C%E8%80%85"><span class="toc-number">5.2.</span> <span class="toc-text">数据工作者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%99%A8"><span class="toc-number">5.3.</span> <span class="toc-text">训练器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%84%E4%BB%B6-Utilities"><span class="toc-number">5.4.</span> <span class="toc-text">组件(Utilities)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLAMABOARD%EF%BC%9ALLAMAFACTORY%E7%9A%84%E7%BB%9F%E4%B8%80%E6%8E%A5%E5%8F%A3"><span class="toc-number">5.5.</span> <span class="toc-text">LLAMABOARD：LLAMAFACTORY的统一接口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E8%AF%81%E7%A0%94%E7%A9%B6"><span class="toc-number">6.</span> <span class="toc-text">实证研究</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87"><span class="toc-number">6.1.</span> <span class="toc-text">训练效率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83"><span class="toc-number">6.2.</span> <span class="toc-text">对下游任务进行微调</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="toc-number">7.</span> <span class="toc-text">结论和未来工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E5%B9%BF%E6%B3%9B%E7%9A%84%E5%BD%B1%E5%93%8D%E5%92%8C%E8%B4%9F%E8%B4%A3%E4%BB%BB%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">8.</span> <span class="toc-text">更广泛的影响和负责任的使用</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E5%85%AD%EF%BC%9ASEEDEDIT%EF%BC%9A%E5%B0%86%E5%9B%BE%E5%83%8F%E9%87%8D%E6%96%B0%E7%94%9F%E6%88%90%E4%B8%8E%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E5%AF%B9%E9%BD%90/" title="论文阅读二十六：SEEDEDIT：将图像重新生成与图像编辑对齐"><img src="/images/magicquill.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读二十六：SEEDEDIT：将图像重新生成与图像编辑对齐"/></a><div class="content"><a class="title" href="/2024/11/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E5%85%AD%EF%BC%9ASEEDEDIT%EF%BC%9A%E5%B0%86%E5%9B%BE%E5%83%8F%E9%87%8D%E6%96%B0%E7%94%9F%E6%88%90%E4%B8%8E%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E5%AF%B9%E9%BD%90/" title="论文阅读二十六：SEEDEDIT：将图像重新生成与图像编辑对齐">论文阅读二十六：SEEDEDIT：将图像重新生成与图像编辑对齐</a><time datetime="2024-11-17T04:10:36.000Z" title="发表于 2024-11-17 12:10:36">2024-11-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%9AMagicQuill%EF%BC%9A%E4%B8%80%E4%B8%AA%E6%99%BA%E8%83%BD%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E7%B3%BB%E7%BB%9F/" title="论文阅读二十五：MagicQuill：一个智能交互式图像编辑系统"><img src="/images/magicquill.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读二十五：MagicQuill：一个智能交互式图像编辑系统"/></a><div class="content"><a class="title" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%9AMagicQuill%EF%BC%9A%E4%B8%80%E4%B8%AA%E6%99%BA%E8%83%BD%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E7%B3%BB%E7%BB%9F/" title="论文阅读二十五：MagicQuill：一个智能交互式图像编辑系统">论文阅读二十五：MagicQuill：一个智能交互式图像编辑系统</a><time datetime="2024-11-16T11:44:36.000Z" title="发表于 2024-11-16 19:44:36">2024-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%9ASqueezed%20Attention%EF%BC%9A%E5%8A%A0%E9%80%9F%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6LLM%E6%8E%A8%E7%90%86/" title="论文阅读二十四：Squeezed Attention：加速长上下文长度LLM推理"><img src="/images/squeeze-attention.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读二十四：Squeezed Attention：加速长上下文长度LLM推理"/></a><div class="content"><a class="title" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%9ASqueezed%20Attention%EF%BC%9A%E5%8A%A0%E9%80%9F%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6LLM%E6%8E%A8%E7%90%86/" title="论文阅读二十四：Squeezed Attention：加速长上下文长度LLM推理">论文阅读二十四：Squeezed Attention：加速长上下文长度LLM推理</a><time datetime="2024-11-16T09:28:36.000Z" title="发表于 2024-11-16 17:28:36">2024-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%9B%B6%E6%A0%B7%E6%9C%AC%E7%9F%A5%E8%AF%86%E6%B5%8B%E8%AF%95%E7%9A%84LLM%E5%B9%BB%E8%A7%89%E6%8E%A8%E7%90%86/" title="论文阅读二十三：基于零样本知识测试的LLM幻觉推理"><img src="/images/hallucination-reasoning.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读二十三：基于零样本知识测试的LLM幻觉推理"/></a><div class="content"><a class="title" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%9B%B6%E6%A0%B7%E6%9C%AC%E7%9F%A5%E8%AF%86%E6%B5%8B%E8%AF%95%E7%9A%84LLM%E5%B9%BB%E8%A7%89%E6%8E%A8%E7%90%86/" title="论文阅读二十三：基于零样本知识测试的LLM幻觉推理">论文阅读二十三：基于零样本知识测试的LLM幻觉推理</a><time datetime="2024-11-16T08:03:36.000Z" title="发表于 2024-11-16 16:03:36">2024-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%9A%E6%9C%89%E9%99%90%E6%95%B0%E6%8D%AE%E5%BE%AE%E8%B0%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/" title="论文阅读二十二：有限数据微调语言模型实用指南"><img src="/images/finetune-methods.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读二十二：有限数据微调语言模型实用指南"/></a><div class="content"><a class="title" href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%9A%E6%9C%89%E9%99%90%E6%95%B0%E6%8D%AE%E5%BE%AE%E8%B0%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/" title="论文阅读二十二：有限数据微调语言模型实用指南">论文阅读二十二：有限数据微调语言模型实用指南</a><time datetime="2024-11-16T02:21:36.000Z" title="发表于 2024-11-16 10:21:36">2024-11-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Serge Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>
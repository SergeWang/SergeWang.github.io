<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>综述十一：大模型微调方法 | Model The World</title><meta name="author" content="Serge Wang"><meta name="copyright" content="Serge Wang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大型语言模型微调方法详解（SFT、PT、RM、PPO、DPO 和 KTO） 以下是对几种关键的大型语言模型（LLM）微调方法的比较，包括重要的“RM”（奖励模型）。 1. SFT（监督式微调）  核心思想： 经典的监督学习。使用输入-输出对（提示和期望的回复）的数据集训练 LLM。 数据： 带有清晰的示例，说明模型应该如何回应的标记数据。 过程： 调整 LLM 的权重，以最大限度地减少其预测与数据">
<meta property="og:type" content="article">
<meta property="og:title" content="综述十一：大模型微调方法">
<meta property="og:url" content="https://sergewang.github.io/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%E4%B8%80%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/">
<meta property="og:site_name" content="Model The World">
<meta property="og:description" content="大型语言模型微调方法详解（SFT、PT、RM、PPO、DPO 和 KTO） 以下是对几种关键的大型语言模型（LLM）微调方法的比较，包括重要的“RM”（奖励模型）。 1. SFT（监督式微调）  核心思想： 经典的监督学习。使用输入-输出对（提示和期望的回复）的数据集训练 LLM。 数据： 带有清晰的示例，说明模型应该如何回应的标记数据。 过程： 调整 LLM 的权重，以最大限度地减少其预测与数据">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sergewang.github.io/images/PEFT.png">
<meta property="article:published_time" content="2024-12-01T11:04:52.000Z">
<meta property="article:modified_time" content="2024-12-20T13:31:50.566Z">
<meta property="article:author" content="Serge Wang">
<meta property="article:tag" content="综述">
<meta property="article:tag" content="LMMs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sergewang.github.io/images/PEFT.png"><link rel="shortcut icon" href="/img/newlogo.png"><link rel="canonical" href="https://sergewang.github.io/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%E4%B8%80%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IDUolVgWkW_cmu1mtW5hsxrrIjQfCHvq5hOTOkXeVNE"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9dcb7eb7a8a6225c2b1f242f3b0894bf";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XERFYF0N5K"></script><script>window.dataLayer = window.dataLayer || []
function gtag(){dataLayer.push(arguments)}
gtag('js', new Date())
gtag('config', 'G-XERFYF0N5K')
btf.addGlobalFn('pjaxComplete', () => {
  gtag('config', 'G-XERFYF0N5K', {'page_path': window.location.pathname})
}, 'google_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '综述十一：大模型微调方法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-20 21:31:50'
}</script></head><body><div id="web_bg" style="background-image: url(/img/background.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/newlogo.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/PEFT.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/newlogo.png" alt="Logo"><span class="site-name">Model The World</span></a><a class="nav-page-title" href="/"><span class="site-name">综述十一：大模型微调方法</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">综述十一：大模型微调方法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-01T11:04:52.000Z" title="发表于 2024-12-01 19:04:52">2024-12-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-20T13:31:50.566Z" title="更新于 2024-12-20 21:31:50">2024-12-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LMMs/">LMMs</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong>大型语言模型微调方法详解（SFT、PT、RM、PPO、DPO 和 KTO）</strong></p>
<p>以下是对几种关键的大型语言模型（LLM）微调方法的比较，包括重要的“RM”（奖励模型）。</p>
<p><strong>1. SFT（监督式微调）</strong></p>
<ul>
<li><strong>核心思想：</strong> 经典的监督学习。使用输入-输出对（提示和期望的回复）的数据集训练 LLM。</li>
<li><strong>数据：</strong> 带有清晰的示例，说明模型应该如何回应的标记数据。</li>
<li><strong>过程：</strong> 调整 LLM 的权重，以最大限度地减少其预测与数据集中正确答案之间的差异。</li>
<li><strong>优点：</strong> 易于实现，通常提供强大的基线性能。</li>
<li><strong>缺点：</strong> 需要高质量的标记数据，在捕捉细微的人类偏好或复杂的任务方面可能效果较差。</li>
</ul>
<p><strong>2. PT（提示微调）</strong></p>
<ul>
<li><strong>核心思想：</strong> 不是大幅度改变 LLM 的权重，而是在输入中添加一小组“提示”参数。</li>
<li><strong>数据：</strong> 类似于 SFT，使用输入-输出对。</li>
<li><strong>过程：</strong> 只训练提示参数，引导 LLM 给出更好的回应，而不改变其核心知识。</li>
<li><strong>优点：</strong> 非常节省参数，适用于资源有限的情况。</li>
<li><strong>缺点：</strong> 对于非常复杂的任务，可能无法达到与完全微调（SFT）相同的性能水平。</li>
</ul>
<p><strong>3. RM（奖励模型）</strong></p>
<ul>
<li><strong>核心思想：</strong> 训练一个单独的模型来预测给定 LLM 输出的“奖励”分数，以衡量其好坏程度。</li>
<li><strong>数据：</strong> 需要人类对 LLM 的不同输出进行排名或评分的数据。</li>
<li><strong>过程：</strong> RM 学习将其分数与人类对质量、有用性等的判断对齐。</li>
<li><strong>优点：</strong> 提供了一种自动评估 LLM 输出的方法，对于强化学习方法至关重要。</li>
<li><strong>缺点：</strong> 需要自己的训练数据，如果人类反馈存在缺陷，可能会引入偏差。</li>
</ul>
<p><strong>4. PPO（近端策略优化）</strong></p>
<ul>
<li><strong>核心思想：</strong> 使用强化学习，根据 RM 提供的奖励来微调 LLM。</li>
<li><strong>数据：</strong> 依赖 RM 对 LLM 输出进行评分，隐含地使用了人类偏好。</li>
<li><strong>过程：</strong> 迭代地生成输出，从 RM 获取分数，并更新 LLM 以最大化预期奖励，同时保持更新的稳定性。</li>
<li><strong>优点：</strong> 通过直接优化所需的行为，可以达到非常高的性能。</li>
<li><strong>缺点：</strong> 难以实现和调整，计算量大，对 RM 的质量敏感。</li>
</ul>
<p><strong>5. DPO（直接偏好优化）</strong></p>
<ul>
<li><strong>核心思想：</strong> 一种比 PPO 更简单的新方法，避免了对显式奖励建模的需求。</li>
<li><strong>数据：</strong> 需要成对比较 LLM 输出，其中人类已表明哪个更受欢迎。</li>
<li><strong>过程：</strong> 直接优化 LLM，以增加生成首选输出而不是不太受欢迎的输出的可能性。</li>
<li><strong>优点：</strong> 比 PPO 更稳定且更易于实现，通常可获得可比的结果。</li>
<li><strong>缺点：</strong> 仍然需要人类偏好数据，对于非常复杂的奖励结构可能不如 PPO 灵活。</li>
</ul>
<p><strong>6. KTO（卡尼曼-特沃斯基优化）</strong></p>
<ul>
<li><strong>核心思想：</strong> 将行为经济学（特别是前景理论）的见解纳入微调过程。</li>
<li><strong>数据：</strong> 可以使用各种形式的数据，但侧重于捕捉人类的决策偏差。</li>
<li><strong>过程：</strong> 修改训练目标，以考虑人类如何看待收益和损失，旨在使 LLM 的选择更像人类。</li>
<li><strong>优点：</strong> 可能导致更符合人类行为的输出，尤其是在决策环境中。</li>
<li><strong>缺点：</strong> 相对较新且探索较少，实际效益仍在研究中。</li>
</ul>
<p><strong>关键区别总结</strong></p>
<ul>
<li><strong>SFT 和 PT：</strong> 基于输入-输出示例的监督学习。PT 更节省参数。</li>
<li><strong>RM：</strong> 一个单独的模型，根据人类偏好对 LLM 输出进行评分。</li>
<li><strong>PPO：</strong> 使用 RM 指导 LLM 训练的强化学习。</li>
<li><strong>DPO：</strong> 直接针对输出之间的人类偏好进行优化，无需显式 RM。</li>
<li><strong>KTO：</strong> 侧重于使 LLM 行为与人类决策偏差保持一致。</li>
</ul>
<p><strong>实际应用</strong></p>
<p>通常会将这些方法结合使用。例如，LLM 最初可以使用 SFT 进行训练以获得良好的基础性能，然后使用人类反馈通过 PPO 或 DPO 进一步改进。选择取决于具体目标、可用数据和资源。</p>
<h2 id="分类">分类</h2>
<p><img src="PEFT-TAX.png" alt=""></p>
<h3 id="加性微调（Additive-FT）-基于适配器的微调（Adapter-based-FT）-适配器设计-（Adapter-Design）">加性微调（Additive FT）&gt; 基于适配器的微调（Adapter-based FT） &gt; 适配器设计 （Adapter Design）</h3>
<h4 id="Serial-Adapter-Parameter-Efficient-Transfer-Learning-for-NLP-19-06">Serial Adapter: Parameter-Efficient Transfer Learning for NLP (19/06)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751">论文地址</a><br>
核心思想： 微调大型预训练模型是NLP中一种有效的迁移机制。然而，在存在许多下游任务的情况下，微调是参数效率低下的：每个任务都需要一个全新的模型。作为替代方案，我们建议使用<strong>适配器模块</strong>（adapter modules）进行迁移。适配器模块产生了一个紧凑且可扩展的模型；它们只为每个任务添加少量可训练的参数，并且可以添加新任务而无需重新访问以前的任务。<strong>原始网络的参数保持不变，实现了高度的参数共享</strong>。为了证明适配器的有效性，我们将最近提出的BERT-Transformer模型迁移到26个不同的文本分类任务中，包括GLUE基准测试。适配器达到了接近最先进的性能，同时每个任务只添加了少量参数。在GLUE上，我们达到了完全微调性能的0.4%以内，每个任务只添加了3.6%的参数。相比之下，微调训练每个任务100%的参数。</p>
<p><img src="serial-adapter.png" alt=""></p>
<h3 id="加性微调（Additive-FT）-基于软提示的微调-Soft-Prompt-based-FT-软提示微调-Soft-Prompt-Design">加性微调（Additive FT）&gt; 基于软提示的微调 (Soft Prompt-based FT) &gt; 软提示微调 (Soft Prompt Design)</h3>
<h4 id="Prefix-Tuning-Optimizing-Continuous-Prompts-for-Generation-21-01">Prefix-Tuning: Optimizing Continuous Prompts for Generation (21/01)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00190">论文地址</a><br>
核心思想：微调是利用大型预训练语言模型执行下游任务的事实上的方法。然而，它修改了所有语言模型参数，因此需要为每个任务存储一个完整副本。在本文中，我们提出了<strong>前缀调优，这是一种轻量级的自然语言生成任务微调替代方案，它保持语言模型参数不变，但优化了一个小的连续任务特定向量（称为前缀）</strong>。前缀调优从提示中汲取灵感，允许后续标记关注此前缀，就像它是“虚拟标记”一样。我们将前缀调优应用于GPT-2以生成表到文本，并应用于BART以进行摘要。我们发现，通过只学习0.1%的参数，前缀调优在完整数据设置中获得了相当的性能，在低数据设置中表现优于微调，并且可以更好地推断出在训练过程中看不到主题的示例。</p>
<p><img src="prefix-tuning.png" alt=""></p>
<h4 id="Prefix-Propagation-Parameter-Efficient-Tuning-for-Long-Sequences-23-05">Prefix Propagation: Parameter-Efficient Tuning for Long Sequences (23/05)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.12086">论文地址</a><br>
核心思想：参数高效调优旨在减轻为下游任务调整预训练语言模型的大内存需求。例如，一种流行的方法，前缀调优，在冻结模型其余参数的同时，为序列添加可训练的标记。尽管这些模型在应用于短到中等长度的序列时，通过微调可以获得相当的性能，<strong>但在建模长序列时，我们发现它们的性能较差</strong>。为了弥合这一差距，我们提出了<strong>前缀传播</strong>，这是一种简单但有效的方法，可以<strong>根据之前的隐藏状态来调整前缀</strong>。我们实证证明，在长文档任务中，前缀传播的性能优于前缀调优，同时使用的参数减少了约50%。为了进一步研究所提出的架构，我们还展示了它在校准方面的优势，并对它与内核注意力的关系进行了进一步的研究。据我们所知，这项工作是第一个专注于长序列语言任务的参数高效学习的工作。<br>
<img src="prefix-propagation.png" alt=""></p>
<h4 id="P-Tuning-v2-Prompt-Tuning-Can-Be-Comparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.07602">论文地址</a><br>
核心思想：提示调优仅使用冻结的语言模型对连续提示进行调优，大大减少了训练时的每任务存储和内存使用。然而，在NLU的背景下，先前的工作表明，对于正常大小的预训练模型，快速调优的效果并不好。我们还发现，现有的提示调优方法无法处理<strong>硬序列标记</strong>任务，表明缺乏通用性。我们提出了一个新的实证发现，<strong>适当优化的提示调优可以在广泛的模型尺度和NLU任务中普遍有效</strong>。它与微调的性能相匹配，但只有0.1%-3%的参数可调。我们的方法P-Tuning v2是<strong>针对NLU优化和调整的深度提示调优的实现</strong>。鉴于P-Tuning v2的普遍性和简单性，我们认为它可以作为微调的替代品，并为未来的研究提供强有力的基线。</p>
<p><img src="p-tuning-v2.png" alt=""></p>
<h4 id="APT-Towards-Adaptive-Prefix-Tuning-for-Parameter-Efficient-Language-Model-Fine-tuning-23-05">APT: Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning (23/05)</h4>
<p>核心思想：在各种下游任务上使用整个参数对大型预训练语言模型进行微调是非常昂贵的。因此，参数高效微调引起了人们的关注，它只使用冻结的预训练模型优化了一些特定任务的参数。在这项工作中，我们专注于前缀调优，它只优化插入到Transformer层中的连续前缀向量（即伪令牌）。基于观察到学习到的语法和语义表示在不同层上差异很大，我们认为自适应前缀将比固定前缀更适合每一层，从而使微调更加有效和高效。因此，我们提出了自适应前缀调整（APT），通过门机制在细粒度令牌级别和粗粒度层级别调整前缀。在SuperGLUE和NER数据集上的实验表明了APT的有效性。此外，以门为探测对象，我们验证了可变前缀的效率和有效性。</p>
<p><img src="APT.png" alt=""></p>
<h4 id="p-tuning-GPT-Understands-Too-21-03">p-tuning: GPT Understands, Too (21/03)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.10385">论文地址</a><br>
核心思想：事实证明，用自然语言模式构建预训练语言模型对自然语言理解（NLU）是有效的。然而，我们的初步研究表明，<strong>手动离散提示通常会导致性能不稳定</strong>，例如，更改提示中的单个单词可能会导致性能大幅下降。我们提出了一种新的P-Tuning方法，该方法<strong>采用可训练的连续提示嵌入与离散提示相结合</strong>。根据经验，P-Tuning不仅通过最小化各种离散提示之间的差距来稳定训练，而且在包括LAMA和SuperGLUE在内的各种NLU任务上也大大提高了性能。P-Tuning通常在完全监督和少样本设置下对冻结和调优的语言模型都有效。<br>
<img src="p-tuning.png" alt=""></p>
<h4 id="prompt-tuning-The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning-21-04">prompt-tuning: The Power of Scale for Parameter-Efficient Prompt Tuning (21/04)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.08691">论文地址</a><br>
核心思想：在这项工作中，我们探索了“提示调优”，这是一种简单而有效的机制，用于学习“软提示”来调节冻结的语言模型，以执行特定的下游任务。与GPT-3使用的离散文本提示不同，软提示是通过反向传播学习的，可以调整为包含来自任何数量的标记示例的信号。我们的端到端学习方法大大优于GPT-3的少样本学习。更值得注意的是，通过使用T5对模型尺寸进行消融，我们发现快速调整与规模更具竞争力：随着模型超过数十亿个参数，我们的方法“缩小了差距”，并与模型调整的强大性能相匹配（所有模型权重都经过调整）。这一发现尤其重要，因为大型模型的共享和服务成本很高，而为多个下游任务重用一个冻结模型的能力可以减轻这一负担。我们的方法可以看作是<strong>最近提出的“前缀调优”的简化</strong>，我们将其与其他类似方法进行了比较。最后，我们表明，用软提示调节冻结模型可以增强对域转移的鲁棒性，并实现高效的“提示集成”。</p>
<p><img src="prompt-tuning.png" alt=""></p>
<h4 id="XPrompt-Exploring-the-Extreme-of-Prompt-Tuning-22-10">XPrompt: Exploring the Extreme of Prompt Tuning (22/10)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.04457">论文地址</a><br>
核心思想：提示调优学习软提示，以调节冻结的预训练语言模型（PLM），从而以参数高效的方式执行下游任务。虽然随着模型规模的增加，快速调优已逐渐达到微调的性能水平，但对于中小规模的模型（通常小于11B参数），快速调优和微调之间仍存在较大的性能差距。在本文中，我们实证表明，训练好的提示标记会对下游任务产生负面影响，从而降低其性能。为了弥合这一差距，我们提出了一种在彩票假设下具有极小尺度的PROMPT调谐模型（XPROMPT）。具体来说，XPROMPT<strong>通过分层结构化修剪消除了不同粒度级别的负提示令牌</strong>，从而产生了一个更具参数效率但具有竞争力的提示。对SuperGLUE任务进行了全面的实验，广泛的结果表明，XPROMPT能够在较小的模型尺度上缩小性能差距。<br>
<img src="xprompt.png" alt=""></p>
<h4 id="IDPG-An-Instance-Dependent-Prompt-Generation-Method-22-04">IDPG: An Instance-Dependent Prompt Generation Method (22/04)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.04497">论文地址</a><br>
核心思想：提示调优是一种新的、高效的NLP迁移学习范式，它在<strong>模型训练阶段的每个输入实例中添加了一个特定于任务的提示</strong>。它<strong>冻结了预训练的语言模型，只优化了一些特定于任务的提示</strong>。在本文中，我们提出了一种<strong>条件提示生成方法，为每个输入实例生成提示</strong>，称为实例依赖提示生成（IDPG）。与使用<strong>固定提示</strong>的传统提示调优方法不同，IDPG引入了一个<strong>轻量级且可训练的组件，可以根据每个输入句子生成提示</strong>。对10个自然语言理解（NLU）任务的广泛实验表明，所提出的策略始终优于各种提示调整基线，并且与Compacter等其他有效的迁移学习方法相当，同时调整的模型参数要少得多。<br>
<img src="IDPG.png" alt=""></p>
<h4 id="Late-Prompt-Tuning-A-Late-Prompt-Could-Be-Better-Than-Many-Prompts-22-10">Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts (22/10)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.11292">论文地址</a><br>
核心思想：提示调优是一种参数高效调优（PETuning）方法，用于利用预训练模型（PTM），该方法只需在输入前添加软提示，并仅优化提示以使PTM适应下游任务。尽管它具有参数和部署效率，但其性能仍然落后于其他最先进的PETuning方法。此外，由于整个模型的反向传播，快速调整的训练成本并没有显著降低。通过实证分析，我们揭示了提示调谐的滞后性能，并认识到从标签信号到插入提示的传播距离与提示对模型输出的影响之间的权衡。此外，我们提出了延迟提示调优（LPT），它将延迟提示插入PTM的中间层，而不是输入层或所有层。后期提示是由神经提示生成器获得的，该生成器以提示插入层之前的隐藏状态为条件，因此具有实例依赖性。通过各种任务和PTM的广泛实验结果，我们表明LPT可以在全数据和少镜头场景下实现与全模型调优和其他PETuning方法竞争的性能，同时具有更快的训练速度和更低的内存成本。</p>
<p><img src="lpt-overall.png" alt=""></p>
<p><img src="LPT.png" alt=""></p>
<h4 id="SPT-Learning-to-Selectively-Insert-Prompts-for-Better-Prompt-Tuning-23-10">SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning (23/10)</h4>
<p><a target="_blank" rel="noopener" href="https://aclanthology.org/2023.emnlp-main.727/">论文地址</a><br>
核心思想：提示调优在输入嵌入或隐藏状态之前添加一个软提示，并且只优化提示以使预训练模型（PTM）适应下游任务。之前的工作手动选择了远非最佳的提示层，并且未能利用提示调优的潜力。在这项工作中，我们提出了一种新的框架，<strong>选择性提示调优</strong>（SPT），它通过<strong>在每个中间层插入由可学习的概率门控制的提示</strong>来<strong>学习选择适当的提示层</strong>。我们进一步提出了一种新的双层优化框架SPT-DARS，可以更好地优化可学习门，并提高学习提示层设置的最终提示调优性能。我们在全数据和少镜头场景下对十个基准数据集进行了广泛的实验。结果表明，我们的SPT框架在可调参数相当或更少的情况下，可以比以前最先进的PETuning基线表现更好。</p>
<p><img src="spt-overall.png" alt=""></p>
<p><img src="SPT.png" alt=""></p>
<h4 id="APrompt-Attention-Prompt-Tuning-for-Efficient-Adaptation-of-Pre-trained-Language-Models-23-10">APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models (23/10)</h4>
<p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=RSuN6p3wXR">论文地址</a><br>
核心思想：随着大型语言模型的不断增长，为新任务微调这些模型的过程变得越来越参数密集。提示调优是一种涉及调优一小部分软提示的方法，已成为适应大型预训练语言模型的有效方法。然而，<strong>大多数现有的提示调优方法只在输入层引入提示，限制了它们的性能</strong>，并留下了很大的改进空间。在这项工作中，我们提出了<strong>一种新的注意力提示调优</strong>方法，即APROMPT，用于有效地适应预训练的语言模型。我们首先证明，<strong>现有的提示调优可以被视为注意力提示调优的一个特例</strong>。然后，我们正式引入APROMPT，它<strong>将查询、键和值提示整合到注意力层中，以在微调过程中指导注意力计算</strong>。SuperGLUE基准测试的实验结果一致表明，我们提出的方法在不同尺度下优于最先进的基线和预训练模型的完全微调方法。此外，一系列全面的消融研究验证了快速设计的有效性以及我们方法的效率。<br>
<img src="Aprompt.png" alt=""></p>
<h3 id="加性微调（Additive-FT）-基于软提示的微调-Soft-Prompt-based-FT-训练假设-Trainng-Speedup">加性微调（Additive FT）&gt; 基于软提示的微调 (Soft Prompt-based FT) &gt; 训练假设 (Trainng Speedup)</h3>
<h4 id="SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer-21-10">SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer (21/10)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.07904">论文地址</a><br>
核心思想：人们对将预训练的语言模型应用于下游任务的参数高效方法越来越感兴趣。基于Lester等人（2021）的PROMPTTUNING方法，该方法学习特定于任务的软提示，以调节冻结的预训练模型执行不同的任务，我们提出了一种新的基于提示的迁移学习方法，称为SPOT:软提示迁移。SPOT<strong>首先学习一个或多个源任务上的提示</strong>，<strong>然后使用它来初始化目标任务的提示</strong>。我们发现，SPOT在许多任务中显著提高了PROMPTUNING的性能。更值得注意的是，在所有模型尺寸中，SPOT在SUPERGLUE基准测试中与标准MODELTUNING（对所有模型参数进行微调）相匹配或表现出色，同时使用的任务特定参数减少了27000倍。为了了解SPOT在哪里最有效，我们对160个组合中的26个NLP任务的任务可转移性进行了大规模研究，并证明许多任务可以通过快速转移相互受益。最后，我们提出了一种高效的检索方法，将任务提示解释为任务嵌入，以识别相似的任务，并预测新目标任务的最可转移的源任务。<br>
<img src="SPoT.png" alt=""></p>
<h3 id="选择性PEFT-Selective-PEFT-非结构化掩码-Unstructural-Masking">选择性PEFT (Selective PEFT) &gt; 非结构化掩码 (Unstructural Masking)</h3>
<h4 id="U-Diff-Prunning-Parameter-Efficient-Transfer-Learning-with-Diff-Pruning-20-12">U-Diff Prunning: Parameter-Efficient Transfer Learning with Diff Pruning (20/12)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.07463">论文地址</a><br>
核心思想：预训练网络的大尺寸使得它们难以在存储受限的环境中部署用于多个任务。Diff剪枝实现了参数高效的迁移学习，可以很好地适应新任务。该方法<strong>学习一个特定于任务的“diff”向量，该向量扩展了原始预训练参数</strong>。在<strong>训练过程中，使用L0范数惩罚的可微近似自适应地修剪这个差异（diff）向量，以鼓励稀疏性</strong>。随着任务数量的增加，差异修剪仍然是参数高效的，因为它只需要为每个任务存储一个小的差异向量。由于它不需要在训练期间访问所有任务，因此在任务以流形式甚至从不同提供商到达的设备部署设置中很有吸引力。Diff剪枝可以与GLUE基准上微调基线的性能相匹配，同时每项任务只修改预训练模型参数的0.5%，与流行的剪枝方法相比具有良好的扩展性。</p>
<h3 id="在参数化微调-Reparameterized-FT-低秩分解-Low-rank-Decomposition">在参数化微调 (Reparameterized FT) &gt; 低秩分解 (Low-rank Decomposition)</h3>
<h4 id="Intrinsic-SAID-Intrinsic-Dimensionality-Explains-the-Effectiveness-of-Language-Model-Fine-Tuning-20-12">Intrinsic SAID: Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning (20/12)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.13255">论文地址</a><br>
核心思想：尽管预训练的语言模型可以进行微调，为非常广泛的语言理解任务产生最先进的结果，但这一过程的动态并没有得到很好的理解，特别是在低数据状态下。**为什么我们可以使用相对普通的梯度下降算法（例如，没有强正则化）来调整只有数百或数千个标记示例的数据集上具有数亿个参数的模型？**本文认为，通过内在维度的视角分析微调，为我们解释这一显著现象提供了经验和理论直觉。我们实证表明，<strong>常见的预训练模型具有非常低的内在维度</strong>；换句话说，<strong>存在一种低维重新参数化，它对微调的效果与全参数空间一样有效</strong>。例如，通过仅优化随机投影回全空间的200个可训练参数，我们可以调整RoBERTa模型，使其在MRPC上达到全参数性能水平的90%。此外，我们实证表明，预训练隐含地最小化了内在维度，也许令人惊讶的是，在固定数量的预训练更新后，较大的模型往往具有较低的内在维度，这至少部分解释了它们的极端有效性。最后，我们将内在维度与低维任务表示和基于压缩的泛化边界联系起来，以提供独立于全参数计数的内在维度泛化边界。</p>
<h4 id="LoRA-Low-Rank-Adaptation-of-Large-Language-Models-21-06">LoRA: Low-Rank Adaptation of Large Language Models (21/06)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">论文地址</a><br>
核心思想：自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。随着我们对更大的模型进行预训练，重新训练所有模型参数的完全微调变得不那么可行。以GPT-3 175B为例，部署每个都有175B参数的微调模型的独立实例是非常昂贵的。我们提出了低秩自适应（LoRA），它冻结了预训练的模型权重，并<strong>将可训练的秩分解矩阵注入到Transformer架构的每一层</strong>，大大减少了下游任务的可训练参数数量。与Adam微调的GPT-3 175B相比，LoRA可以将可训练参数的数量减少10000倍，GPU内存需求减少3倍。LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3上的模型质量方面与微调相当或更好，尽管其可训练参数较少，训练吞吐量较高，并且与适配器不同，没有额外的推理延迟。我们还对语言模型适应中的等级缺陷进行了实证研究，揭示了LoRA的有效性。我们发布了一个软件包，该软件包促进了LoRA与PyTorch模型的集成，并为RoBERTa、DeBERTa和GPT-2提供了我们的实现和模型检查点<a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a> 。</p>
<p><img src="LoRA.png" alt=""></p>
<h4 id="A-Note-on-LoRA-24-04">A Note on LoRA (24/04)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.05086">论文地址</a><br>
核心思想：LoRA（低秩自适应）已成为高效适应大型语言模型（LLM）的首选方法，具有显著的简单性和有效性。本说明扩展了最初的LoRA论文，提供了最初未讨论的新观点，并为大规模部署LoRA提供了一系列见解。在不引入新实验的情况下，我们的目标是提高对LoRA的理解和应用。</p>
<h2 id="PEFT设计">PEFT设计</h2>
<p><img src="PEFT-design.png" alt=""></p>
<h2 id="分析">分析</h2>
<h3 id="提示微调">提示微调</h3>
<h4 id="When-do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations-23-10">When do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations (23/10)</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.19698">论文地址</a><br>
核心思想：基于上下文的微调方法，包括提示（prompting）、上下文学习（in-context learning）、软提示（soft prompting，也称为提示调优（prompt tuning））和前缀调优（prefix-tunning），因其能够经常将完全微调的性能与一小部分参数相匹配而广受欢迎。尽管它们在实证上取得了成功，但对于这些技术<strong>如何影响模型的内部计算及其表现力的局限性</strong>，理论上几乎没有什么了解。我们发现，尽管<strong>连续嵌入空间比离散标记空间更具表现力</strong>，但即使具有相同数量的可学习参数，软提示和前缀调优的表现力也可能不如完全微调。具体来说，基于上下文的微调<strong>不能改变内容上的相对注意力模式，只能将注意力层的输出偏向固定方向</strong>。这表明，虽然提示、情境学习、软提示和预加等技术可以有效地引出预训练模型中存在的技能，但它们可能<strong>无法学习需要新注意力模式的新任务</strong>。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SergeWang.github.io">Serge Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sergewang.github.io/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%E4%B8%80%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/">https://sergewang.github.io/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%E4%B8%80%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SergeWang.github.io" target="_blank">Model The World</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a><a class="post-meta__tags" href="/tags/LMMs/">LMMs</a></div><div class="post-share"><div class="social-share" data-image="/images/PEFT.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%9B%9B%E5%8D%81%E4%B8%83%EF%BC%9A3DGS-zip-3D%E9%AB%98%E6%96%AF%E6%B3%BC%E6%BA%85%E5%8E%8B%E7%BC%A9%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0/" title="论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述"><img class="cover" src="/images/3dgszip.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">论文阅读四十七：3DGS.zip:3D高斯泼溅压缩方法综述</div></div><div class="info-2"><div class="info-item-1">摘要 3D 高斯泼溅（3DGS）已成为实时辐射场渲染的前沿技术，提供质量和速度的先进性能。3DGS将场景建模为三维高斯的集合，或“泼溅”，以及额外的属性优化以符合场景的集合和视觉特性。尽管它在渲染速度和图像保真度中的优势，3DGS受到其显著的存储和内存需要的限制。这些高需求使得3DGS对于移动设备或耳机不实际，减少它在计算机图形学重要领域中的应用。为解决这些挑战，并促进3DGS的实用，该先进报告（STAR）提供综合和细致的关于3DGS更加有效的压缩和压实技术的检测。我们分类当前方法为压缩技术，旨在以最小数据量获得的最高质量，和压实技术，旨在使用最少的高斯取得最优质量。我们介绍了所分析方法背后的基本数学概念，以及关键的实现细节和设计选择。我们的报告深入讨论了这些方法之间的异同，以及它们各自的优缺点。我们根据关键性能指标和数据集建立了比较这些方法的一致标准。具体而言，由于这些方法是在短时间内并行开发的，目前还没有全面的比较。这项调查首次提出了评估3DGS压缩技术的统一标准。为了促进对新兴方法的持续监测，我们维护了一个专门的网站，该网站将定期更新新技术和对现有发现的修订...</div></div></div></a><a class="pagination-related" href="/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%E4%BA%8C%EF%BC%9A%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" title="综述十二：视频生成模型"><img class="cover" src="/images/PEFT.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">综述十二：视频生成模型</div></div><div class="info-2"><div class="info-item-1">模型 STIV：Scalable Text and Image Conditioned Video Generation (24/12) 论文地址 核心思想：视频生成领域取得了显著进展，但仍然迫切需要一个清晰、系统的配方来指导稳健和可扩展模型的开发。在这项工作中，我们提出了一项全面的研究，系统地探讨了模型架构、训练配方和数据管理策略的相互作用，最终提出了一种简单且可扩展的文本图像条件视频生成方法，称为STIV。我们的框架通过帧替换将图像条件集成到扩散Transformer（DiT）中，同时通过图像-文本条件无分类的联合引导引入文本条件。这种设计使STIV能够同时执行文本到视频（T2V）和文本图像到视频（TI2V）任务。此外，STIV可以很容易地扩展到各种应用，如视频预测、帧插值、多视图生成和长视频生成等。通过对T2I、T2V和TI2V的全面消融研究，STIV尽管设计简单，但表现出了强大的性能。分辨率为5122的8.7B型号在VBench...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/01/%E7%BB%BC%E8%BF%B0%E5%8D%81%EF%BC%9A%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/" title="综述十：多模态模型"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-01</div><div class="info-item-2">综述十：多模态模型</div></div><div class="info-2"><div class="info-item-1">模型 基础模型 LLaVA: Visual Instruction Tuning (23/04) 论文地址 代码 核心思想：指令调优大型语言模型（LLMs）使用机器生成指令遵循数据，被证明在新任务上提升零样本能力，但这一想法在多模态领域的探索较少。提出第一个尝试，使用仅语言GPT-4来生成多模态语言图像指令遵循数据。通过在这种生成数据上指令调优，引入LLaVA:大型语言和视觉助手，一种端到端训练的大型多模态模型，连接视觉编码器和LLM，用于通用用途视觉和语言理解。为促进在视觉指令遵循上的进一步研究，构建两个具有多样性和挑战性的面向应用任务的评估基准。  Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond...</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%80%EF%BC%9ATransformer%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述一：Transformer及其变体"><img class="cover" src="/images/transformer.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述一：Transformer及其变体</div></div><div class="info-2"><div class="info-item-1">Transformer架构，因其自注意力机制而闻名，能够让模型根据输入序列中不同的标记之间的关系进行加权。这种机制消除了循环神经网络的需求，使得训练变得更加高效。自从2017年原始Transformer的提出以来，已经出现了多个变种，旨在优化性能、扩展其适用范围或解决一些挑战。以下是一些著名的Transformer变种，特别是那些专注于自注意力机制的： 1. 原始Transformer (Vanilla Transformer)  关键特性：原始Transformer架构包括一个编码器-解码器结构，采用自注意力机制。 目的：消除了递归神经网络的需求，使得模型训练更加并行化和高效。 自注意力机制：多头自注意力，通过计算输入序列中所有标记之间的关系来决定重要性。  2. BERT（双向编码器表示）  关键特性：BERT只使用Transformer的编码器部分，采用双向自注意力机制来捕获标记的左右上下文。 目的：通过掩蔽语言模型（MLM）进行预训练，并可以通过微调来提升在下游NLP任务中的表现。  3....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%83%EF%BC%9ACNN%E6%A8%A1%E5%9E%8B/" title="综述七：CNN模型"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述七：CNN模型</div></div><div class="info-2"><div class="info-item-1">Here’s a list of major CNN (Convolutional Neural Network) variants ordered by the time of their publication. CNNs have evolved over the years to improve accuracy, reduce computational costs, and adapt to different domains.  1. LeNet (1998)  Key Idea: One of the first CNN architectures, designed for handwritten digit recognition (MNIST). It used convolutional layers followed by pooling and fully connected layers. Application: Digit classification. Notable Paper: Yann LeCun et al.,...</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B8%89%EF%BC%9A%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%85%B6%E6%96%B9%E6%B3%95/" title="综述三：持续学习及其方法"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述三：持续学习及其方法</div></div><div class="info-2"><div class="info-item-1">持续学习（Continual Learning，CL），也称为终身学习（Lifelong Learning），指的是模型能够从持续不断的数据流中学习，并随着时间的推移不断适应和获得新知识，而不会遗忘先前学习的内容。持续学习面临的一个主要挑战是灾难性遗忘（Catastrophic Forgetting），即在学习新任务时，模型容易遗忘之前学习的任务。 为了应对这些挑战，提出了多种方法，这些方法可以根据它们如何处理遗忘、如何存储知识以及如何使用新数据进行分类。 下面是主要的持续学习方法，按它们所采用的主要策略进行组织： 1....</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9ALLM/" title="综述九：LLM"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述九：LLM</div></div><div class="info-2"><div class="info-item-1">提示（Prompts） Large Lanuage Models Can Self-Improve in Long-context Reasoning (24/10) 论文地址 核心思想：大型语言模型（LLM）在处理长上下文方面取得了实质性进展，但在长上下文推理方面仍存在困难。现有的方法通常涉及使用合成数据对LLM进行微调，这取决于人类专家的注释或GPT-4等高级模型，从而限制了进一步的进步。为了解决这个问题，研究了LLM在长上下文推理中自我改进的潜力，并提出了专门为此目的设计的SEALONG方法。这种方法很简单：对每个问题的多个输出进行采样，用最小贝叶斯风险对其进行评分，然后根据这些输出进行监督微调或偏好优化。在几个领先的LLM上进行的广泛实验证明了SEALONG的有效性，Llama-3.1-8B-Instruct的绝对提高了4.2分。此外，与依赖于人类专家或高级模型生成的数据的先前方法相比，SEALONG实现了更优的性能。我们预计，这项工作将为长期情景下的自我提升技术开辟新的途径，这对LLM的持续发展至关重要。 </div></div></div></a><a class="pagination-related" href="/2024/11/24/%E7%BB%BC%E8%BF%B0%E4%B9%9D%EF%BC%9AMamba%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/" title="综述九：Mamba及其变体"><img class="cover" src="/images/mamba.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">综述九：Mamba及其变体</div></div><div class="info-2"><div class="info-item-1">SSM模型 Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (20/06) 论文地址 核心思想：将自注意表示为核特征图的线性点积，并使用矩阵乘法的结合属性将复杂度从 O(N2)\mathcal{O}(N^2)O(N2) 减少到 O(N)\mathcal{O}(N)O(N) ，其中N是序列长度。我们证明，这个公式允许迭代实现，大大加速了自回归Transformers，并揭示了它们与循环神经网络的关系。(一种涉及循环的自我注意近似，可以看作是退化的线性SSM)  Hungry Hungry Hippos: Towards Language Modeling with State Space Models...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/newlogo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Serge Wang</div><div class="author-info-description">今日事，今日毕</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SergeWang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SergeWang" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sergew027@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E5%BE%AE%E8%B0%83%EF%BC%88Additive-FT%EF%BC%89-%E5%9F%BA%E4%BA%8E%E9%80%82%E9%85%8D%E5%99%A8%E7%9A%84%E5%BE%AE%E8%B0%83%EF%BC%88Adapter-based-FT%EF%BC%89-%E9%80%82%E9%85%8D%E5%99%A8%E8%AE%BE%E8%AE%A1-%EF%BC%88Adapter-Design%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">加性微调（Additive FT）&gt; 基于适配器的微调（Adapter-based FT） &gt; 适配器设计 （Adapter Design）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Serial-Adapter-Parameter-Efficient-Transfer-Learning-for-NLP-19-06"><span class="toc-number">1.1.1.</span> <span class="toc-text">Serial Adapter: Parameter-Efficient Transfer Learning for NLP (19&#x2F;06)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E5%BE%AE%E8%B0%83%EF%BC%88Additive-FT%EF%BC%89-%E5%9F%BA%E4%BA%8E%E8%BD%AF%E6%8F%90%E7%A4%BA%E7%9A%84%E5%BE%AE%E8%B0%83-Soft-Prompt-based-FT-%E8%BD%AF%E6%8F%90%E7%A4%BA%E5%BE%AE%E8%B0%83-Soft-Prompt-Design"><span class="toc-number">1.2.</span> <span class="toc-text">加性微调（Additive FT）&gt; 基于软提示的微调 (Soft Prompt-based FT) &gt; 软提示微调 (Soft Prompt Design)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Prefix-Tuning-Optimizing-Continuous-Prompts-for-Generation-21-01"><span class="toc-number">1.2.1.</span> <span class="toc-text">Prefix-Tuning: Optimizing Continuous Prompts for Generation (21&#x2F;01)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Prefix-Propagation-Parameter-Efficient-Tuning-for-Long-Sequences-23-05"><span class="toc-number">1.2.2.</span> <span class="toc-text">Prefix Propagation: Parameter-Efficient Tuning for Long Sequences (23&#x2F;05)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#P-Tuning-v2-Prompt-Tuning-Can-Be-Comparable-to-Fine-tuning-Universally-Across-Scales-and-Tasks"><span class="toc-number">1.2.3.</span> <span class="toc-text">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#APT-Towards-Adaptive-Prefix-Tuning-for-Parameter-Efficient-Language-Model-Fine-tuning-23-05"><span class="toc-number">1.2.4.</span> <span class="toc-text">APT: Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning (23&#x2F;05)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#p-tuning-GPT-Understands-Too-21-03"><span class="toc-number">1.2.5.</span> <span class="toc-text">p-tuning: GPT Understands, Too (21&#x2F;03)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#prompt-tuning-The-Power-of-Scale-for-Parameter-Efficient-Prompt-Tuning-21-04"><span class="toc-number">1.2.6.</span> <span class="toc-text">prompt-tuning: The Power of Scale for Parameter-Efficient Prompt Tuning (21&#x2F;04)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#XPrompt-Exploring-the-Extreme-of-Prompt-Tuning-22-10"><span class="toc-number">1.2.7.</span> <span class="toc-text">XPrompt: Exploring the Extreme of Prompt Tuning (22&#x2F;10)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IDPG-An-Instance-Dependent-Prompt-Generation-Method-22-04"><span class="toc-number">1.2.8.</span> <span class="toc-text">IDPG: An Instance-Dependent Prompt Generation Method (22&#x2F;04)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Late-Prompt-Tuning-A-Late-Prompt-Could-Be-Better-Than-Many-Prompts-22-10"><span class="toc-number">1.2.9.</span> <span class="toc-text">Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts (22&#x2F;10)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SPT-Learning-to-Selectively-Insert-Prompts-for-Better-Prompt-Tuning-23-10"><span class="toc-number">1.2.10.</span> <span class="toc-text">SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning (23&#x2F;10)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#APrompt-Attention-Prompt-Tuning-for-Efficient-Adaptation-of-Pre-trained-Language-Models-23-10"><span class="toc-number">1.2.11.</span> <span class="toc-text">APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models (23&#x2F;10)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E5%BE%AE%E8%B0%83%EF%BC%88Additive-FT%EF%BC%89-%E5%9F%BA%E4%BA%8E%E8%BD%AF%E6%8F%90%E7%A4%BA%E7%9A%84%E5%BE%AE%E8%B0%83-Soft-Prompt-based-FT-%E8%AE%AD%E7%BB%83%E5%81%87%E8%AE%BE-Trainng-Speedup"><span class="toc-number">1.3.</span> <span class="toc-text">加性微调（Additive FT）&gt; 基于软提示的微调 (Soft Prompt-based FT) &gt; 训练假设 (Trainng Speedup)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SPoT-Better-Frozen-Model-Adaptation-through-Soft-Prompt-Transfer-21-10"><span class="toc-number">1.3.1.</span> <span class="toc-text">SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer (21&#x2F;10)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E6%80%A7PEFT-Selective-PEFT-%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%8E%A9%E7%A0%81-Unstructural-Masking"><span class="toc-number">1.4.</span> <span class="toc-text">选择性PEFT (Selective PEFT) &gt; 非结构化掩码 (Unstructural Masking)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#U-Diff-Prunning-Parameter-Efficient-Transfer-Learning-with-Diff-Pruning-20-12"><span class="toc-number">1.4.1.</span> <span class="toc-text">U-Diff Prunning: Parameter-Efficient Transfer Learning with Diff Pruning (20&#x2F;12)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E5%8F%82%E6%95%B0%E5%8C%96%E5%BE%AE%E8%B0%83-Reparameterized-FT-%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3-Low-rank-Decomposition"><span class="toc-number">1.5.</span> <span class="toc-text">在参数化微调 (Reparameterized FT) &gt; 低秩分解 (Low-rank Decomposition)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Intrinsic-SAID-Intrinsic-Dimensionality-Explains-the-Effectiveness-of-Language-Model-Fine-Tuning-20-12"><span class="toc-number">1.5.1.</span> <span class="toc-text">Intrinsic SAID: Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning (20&#x2F;12)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LoRA-Low-Rank-Adaptation-of-Large-Language-Models-21-06"><span class="toc-number">1.5.2.</span> <span class="toc-text">LoRA: Low-Rank Adaptation of Large Language Models (21&#x2F;06)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#A-Note-on-LoRA-24-04"><span class="toc-number">1.5.3.</span> <span class="toc-text">A Note on LoRA (24&#x2F;04)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PEFT%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.</span> <span class="toc-text">PEFT设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E5%BE%AE%E8%B0%83"><span class="toc-number">3.1.</span> <span class="toc-text">提示微调</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#When-do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations-23-10"><span class="toc-number">3.1.1.</span> <span class="toc-text">When do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations (23&#x2F;10)</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%BA%94%EF%BC%9ATransformer2%EF%BC%9A%E8%87%AA%E9%80%82%E5%BA%94LLMs/" title="论文阅读五十五：Transformer2：自适应LLMs"><img src="/images/transformer2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读五十五：Transformer2：自适应LLMs"/></a><div class="content"><a class="title" href="/2025/01/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%BA%94%EF%BC%9ATransformer2%EF%BC%9A%E8%87%AA%E9%80%82%E5%BA%94LLMs/" title="论文阅读五十五：Transformer2：自适应LLMs">论文阅读五十五：Transformer2：自适应LLMs</a><time datetime="2025-01-15T07:54:31.000Z" title="发表于 2025-01-15 15:54:31">2025-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E5%9B%9B%EF%BC%9ATitans%EF%BC%9A%E5%9C%A8%E6%B5%8B%E8%AF%95%E6%97%B6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BF%86/" title="论文阅读五十四：Titans：在测试时学习记忆"><img src="/images/titans.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读五十四：Titans：在测试时学习记忆"/></a><div class="content"><a class="title" href="/2025/01/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E5%9B%9B%EF%BC%9ATitans%EF%BC%9A%E5%9C%A8%E6%B5%8B%E8%AF%95%E6%97%B6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BF%86/" title="论文阅读五十四：Titans：在测试时学习记忆">论文阅读五十四：Titans：在测试时学习记忆</a><time datetime="2025-01-15T07:29:34.000Z" title="发表于 2025-01-15 15:29:34">2025-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%B8%89%EF%BC%9AGAN%E5%B7%B2%E6%AD%BB%EF%BC%9BGAN%E4%B8%87%E5%B2%81%EF%BC%81%E7%8E%B0%E4%BB%A3GAN%E5%9F%BA%E7%BA%BF/" title="论文阅读五十三：GAN已死；GAN万岁！现代GAN基线"><img src="/images/R3GAN.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读五十三：GAN已死；GAN万岁！现代GAN基线"/></a><div class="content"><a class="title" href="/2025/01/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%B8%89%EF%BC%9AGAN%E5%B7%B2%E6%AD%BB%EF%BC%9BGAN%E4%B8%87%E5%B2%81%EF%BC%81%E7%8E%B0%E4%BB%A3GAN%E5%9F%BA%E7%BA%BF/" title="论文阅读五十三：GAN已死；GAN万岁！现代GAN基线">论文阅读五十三：GAN已死；GAN万岁！现代GAN基线</a><time datetime="2025-01-14T21:43:02.000Z" title="发表于 2025-01-15 05:43:02">2025-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%BA%8C%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%EF%BC%9A%E7%BB%BC%E8%BF%B0/" title="论文阅读五十二：大模型的参数高效微调：综述"><img src="/images/PEFT.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读五十二：大模型的参数高效微调：综述"/></a><div class="content"><a class="title" href="/2024/12/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%BA%8C%EF%BC%9A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83%EF%BC%9A%E7%BB%BC%E8%BF%B0/" title="论文阅读五十二：大模型的参数高效微调：综述">论文阅读五十二：大模型的参数高效微调：综述</a><time datetime="2024-12-19T05:36:50.000Z" title="发表于 2024-12-19 13:36:50">2024-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%B8%80%EF%BC%9AReFT%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%BE%AE%E8%B0%83%E6%8E%A8%E7%90%86/" title="论文阅读五十一：ReFT：强化微调推理"><img src="/images/ReFT.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读五十一：ReFT：强化微调推理"/></a><div class="content"><a class="title" href="/2024/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%BA%94%E5%8D%81%E4%B8%80%EF%BC%9AReFT%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%BE%AE%E8%B0%83%E6%8E%A8%E7%90%86/" title="论文阅读五十一：ReFT：强化微调推理">论文阅读五十一：ReFT：强化微调推理</a><time datetime="2024-12-17T11:56:17.000Z" title="发表于 2024-12-17 19:56:17">2024-12-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Serge Wang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>